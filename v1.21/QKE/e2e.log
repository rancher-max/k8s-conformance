I0113 07:33:36.110640      24 e2e.go:129] Starting e2e run "2c9cc809-df4c-4713-88fa-a5e7697f5ce9" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1642059215 - Will randomize all specs
Will run 339 of 5770 specs

Jan 13 07:33:36.120: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:33:36.122: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 13 07:33:36.140: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 13 07:33:36.175: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 13 07:33:36.175: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jan 13 07:33:36.175: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 13 07:33:36.182: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 13 07:33:36.182: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-qingcloud-node' (0 seconds elapsed)
Jan 13 07:33:36.182: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 13 07:33:36.182: INFO: e2e test version: v1.21.5
Jan 13 07:33:36.184: INFO: kube-apiserver version: v1.21.5
Jan 13 07:33:36.184: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:33:36.194: INFO: Cluster IP family: ipv4
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:33:36.194: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
Jan 13 07:33:36.240: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
W0113 07:33:36.240330      24 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:33:36.247: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4" in namespace "projected-2722" to be "Succeeded or Failed"
Jan 13 07:33:36.255: INFO: Pod "downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.241367ms
Jan 13 07:33:38.258: INFO: Pod "downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010949279s
STEP: Saw pod success
Jan 13 07:33:38.258: INFO: Pod "downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4" satisfied condition "Succeeded or Failed"
Jan 13 07:33:38.260: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4 container client-container: <nil>
STEP: delete the pod
Jan 13 07:33:38.281: INFO: Waiting for pod downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4 to disappear
Jan 13 07:33:38.287: INFO: Pod downwardapi-volume-3539d6e2-f309-4ee0-92e9-43482fcb80c4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:33:38.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2722" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":1,"skipped":7,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:33:38.292: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-2vg5
STEP: Creating a pod to test atomic-volume-subpath
Jan 13 07:33:38.334: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2vg5" in namespace "subpath-3677" to be "Succeeded or Failed"
Jan 13 07:33:38.348: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.316257ms
Jan 13 07:33:40.352: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.0180625s
Jan 13 07:33:42.357: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.023044677s
Jan 13 07:33:44.362: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.028019171s
Jan 13 07:33:46.367: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.033294581s
Jan 13 07:33:48.372: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.038247619s
Jan 13 07:33:50.375: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.041671969s
Jan 13 07:33:52.381: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.047161057s
Jan 13 07:33:54.386: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.052300945s
Jan 13 07:33:56.393: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.059546436s
Jan 13 07:33:58.400: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.065897486s
Jan 13 07:34:00.407: INFO: Pod "pod-subpath-test-projected-2vg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.07341313s
STEP: Saw pod success
Jan 13 07:34:00.407: INFO: Pod "pod-subpath-test-projected-2vg5" satisfied condition "Succeeded or Failed"
Jan 13 07:34:00.409: INFO: Trying to get logs from node worker-s001 pod pod-subpath-test-projected-2vg5 container test-container-subpath-projected-2vg5: <nil>
STEP: delete the pod
Jan 13 07:34:00.422: INFO: Waiting for pod pod-subpath-test-projected-2vg5 to disappear
Jan 13 07:34:00.426: INFO: Pod pod-subpath-test-projected-2vg5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-2vg5
Jan 13 07:34:00.426: INFO: Deleting pod "pod-subpath-test-projected-2vg5" in namespace "subpath-3677"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:34:00.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3677" for this suite.

• [SLOW TEST:22.157 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":2,"skipped":8,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:34:00.449: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jan 13 07:34:00.487: INFO: Waiting up to 5m0s for pod "client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e" in namespace "containers-6399" to be "Succeeded or Failed"
Jan 13 07:34:00.491: INFO: Pod "client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.934734ms
Jan 13 07:34:02.494: INFO: Pod "client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006600025s
STEP: Saw pod success
Jan 13 07:34:02.494: INFO: Pod "client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e" satisfied condition "Succeeded or Failed"
Jan 13 07:34:02.499: INFO: Trying to get logs from node worker-s001 pod client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e container agnhost-container: <nil>
STEP: delete the pod
Jan 13 07:34:02.514: INFO: Waiting for pod client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e to disappear
Jan 13 07:34:02.520: INFO: Pod client-containers-334ae49f-d1e9-4b7c-ac8d-54ac3ce6bb2e no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:34:02.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6399" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":3,"skipped":83,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:34:02.524: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:34:02.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3409" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":4,"skipped":88,"failed":0}
S
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:34:02.565: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jan 13 07:34:02.600: INFO: Major version: 1
STEP: Confirm minor version
Jan 13 07:34:02.600: INFO: cleanMinorVersion: 21
Jan 13 07:34:02.600: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:34:02.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8006" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":5,"skipped":89,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:34:02.604: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0113 07:34:12.718879      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 13 07:35:14.736: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jan 13 07:35:14.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-74zqt" in namespace "gc-2692"
Jan 13 07:35:14.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-8p5tv" in namespace "gc-2692"
Jan 13 07:35:14.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccsl2" in namespace "gc-2692"
Jan 13 07:35:14.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpslc" in namespace "gc-2692"
Jan 13 07:35:14.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwxz8" in namespace "gc-2692"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:35:14.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2692" for this suite.

• [SLOW TEST:72.213 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":6,"skipped":89,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:35:14.817: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jan 13 07:35:14.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-5909 api-versions'
Jan 13 07:35:14.922: INFO: stderr: ""
Jan 13 07:35:14.922: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.k8s.io/v1beta1\napplication.kubesphere.io/v1alpha1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.kubesphere.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\ngateway.kubesphere.io/v1alpha1\niam.kubesphere.io/v1alpha2\ninstaller.kubesphere.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.kubesphere.io/v1alpha1\nmonitoring.kubesphere.io/v1alpha2\nnetwork.kubesphere.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nnotification.kubesphere.io/v2beta1\nnotification.kubesphere.io/v2beta2\npolicy/v1\npolicy/v1beta1\nquota.kubesphere.io/v1alpha2\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nservicemesh.kubesphere.io/v1alpha2\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntenant.kubesphere.io/v1alpha1\ntenant.kubesphere.io/v1alpha2\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:35:14.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5909" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":7,"skipped":114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:35:14.928: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:35:14.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4178" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":8,"skipped":136,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:35:14.977: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 13 07:35:15.023: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 07:36:15.059: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:36:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:36:15.099: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 13 07:36:15.101: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:36:15.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7364" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:36:15.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3505" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.194 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":9,"skipped":148,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:36:15.171: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2738
STEP: creating service affinity-clusterip-transition in namespace services-2738
STEP: creating replication controller affinity-clusterip-transition in namespace services-2738
I0113 07:36:15.205431      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2738, replica count: 3
I0113 07:36:18.256226      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:36:18.262: INFO: Creating new exec pod
Jan 13 07:36:21.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2738 exec execpod-affinitysfvzn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 13 07:36:21.576: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 13 07:36:21.576: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:36:21.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2738 exec execpod-affinitysfvzn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.101.59 80'
Jan 13 07:36:21.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.101.59 80\nConnection to 10.96.101.59 80 port [tcp/http] succeeded!\n"
Jan 13 07:36:21.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:36:21.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2738 exec execpod-affinitysfvzn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.101.59:80/ ; done'
Jan 13 07:36:21.909: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n"
Jan 13 07:36:21.909: INFO: stdout: "\naffinity-clusterip-transition-ftw7z\naffinity-clusterip-transition-mz9hn\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-ftw7z\naffinity-clusterip-transition-mz9hn\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-ftw7z\naffinity-clusterip-transition-mz9hn\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-ftw7z\naffinity-clusterip-transition-mz9hn\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-ftw7z\naffinity-clusterip-transition-mz9hn\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-ftw7z"
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-ftw7z
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-mz9hn
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-ftw7z
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-mz9hn
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-ftw7z
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-mz9hn
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-ftw7z
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-mz9hn
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-ftw7z
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-mz9hn
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:21.909: INFO: Received response from host: affinity-clusterip-transition-ftw7z
Jan 13 07:36:21.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2738 exec execpod-affinitysfvzn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.101.59:80/ ; done'
Jan 13 07:36:22.106: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.101.59:80/\n"
Jan 13 07:36:22.106: INFO: stdout: "\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26\naffinity-clusterip-transition-nwn26"
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Received response from host: affinity-clusterip-transition-nwn26
Jan 13 07:36:22.106: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2738, will wait for the garbage collector to delete the pods
Jan 13 07:36:22.173: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.585836ms
Jan 13 07:36:22.274: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.090285ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:36:28.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2738" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:13.825 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":10,"skipped":151,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:36:28.996: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 13 07:36:29.029: INFO: Waiting up to 5m0s for pod "downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff" in namespace "downward-api-8075" to be "Succeeded or Failed"
Jan 13 07:36:29.033: INFO: Pod "downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622723ms
Jan 13 07:36:31.036: INFO: Pod "downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007460513s
STEP: Saw pod success
Jan 13 07:36:31.037: INFO: Pod "downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff" satisfied condition "Succeeded or Failed"
Jan 13 07:36:31.039: INFO: Trying to get logs from node worker-s001 pod downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff container dapi-container: <nil>
STEP: delete the pod
Jan 13 07:36:31.058: INFO: Waiting for pod downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff to disappear
Jan 13 07:36:31.061: INFO: Pod downward-api-97d5c62c-72a9-40a7-a704-8ef3b7d9c6ff no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:36:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8075" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":11,"skipped":152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:36:31.067: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:36:31.152: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"530a06eb-e798-447e-a92a-bd81b47ffb86", Controller:(*bool)(0xc0037bed52), BlockOwnerDeletion:(*bool)(0xc0037bed53)}}
Jan 13 07:36:31.156: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b026a7f5-8910-4297-a592-9e0f2a251a97", Controller:(*bool)(0xc0024dcb3a), BlockOwnerDeletion:(*bool)(0xc0024dcb3b)}}
Jan 13 07:36:31.167: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a6e4df0e-3b12-4ac5-b3ed-38abf065d087", Controller:(*bool)(0xc0031c0fda), BlockOwnerDeletion:(*bool)(0xc0031c0fdb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:36:36.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6440" for this suite.

• [SLOW TEST:5.124 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":12,"skipped":220,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:36:36.191: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jan 13 07:36:36.228: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 07:37:36.262: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:37:36.265: INFO: Starting informer...
STEP: Starting pods...
Jan 13 07:37:36.479: INFO: Pod1 is running on worker-s001. Tainting Node
Jan 13 07:37:38.698: INFO: Pod2 is running on worker-s001. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 13 07:37:45.719: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 13 07:38:08.876: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:38:08.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1081" for this suite.

• [SLOW TEST:92.717 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":13,"skipped":232,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:38:08.908: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-411
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 13 07:38:09.000: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 07:38:09.045: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:38:11.048: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:38:13.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:15.051: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:17.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:19.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:21.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:23.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:25.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:27.049: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:38:29.048: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 13 07:38:29.052: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 13 07:38:31.056: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 13 07:38:33.085: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 07:38:33.085: INFO: Breadth first check of 10.10.28.11 on host 192.168.100.59...
Jan 13 07:38:33.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.28.18:9080/dial?request=hostname&protocol=http&host=10.10.28.11&port=8080&tries=1'] Namespace:pod-network-test-411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:38:33.087: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:38:33.159: INFO: Waiting for responses: map[]
Jan 13 07:38:33.159: INFO: reached 10.10.28.11 after 0/1 tries
Jan 13 07:38:33.159: INFO: Breadth first check of 10.10.131.185 on host 192.168.100.60...
Jan 13 07:38:33.162: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.28.18:9080/dial?request=hostname&protocol=http&host=10.10.131.185&port=8080&tries=1'] Namespace:pod-network-test-411 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:38:33.162: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:38:33.236: INFO: Waiting for responses: map[]
Jan 13 07:38:33.236: INFO: reached 10.10.131.185 after 0/1 tries
Jan 13 07:38:33.236: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:38:33.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-411" for this suite.

• [SLOW TEST:24.333 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":14,"skipped":240,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:38:33.242: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:38:50.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4108" for this suite.

• [SLOW TEST:17.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":15,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:38:50.329: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 13 07:38:50.378: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.378: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.391: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.391: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.417: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.417: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.451: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:50.451: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 13 07:38:52.387: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 13 07:38:52.387: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 13 07:38:53.088: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 13 07:38:53.099: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.100: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 0
Jan 13 07:38:53.101: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:38:53.101: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:38:53.101: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.101: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.101: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.101: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.102: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.102: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.119: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.119: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.139: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.139: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:53.149: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:38:53.149: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:38:56.370: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:56.370: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:38:56.394: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
STEP: listing Deployments
Jan 13 07:38:56.400: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 13 07:38:56.407: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 13 07:38:56.411: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:56.415: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:56.440: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:56.450: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:56.459: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:58.467: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:58.599: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:58.650: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:38:58.659: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 13 07:39:00.558: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 13 07:39:00.586: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:39:00.586: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:39:00.586: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:39:00.586: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:39:00.586: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 1
Jan 13 07:39:00.587: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:39:00.587: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 3
Jan 13 07:39:00.587: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:39:00.587: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 2
Jan 13 07:39:00.587: INFO: observed Deployment test-deployment in namespace deployment-5918 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.591: INFO: observed event type MODIFIED
Jan 13 07:39:00.592: INFO: observed event type MODIFIED
Jan 13 07:39:00.592: INFO: observed event type MODIFIED
Jan 13 07:39:00.592: INFO: observed event type MODIFIED
Jan 13 07:39:00.592: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 07:39:00.594: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 13 07:39:00.599: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-5918  1aba5990-9f24-40d0-811b-78fd873e2f7b 383197 4 2022-01-13 07:38:53 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 94196e5f-7827-4542-8e7b-d1317b1a531b 0xc002ed7297 0xc002ed7298}] []  [{kube-controller-manager Update apps/v1 2022-01-13 07:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94196e5f-7827-4542-8e7b-d1317b1a531b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002ed7300 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 13 07:39:00.604: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-5918  3c9d9fd6-4ff5-4239-ab50-e898857d7706 383084 3 2022-01-13 07:38:50 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 94196e5f-7827-4542-8e7b-d1317b1a531b 0xc002ed7377 0xc002ed7378}] []  [{kube-controller-manager Update apps/v1 2022-01-13 07:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94196e5f-7827-4542-8e7b-d1317b1a531b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002ed7400 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 13 07:39:00.607: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-5918  661c18f0-94d6-49e0-94fc-e7f1f04effa8 383187 2 2022-01-13 07:38:56 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 94196e5f-7827-4542-8e7b-d1317b1a531b 0xc002ed7467 0xc002ed7468}] []  [{kube-controller-manager Update apps/v1 2022-01-13 07:38:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94196e5f-7827-4542-8e7b-d1317b1a531b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002ed74f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 13 07:39:00.609: INFO: pod: "test-deployment-85d87c6f4b-8f6jm":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-8f6jm test-deployment-85d87c6f4b- deployment-5918  cf147166-3109-4cf5-b91c-680a7ca4caab 383139 0 2022-01-13 07:38:56 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:10.10.28.23/32 cni.projectcalico.org/podIPs:10.10.28.23/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 661c18f0-94d6-49e0-94fc-e7f1f04effa8 0xc00311bbb7 0xc00311bbb8}] []  [{kube-controller-manager Update v1 2022-01-13 07:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661c18f0-94d6-49e0-94fc-e7f1f04effa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 07:38:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 07:38:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6thsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6thsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:38:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:38:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:38:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:38:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.23,StartTime:2022-01-13 07:38:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 07:38:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://583c124104055348c46d70f58074dede2691e83b24ba7d34744a8fb671d27299,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 13 07:39:00.609: INFO: pod: "test-deployment-85d87c6f4b-gkhtc":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-gkhtc test-deployment-85d87c6f4b- deployment-5918  3af8aa84-96ee-4412-8dff-88b9aa677ede 383186 0 2022-01-13 07:38:58 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:10.10.131.141/32 cni.projectcalico.org/podIPs:10.10.131.141/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 661c18f0-94d6-49e0-94fc-e7f1f04effa8 0xc00311bdd7 0xc00311bdd8}] []  [{kube-controller-manager Update v1 2022-01-13 07:38:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661c18f0-94d6-49e0-94fc-e7f1f04effa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 07:38:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 07:39:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xr92z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xr92z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:38:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:38:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:10.10.131.141,StartTime:2022-01-13 07:38:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 07:38:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://a1a79398dafd8b85b8b6a602e35d6ab7fa8af44ba0e16df9deaf9f449f7cf083,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:00.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5918" for this suite.

• [SLOW TEST:10.287 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":16,"skipped":272,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:00.616: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 13 07:39:00.677: INFO: Waiting up to 5m0s for pod "pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd" in namespace "emptydir-3143" to be "Succeeded or Failed"
Jan 13 07:39:00.684: INFO: Pod "pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.5471ms
Jan 13 07:39:02.688: INFO: Pod "pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010331676s
STEP: Saw pod success
Jan 13 07:39:02.688: INFO: Pod "pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd" satisfied condition "Succeeded or Failed"
Jan 13 07:39:02.690: INFO: Trying to get logs from node worker-s001 pod pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd container test-container: <nil>
STEP: delete the pod
Jan 13 07:39:02.711: INFO: Waiting for pod pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd to disappear
Jan 13 07:39:02.712: INFO: Pod pod-ca268549-2af6-46e8-959f-f9ba0ab7d7dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:02.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3143" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":17,"skipped":284,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:02.719: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 13 07:39:02.751: INFO: Waiting up to 5m0s for pod "downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c" in namespace "downward-api-1866" to be "Succeeded or Failed"
Jan 13 07:39:02.760: INFO: Pod "downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.018756ms
Jan 13 07:39:04.763: INFO: Pod "downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012487831s
STEP: Saw pod success
Jan 13 07:39:04.763: INFO: Pod "downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c" satisfied condition "Succeeded or Failed"
Jan 13 07:39:04.765: INFO: Trying to get logs from node worker-s001 pod downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c container dapi-container: <nil>
STEP: delete the pod
Jan 13 07:39:04.785: INFO: Waiting for pod downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c to disappear
Jan 13 07:39:04.786: INFO: Pod downward-api-b42fd77f-5017-454f-9cec-24af6bcaea3c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:04.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1866" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":18,"skipped":289,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:04.792: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:39:04.833: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 13 07:39:04.838: INFO: Number of nodes with available pods: 0
Jan 13 07:39:04.838: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 13 07:39:04.856: INFO: Number of nodes with available pods: 0
Jan 13 07:39:04.856: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:05.860: INFO: Number of nodes with available pods: 0
Jan 13 07:39:05.860: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:06.860: INFO: Number of nodes with available pods: 1
Jan 13 07:39:06.860: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 13 07:39:06.874: INFO: Number of nodes with available pods: 1
Jan 13 07:39:06.874: INFO: Number of running nodes: 0, number of available pods: 1
Jan 13 07:39:07.877: INFO: Number of nodes with available pods: 0
Jan 13 07:39:07.877: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 13 07:39:07.885: INFO: Number of nodes with available pods: 0
Jan 13 07:39:07.885: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:08.888: INFO: Number of nodes with available pods: 0
Jan 13 07:39:08.888: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:09.888: INFO: Number of nodes with available pods: 0
Jan 13 07:39:09.889: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:10.889: INFO: Number of nodes with available pods: 0
Jan 13 07:39:10.889: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:11.888: INFO: Number of nodes with available pods: 0
Jan 13 07:39:11.888: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 07:39:12.889: INFO: Number of nodes with available pods: 1
Jan 13 07:39:12.889: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9110, will wait for the garbage collector to delete the pods
Jan 13 07:39:12.946: INFO: Deleting DaemonSet.extensions daemon-set took: 2.626379ms
Jan 13 07:39:13.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.009768ms
Jan 13 07:39:16.950: INFO: Number of nodes with available pods: 0
Jan 13 07:39:16.950: INFO: Number of running nodes: 0, number of available pods: 0
Jan 13 07:39:16.954: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"383468"},"items":null}

Jan 13 07:39:16.956: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"383468"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:16.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9110" for this suite.

• [SLOW TEST:12.185 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":19,"skipped":295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:16.977: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-ef942bbd-32e0-4a7f-9a97-810dc4ce64b2
STEP: Creating a pod to test consume secrets
Jan 13 07:39:17.022: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f" in namespace "projected-3075" to be "Succeeded or Failed"
Jan 13 07:39:17.027: INFO: Pod "pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.134365ms
Jan 13 07:39:19.031: INFO: Pod "pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009117759s
STEP: Saw pod success
Jan 13 07:39:19.031: INFO: Pod "pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f" satisfied condition "Succeeded or Failed"
Jan 13 07:39:19.033: INFO: Trying to get logs from node worker-s001 pod pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 07:39:19.054: INFO: Waiting for pod pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f to disappear
Jan 13 07:39:19.061: INFO: Pod pod-projected-secrets-62f323b5-b77e-40a6-a765-305d9a7a030f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:19.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3075" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":20,"skipped":321,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:19.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4078" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":21,"skipped":322,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:19.126: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:39:19.469: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 13 07:39:19.470: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 13 07:39:19.470: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.470: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 13 07:39:19.470: INFO: Checking APIGroup: apps
Jan 13 07:39:19.470: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 13 07:39:19.470: INFO: Versions found [{apps/v1 v1}]
Jan 13 07:39:19.470: INFO: apps/v1 matches apps/v1
Jan 13 07:39:19.470: INFO: Checking APIGroup: events.k8s.io
Jan 13 07:39:19.471: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 13 07:39:19.471: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.471: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 13 07:39:19.471: INFO: Checking APIGroup: authentication.k8s.io
Jan 13 07:39:19.472: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 13 07:39:19.472: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.472: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 13 07:39:19.472: INFO: Checking APIGroup: authorization.k8s.io
Jan 13 07:39:19.472: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 13 07:39:19.472: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.472: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 13 07:39:19.472: INFO: Checking APIGroup: autoscaling
Jan 13 07:39:19.473: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jan 13 07:39:19.473: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 13 07:39:19.473: INFO: autoscaling/v1 matches autoscaling/v1
Jan 13 07:39:19.473: INFO: Checking APIGroup: batch
Jan 13 07:39:19.473: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 13 07:39:19.473: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 13 07:39:19.473: INFO: batch/v1 matches batch/v1
Jan 13 07:39:19.473: INFO: Checking APIGroup: certificates.k8s.io
Jan 13 07:39:19.474: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 13 07:39:19.474: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.474: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 13 07:39:19.474: INFO: Checking APIGroup: networking.k8s.io
Jan 13 07:39:19.475: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 13 07:39:19.475: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.475: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 13 07:39:19.475: INFO: Checking APIGroup: extensions
Jan 13 07:39:19.475: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jan 13 07:39:19.475: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jan 13 07:39:19.475: INFO: extensions/v1beta1 matches extensions/v1beta1
Jan 13 07:39:19.475: INFO: Checking APIGroup: policy
Jan 13 07:39:19.476: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 13 07:39:19.476: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 13 07:39:19.476: INFO: policy/v1 matches policy/v1
Jan 13 07:39:19.476: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 13 07:39:19.477: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 13 07:39:19.477: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.477: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 13 07:39:19.477: INFO: Checking APIGroup: storage.k8s.io
Jan 13 07:39:19.477: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 13 07:39:19.477: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.477: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 13 07:39:19.477: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 13 07:39:19.478: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 13 07:39:19.478: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.478: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 13 07:39:19.478: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 13 07:39:19.479: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 13 07:39:19.479: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.479: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 13 07:39:19.479: INFO: Checking APIGroup: scheduling.k8s.io
Jan 13 07:39:19.479: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 13 07:39:19.479: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.479: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 13 07:39:19.479: INFO: Checking APIGroup: coordination.k8s.io
Jan 13 07:39:19.480: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 13 07:39:19.480: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.480: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 13 07:39:19.480: INFO: Checking APIGroup: node.k8s.io
Jan 13 07:39:19.481: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 13 07:39:19.481: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.481: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 13 07:39:19.481: INFO: Checking APIGroup: discovery.k8s.io
Jan 13 07:39:19.481: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 13 07:39:19.481: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.481: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 13 07:39:19.481: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 13 07:39:19.485: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jan 13 07:39:19.485: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.485: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jan 13 07:39:19.485: INFO: Checking APIGroup: crd.projectcalico.org
Jan 13 07:39:19.486: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 13 07:39:19.486: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 13 07:39:19.486: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 13 07:39:19.486: INFO: Checking APIGroup: monitoring.coreos.com
Jan 13 07:39:19.486: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 13 07:39:19.486: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 13 07:39:19.486: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 13 07:39:19.486: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 13 07:39:19.487: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 13 07:39:19.487: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.487: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 13 07:39:19.487: INFO: Checking APIGroup: application.kubesphere.io
Jan 13 07:39:19.487: INFO: PreferredVersion.GroupVersion: application.kubesphere.io/v1alpha1
Jan 13 07:39:19.488: INFO: Versions found [{application.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.488: INFO: application.kubesphere.io/v1alpha1 matches application.kubesphere.io/v1alpha1
Jan 13 07:39:19.488: INFO: Checking APIGroup: cluster.kubesphere.io
Jan 13 07:39:19.488: INFO: PreferredVersion.GroupVersion: cluster.kubesphere.io/v1alpha1
Jan 13 07:39:19.488: INFO: Versions found [{cluster.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.488: INFO: cluster.kubesphere.io/v1alpha1 matches cluster.kubesphere.io/v1alpha1
Jan 13 07:39:19.488: INFO: Checking APIGroup: gateway.kubesphere.io
Jan 13 07:39:19.489: INFO: PreferredVersion.GroupVersion: gateway.kubesphere.io/v1alpha1
Jan 13 07:39:19.489: INFO: Versions found [{gateway.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.489: INFO: gateway.kubesphere.io/v1alpha1 matches gateway.kubesphere.io/v1alpha1
Jan 13 07:39:19.489: INFO: Checking APIGroup: installer.kubesphere.io
Jan 13 07:39:19.489: INFO: PreferredVersion.GroupVersion: installer.kubesphere.io/v1alpha1
Jan 13 07:39:19.489: INFO: Versions found [{installer.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.489: INFO: installer.kubesphere.io/v1alpha1 matches installer.kubesphere.io/v1alpha1
Jan 13 07:39:19.489: INFO: Checking APIGroup: monitoring.kubesphere.io
Jan 13 07:39:19.490: INFO: PreferredVersion.GroupVersion: monitoring.kubesphere.io/v1alpha2
Jan 13 07:39:19.490: INFO: Versions found [{monitoring.kubesphere.io/v1alpha2 v1alpha2} {monitoring.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.490: INFO: monitoring.kubesphere.io/v1alpha2 matches monitoring.kubesphere.io/v1alpha2
Jan 13 07:39:19.490: INFO: Checking APIGroup: network.kubesphere.io
Jan 13 07:39:19.491: INFO: PreferredVersion.GroupVersion: network.kubesphere.io/v1alpha1
Jan 13 07:39:19.491: INFO: Versions found [{network.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.491: INFO: network.kubesphere.io/v1alpha1 matches network.kubesphere.io/v1alpha1
Jan 13 07:39:19.491: INFO: Checking APIGroup: tenant.kubesphere.io
Jan 13 07:39:19.491: INFO: PreferredVersion.GroupVersion: tenant.kubesphere.io/v1alpha2
Jan 13 07:39:19.491: INFO: Versions found [{tenant.kubesphere.io/v1alpha2 v1alpha2} {tenant.kubesphere.io/v1alpha1 v1alpha1}]
Jan 13 07:39:19.491: INFO: tenant.kubesphere.io/v1alpha2 matches tenant.kubesphere.io/v1alpha2
Jan 13 07:39:19.491: INFO: Checking APIGroup: iam.kubesphere.io
Jan 13 07:39:19.492: INFO: PreferredVersion.GroupVersion: iam.kubesphere.io/v1alpha2
Jan 13 07:39:19.492: INFO: Versions found [{iam.kubesphere.io/v1alpha2 v1alpha2}]
Jan 13 07:39:19.492: INFO: iam.kubesphere.io/v1alpha2 matches iam.kubesphere.io/v1alpha2
Jan 13 07:39:19.492: INFO: Checking APIGroup: quota.kubesphere.io
Jan 13 07:39:19.493: INFO: PreferredVersion.GroupVersion: quota.kubesphere.io/v1alpha2
Jan 13 07:39:19.493: INFO: Versions found [{quota.kubesphere.io/v1alpha2 v1alpha2}]
Jan 13 07:39:19.493: INFO: quota.kubesphere.io/v1alpha2 matches quota.kubesphere.io/v1alpha2
Jan 13 07:39:19.493: INFO: Checking APIGroup: servicemesh.kubesphere.io
Jan 13 07:39:19.493: INFO: PreferredVersion.GroupVersion: servicemesh.kubesphere.io/v1alpha2
Jan 13 07:39:19.493: INFO: Versions found [{servicemesh.kubesphere.io/v1alpha2 v1alpha2}]
Jan 13 07:39:19.493: INFO: servicemesh.kubesphere.io/v1alpha2 matches servicemesh.kubesphere.io/v1alpha2
Jan 13 07:39:19.493: INFO: Checking APIGroup: app.k8s.io
Jan 13 07:39:19.494: INFO: PreferredVersion.GroupVersion: app.k8s.io/v1beta1
Jan 13 07:39:19.494: INFO: Versions found [{app.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.494: INFO: app.k8s.io/v1beta1 matches app.k8s.io/v1beta1
Jan 13 07:39:19.494: INFO: Checking APIGroup: notification.kubesphere.io
Jan 13 07:39:19.494: INFO: PreferredVersion.GroupVersion: notification.kubesphere.io/v2beta2
Jan 13 07:39:19.495: INFO: Versions found [{notification.kubesphere.io/v2beta2 v2beta2} {notification.kubesphere.io/v2beta1 v2beta1}]
Jan 13 07:39:19.495: INFO: notification.kubesphere.io/v2beta2 matches notification.kubesphere.io/v2beta2
Jan 13 07:39:19.495: INFO: Checking APIGroup: metrics.k8s.io
Jan 13 07:39:19.495: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 13 07:39:19.495: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 13 07:39:19.495: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:19.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3569" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":22,"skipped":341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:19.501: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1436
STEP: creating service affinity-nodeport-transition in namespace services-1436
STEP: creating replication controller affinity-nodeport-transition in namespace services-1436
I0113 07:39:19.537673      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1436, replica count: 3
I0113 07:39:22.587996      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:39:22.601: INFO: Creating new exec pod
Jan 13 07:39:25.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1436 exec execpod-affinityn6f6x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 13 07:39:25.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 13 07:39:25.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:39:25.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1436 exec execpod-affinityn6f6x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.41.139 80'
Jan 13 07:39:25.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.41.139 80\nConnection to 10.96.41.139 80 port [tcp/http] succeeded!\n"
Jan 13 07:39:25.895: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:39:25.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1436 exec execpod-affinityn6f6x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.59 32010'
Jan 13 07:39:26.018: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.59 32010\nConnection to 192.168.100.59 32010 port [tcp/*] succeeded!\n"
Jan 13 07:39:26.019: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:39:26.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1436 exec execpod-affinityn6f6x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.60 32010'
Jan 13 07:39:26.160: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.60 32010\nConnection to 192.168.100.60 32010 port [tcp/*] succeeded!\n"
Jan 13 07:39:26.160: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:39:26.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1436 exec execpod-affinityn6f6x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.59:32010/ ; done'
Jan 13 07:39:26.359: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n"
Jan 13 07:39:26.359: INFO: stdout: "\naffinity-nodeport-transition-8b9ph\naffinity-nodeport-transition-5r6gq\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-8b9ph\naffinity-nodeport-transition-5r6gq\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-8b9ph\naffinity-nodeport-transition-5r6gq\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-8b9ph\naffinity-nodeport-transition-5r6gq\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-8b9ph\naffinity-nodeport-transition-5r6gq\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-8b9ph"
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-8b9ph
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-5r6gq
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-8b9ph
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-5r6gq
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-8b9ph
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-5r6gq
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-8b9ph
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-5r6gq
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-8b9ph
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-5r6gq
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.359: INFO: Received response from host: affinity-nodeport-transition-8b9ph
Jan 13 07:39:26.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1436 exec execpod-affinityn6f6x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.59:32010/ ; done'
Jan 13 07:39:26.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:32010/\n"
Jan 13 07:39:26.545: INFO: stdout: "\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8\naffinity-nodeport-transition-br8p8"
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Received response from host: affinity-nodeport-transition-br8p8
Jan 13 07:39:26.545: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1436, will wait for the garbage collector to delete the pods
Jan 13 07:39:26.616: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.669062ms
Jan 13 07:39:26.717: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.444972ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:31.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1436" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:11.748 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":23,"skipped":380,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:31.249: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 13 07:39:31.282: INFO: Waiting up to 5m0s for pod "pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6" in namespace "emptydir-547" to be "Succeeded or Failed"
Jan 13 07:39:31.287: INFO: Pod "pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.631477ms
Jan 13 07:39:33.291: INFO: Pod "pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008369544s
STEP: Saw pod success
Jan 13 07:39:33.291: INFO: Pod "pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6" satisfied condition "Succeeded or Failed"
Jan 13 07:39:33.293: INFO: Trying to get logs from node worker-s001 pod pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6 container test-container: <nil>
STEP: delete the pod
Jan 13 07:39:33.309: INFO: Waiting for pod pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6 to disappear
Jan 13 07:39:33.313: INFO: Pod pod-8bcc3129-2053-4e9f-8bc1-4c19aa5d93e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:33.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-547" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":24,"skipped":387,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:33.319: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 13 07:39:33.380: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8219  c163fbba-d9d0-40df-a989-6671709d8658 383784 0 2022-01-13 07:39:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-01-13 07:39:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 07:39:33.380: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8219  c163fbba-d9d0-40df-a989-6671709d8658 383785 0 2022-01-13 07:39:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-01-13 07:39:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:33.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8219" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":25,"skipped":394,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:33.395: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 13 07:39:33.440: INFO: Waiting up to 5m0s for pod "pod-f22d5328-5a7d-456a-a0a0-0c112818a637" in namespace "emptydir-5253" to be "Succeeded or Failed"
Jan 13 07:39:33.442: INFO: Pod "pod-f22d5328-5a7d-456a-a0a0-0c112818a637": Phase="Pending", Reason="", readiness=false. Elapsed: 2.235045ms
Jan 13 07:39:35.446: INFO: Pod "pod-f22d5328-5a7d-456a-a0a0-0c112818a637": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006193657s
STEP: Saw pod success
Jan 13 07:39:35.446: INFO: Pod "pod-f22d5328-5a7d-456a-a0a0-0c112818a637" satisfied condition "Succeeded or Failed"
Jan 13 07:39:35.448: INFO: Trying to get logs from node worker-s001 pod pod-f22d5328-5a7d-456a-a0a0-0c112818a637 container test-container: <nil>
STEP: delete the pod
Jan 13 07:39:35.469: INFO: Waiting for pod pod-f22d5328-5a7d-456a-a0a0-0c112818a637 to disappear
Jan 13 07:39:35.479: INFO: Pod pod-f22d5328-5a7d-456a-a0a0-0c112818a637 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:35.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5253" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":26,"skipped":409,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:35.484: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 13 07:39:35.541: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:39:37.546: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.100.60 on the node which pod1 resides and expect scheduled
Jan 13 07:39:37.554: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:39:39.560: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:39:41.558: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.100.60 but use UDP protocol on the node which pod2 resides
Jan 13 07:39:41.574: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:39:43.577: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:39:45.577: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 13 07:39:45.595: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:39:47.600: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 13 07:39:47.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.100.60 http://127.0.0.1:54323/hostname] Namespace:hostport-4344 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:39:47.610: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.100.60, port: 54323
Jan 13 07:39:47.701: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.100.60:54323/hostname] Namespace:hostport-4344 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:39:47.701: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.100.60, port: 54323 UDP
Jan 13 07:39:47.785: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.100.60 54323] Namespace:hostport-4344 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:39:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:52.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4344" for this suite.

• [SLOW TEST:17.380 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":27,"skipped":420,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:52.865: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 07:39:53.124: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 07:39:56.140: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:39:56.144: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5348-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:39:59.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1515" for this suite.
STEP: Destroying namespace "webhook-1515-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":28,"skipped":439,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:39:59.376: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:39:59.432: INFO: Waiting up to 5m0s for pod "downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c" in namespace "projected-4489" to be "Succeeded or Failed"
Jan 13 07:39:59.436: INFO: Pod "downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184996ms
Jan 13 07:40:01.442: INFO: Pod "downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009990547s
Jan 13 07:40:03.445: INFO: Pod "downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013259755s
STEP: Saw pod success
Jan 13 07:40:03.445: INFO: Pod "downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c" satisfied condition "Succeeded or Failed"
Jan 13 07:40:03.447: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c container client-container: <nil>
STEP: delete the pod
Jan 13 07:40:03.463: INFO: Waiting for pod downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c to disappear
Jan 13 07:40:03.467: INFO: Pod downwardapi-volume-942c6b10-7f7a-4737-b408-09fc3c58c35c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:40:03.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4489" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":29,"skipped":439,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:40:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-a720a85f-ade8-4a79-83d4-b7b66c85bdf2
STEP: Creating a pod to test consume secrets
Jan 13 07:40:03.517: INFO: Waiting up to 5m0s for pod "pod-secrets-7b240f25-5736-470e-825c-95b793a49317" in namespace "secrets-2656" to be "Succeeded or Failed"
Jan 13 07:40:03.520: INFO: Pod "pod-secrets-7b240f25-5736-470e-825c-95b793a49317": Phase="Pending", Reason="", readiness=false. Elapsed: 3.411464ms
Jan 13 07:40:05.524: INFO: Pod "pod-secrets-7b240f25-5736-470e-825c-95b793a49317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007550095s
Jan 13 07:40:07.530: INFO: Pod "pod-secrets-7b240f25-5736-470e-825c-95b793a49317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012856845s
STEP: Saw pod success
Jan 13 07:40:07.530: INFO: Pod "pod-secrets-7b240f25-5736-470e-825c-95b793a49317" satisfied condition "Succeeded or Failed"
Jan 13 07:40:07.532: INFO: Trying to get logs from node worker-s001 pod pod-secrets-7b240f25-5736-470e-825c-95b793a49317 container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 07:40:07.553: INFO: Waiting for pod pod-secrets-7b240f25-5736-470e-825c-95b793a49317 to disappear
Jan 13 07:40:07.562: INFO: Pod pod-secrets-7b240f25-5736-470e-825c-95b793a49317 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:40:07.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2656" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":30,"skipped":446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:40:07.570: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4459.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4459.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4459.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4459.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4459.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 181.4.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.4.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.4.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.4.181_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4459.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4459.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4459.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4459.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4459.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4459.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 181.4.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.4.181_udp@PTR;check="$$(dig +tcp +noall +answer +search 181.4.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.4.181_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 07:40:09.643: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.645: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.648: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.650: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.663: INFO: Unable to read jessie_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.664: INFO: Unable to read jessie_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.666: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.668: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:09.679: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@dns-test-service.dns-4459.svc.cluster.local jessie_tcp@dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:14.684: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.686: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.705: INFO: Unable to read jessie_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.707: INFO: Unable to read jessie_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.709: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.711: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:14.726: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@dns-test-service.dns-4459.svc.cluster.local jessie_tcp@dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:19.684: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.690: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.693: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.695: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.711: INFO: Unable to read jessie_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.715: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.717: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:19.729: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@dns-test-service.dns-4459.svc.cluster.local jessie_tcp@dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:24.685: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.689: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.692: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.712: INFO: Unable to read jessie_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.714: INFO: Unable to read jessie_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.717: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.718: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:24.731: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@dns-test-service.dns-4459.svc.cluster.local jessie_tcp@dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:29.684: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.686: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.704: INFO: Unable to read jessie_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.706: INFO: Unable to read jessie_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.708: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.710: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:29.722: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@dns-test-service.dns-4459.svc.cluster.local jessie_tcp@dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:34.686: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.689: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.692: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.694: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.718: INFO: Unable to read jessie_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.721: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.727: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:34.745: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@dns-test-service.dns-4459.svc.cluster.local jessie_tcp@dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:39.685: INFO: Unable to read wheezy_udp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:39.687: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:39.689: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:39.691: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:39.719: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:39.720: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local from pod dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3: the server could not find the requested resource (get pods dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3)
Jan 13 07:40:39.732: INFO: Lookups using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 failed for: [wheezy_udp@dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@dns-test-service.dns-4459.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4459.svc.cluster.local]

Jan 13 07:40:44.733: INFO: DNS probes using dns-4459/dns-test-cfacb402-54b0-4841-b2ab-28e5e90c70e3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:40:44.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4459" for this suite.

• [SLOW TEST:37.240 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":31,"skipped":489,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:40:44.810: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jan 13 07:40:44.835: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-8002 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:40:44.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8002" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":32,"skipped":490,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:40:44.890: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 13 07:40:44.917: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 07:40:44.921: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 07:40:44.922: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Jan 13 07:40:44.928: INFO: calico-node-vbqlj from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 07:40:44.928: INFO: csi-qingcloud-node-hxljj from kube-system started at 2022-01-13 07:38:08 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 07:40:44.928: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 07:40:44.928: INFO: kube-proxy-r2zxx from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 07:40:44.928: INFO: node-exporter-987g4 from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:40:44.928: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 07:40:44.928: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2022-01-13 07:38:48 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:40:44.928: INFO: 	Container prometheus ready: true, restart count 1
Jan 13 07:40:44.928: INFO: sonobuoy from sonobuoy started at 2022-01-13 07:33:31 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 07:40:44.928: INFO: sonobuoy-e2e-job-3cc758d0b1b14270 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container e2e ready: true, restart count 0
Jan 13 07:40:44.928: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 07:40:44.928: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-shz85 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.928: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 07:40:44.928: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 07:40:44.928: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Jan 13 07:40:44.935: INFO: calico-node-nvdsl from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 07:40:44.935: INFO: csi-qingcloud-controller-6f8f6458f7-vkmtw from kube-system started at 2022-01-12 18:28:18 +0000 UTC (5 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 13 07:40:44.935: INFO: csi-qingcloud-node-84m8s from kube-system started at 2022-01-12 06:37:21 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 07:40:44.935: INFO: kube-proxy-gdx9n from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 07:40:44.935: INFO: metrics-server-687cb5444-gwn4h from kube-system started at 2022-01-12 06:37:45 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container metrics-server ready: true, restart count 0
Jan 13 07:40:44.935: INFO: snapshot-controller-0 from kube-system started at 2022-01-12 06:38:20 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 13 07:40:44.935: INFO: default-http-backend-5bf68ff9b8-gt7w6 from kubesphere-controls-system started at 2022-01-12 06:39:06 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 13 07:40:44.935: INFO: kubectl-admin-6667774bb-lfhx9 from kubesphere-controls-system started at 2022-01-12 06:41:27 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container kubectl ready: true, restart count 0
Jan 13 07:40:44.935: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2022-01-12 18:28:22 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:40:44.935: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2022-01-12 06:40:50 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:40:44.935: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2022-01-12 17:16:42 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:40:44.935: INFO: kube-state-metrics-5547ddd4cc-79dkj from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (3 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 13 07:40:44.935: INFO: node-exporter-wjvtx from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 07:40:44.935: INFO: notification-manager-deployment-78664576cb-7cqhm from kubesphere-monitoring-system started at 2022-01-12 18:28:18 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container tenant ready: true, restart count 0
Jan 13 07:40:44.935: INFO: notification-manager-deployment-78664576cb-pppk6 from kubesphere-monitoring-system started at 2022-01-12 06:41:16 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container tenant ready: true, restart count 0
Jan 13 07:40:44.935: INFO: notification-manager-operator-7d44854f54-97b8n from kubesphere-monitoring-system started at 2022-01-12 06:41:10 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container notification-manager-operator ready: true, restart count 0
Jan 13 07:40:44.935: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2022-01-12 06:41:05 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container prometheus ready: true, restart count 0
Jan 13 07:40:44.935: INFO: prometheus-operator-5c5db79546-gl2sj from kubesphere-monitoring-system started at 2022-01-12 06:40:40 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 13 07:40:44.935: INFO: ks-installer-6cf7f47896-d9fqp from kubesphere-system started at 2022-01-12 06:37:23 +0000 UTC (1 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container installer ready: true, restart count 0
Jan 13 07:40:44.935: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-6f2nd from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 07:40:44.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 07:40:44.935: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-469bfcf3-2b17-4355-9269-80d509610b8d 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.100.59 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-469bfcf3-2b17-4355-9269-80d509610b8d off the node worker-s001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-469bfcf3-2b17-4355-9269-80d509610b8d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:45:51.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2996" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.155 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":33,"skipped":507,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:45:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:45:51.075: INFO: Got root ca configmap in namespace "svcaccounts-616"
Jan 13 07:45:51.078: INFO: Deleted root ca configmap in namespace "svcaccounts-616"
STEP: waiting for a new root ca configmap created
Jan 13 07:45:51.582: INFO: Recreated root ca configmap in namespace "svcaccounts-616"
Jan 13 07:45:51.585: INFO: Updated root ca configmap in namespace "svcaccounts-616"
STEP: waiting for the root ca configmap reconciled
Jan 13 07:45:52.092: INFO: Reconciled root ca configmap in namespace "svcaccounts-616"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:45:52.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-616" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":34,"skipped":519,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:45:52.097: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-7fb967f0-2c67-4e99-b15d-6ed93a4ad3a3
STEP: Creating a pod to test consume configMaps
Jan 13 07:45:52.141: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8" in namespace "projected-2060" to be "Succeeded or Failed"
Jan 13 07:45:52.146: INFO: Pod "pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231108ms
Jan 13 07:45:54.148: INFO: Pod "pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006998001s
STEP: Saw pod success
Jan 13 07:45:54.148: INFO: Pod "pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8" satisfied condition "Succeeded or Failed"
Jan 13 07:45:54.150: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 07:45:54.189: INFO: Waiting for pod pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8 to disappear
Jan 13 07:45:54.190: INFO: Pod pod-projected-configmaps-3a1c302b-3159-40ea-85a0-0f3468ea85b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:45:54.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2060" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":35,"skipped":522,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:45:54.197: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jan 13 07:45:54.226: INFO: Waiting up to 5m0s for pod "var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f" in namespace "var-expansion-4632" to be "Succeeded or Failed"
Jan 13 07:45:54.230: INFO: Pod "var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.714787ms
Jan 13 07:45:56.233: INFO: Pod "var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00721597s
STEP: Saw pod success
Jan 13 07:45:56.233: INFO: Pod "var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f" satisfied condition "Succeeded or Failed"
Jan 13 07:45:56.235: INFO: Trying to get logs from node worker-s001 pod var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f container dapi-container: <nil>
STEP: delete the pod
Jan 13 07:45:56.254: INFO: Waiting for pod var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f to disappear
Jan 13 07:45:56.261: INFO: Pod var-expansion-1ebf66c4-b2e8-4147-ac74-7a6de2f63e3f no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:45:56.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4632" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":36,"skipped":529,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:45:56.270: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:45:56.338: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:45:58.348: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:00.342: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:02.352: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:04.342: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:06.342: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:08.343: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:10.342: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:12.341: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:14.342: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:16.353: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = false)
Jan 13 07:46:18.348: INFO: The status of Pod test-webserver-de7576d8-ca3b-4616-ac83-0ebea9573f59 is Running (Ready = true)
Jan 13 07:46:18.350: INFO: Container started at 2022-01-13 07:45:57 +0000 UTC, pod became ready at 2022-01-13 07:46:16 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:46:18.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3023" for this suite.

• [SLOW TEST:22.086 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":37,"skipped":531,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:46:18.355: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 13 07:46:18.391: INFO: Waiting up to 5m0s for pod "pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a" in namespace "emptydir-4423" to be "Succeeded or Failed"
Jan 13 07:46:18.396: INFO: Pod "pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211113ms
Jan 13 07:46:20.400: INFO: Pod "pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009396095s
Jan 13 07:46:22.403: INFO: Pod "pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012556441s
STEP: Saw pod success
Jan 13 07:46:22.403: INFO: Pod "pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a" satisfied condition "Succeeded or Failed"
Jan 13 07:46:22.405: INFO: Trying to get logs from node worker-s001 pod pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a container test-container: <nil>
STEP: delete the pod
Jan 13 07:46:22.419: INFO: Waiting for pod pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a to disappear
Jan 13 07:46:22.422: INFO: Pod pod-0295c7aa-e88f-49cc-b180-1b1ddc5f004a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:46:22.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4423" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":38,"skipped":548,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:46:22.432: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:46:24.466: INFO: Deleting pod "var-expansion-ac69a1d0-8ed6-4a7e-aa69-322a1a39ec5a" in namespace "var-expansion-9185"
Jan 13 07:46:24.469: INFO: Wait up to 5m0s for pod "var-expansion-ac69a1d0-8ed6-4a7e-aa69-322a1a39ec5a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:46:38.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9185" for this suite.

• [SLOW TEST:16.051 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":39,"skipped":557,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:46:38.483: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jan 13 07:46:38.517: INFO: created test-pod-1
Jan 13 07:46:38.525: INFO: created test-pod-2
Jan 13 07:46:38.532: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:46:38.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4451" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":40,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:46:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-9jvv
STEP: Creating a pod to test atomic-volume-subpath
Jan 13 07:46:38.608: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9jvv" in namespace "subpath-7147" to be "Succeeded or Failed"
Jan 13 07:46:38.610: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237281ms
Jan 13 07:46:40.613: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005187099s
Jan 13 07:46:42.617: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009190634s
Jan 13 07:46:44.620: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 6.012409274s
Jan 13 07:46:46.624: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 8.01607122s
Jan 13 07:46:48.628: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 10.01997342s
Jan 13 07:46:50.631: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 12.023479011s
Jan 13 07:46:52.634: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 14.026688949s
Jan 13 07:46:54.638: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 16.030705312s
Jan 13 07:46:56.642: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 18.034670002s
Jan 13 07:46:58.647: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 20.03935681s
Jan 13 07:47:00.651: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Running", Reason="", readiness=true. Elapsed: 22.04365315s
Jan 13 07:47:02.655: INFO: Pod "pod-subpath-test-secret-9jvv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.047460468s
STEP: Saw pod success
Jan 13 07:47:02.655: INFO: Pod "pod-subpath-test-secret-9jvv" satisfied condition "Succeeded or Failed"
Jan 13 07:47:02.657: INFO: Trying to get logs from node worker-s001 pod pod-subpath-test-secret-9jvv container test-container-subpath-secret-9jvv: <nil>
STEP: delete the pod
Jan 13 07:47:02.686: INFO: Waiting for pod pod-subpath-test-secret-9jvv to disappear
Jan 13 07:47:02.687: INFO: Pod pod-subpath-test-secret-9jvv no longer exists
STEP: Deleting pod pod-subpath-test-secret-9jvv
Jan 13 07:47:02.687: INFO: Deleting pod "pod-subpath-test-secret-9jvv" in namespace "subpath-7147"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:02.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7147" for this suite.

• [SLOW TEST:24.119 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":41,"skipped":578,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:02.697: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 13 07:47:02.742: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:04.745: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 13 07:47:04.757: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:06.760: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:08.761: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 13 07:47:08.763: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:08.763: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:08.837: INFO: Exec stderr: ""
Jan 13 07:47:08.837: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:08.908: INFO: Exec stderr: ""
Jan 13 07:47:08.908: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:08.908: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:08.981: INFO: Exec stderr: ""
Jan 13 07:47:08.981: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:08.981: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.053: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 13 07:47:09.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:09.053: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.125: INFO: Exec stderr: ""
Jan 13 07:47:09.125: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.196: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 13 07:47:09.196: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.274: INFO: Exec stderr: ""
Jan 13 07:47:09.274: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:09.274: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.355: INFO: Exec stderr: ""
Jan 13 07:47:09.355: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:09.355: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.420: INFO: Exec stderr: ""
Jan 13 07:47:09.420: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5217 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:47:09.420: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:47:09.490: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:09.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5217" for this suite.

• [SLOW TEST:6.802 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":42,"skipped":598,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:09.499: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jan 13 07:47:09.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-5521 cluster-info'
Jan 13 07:47:09.712: INFO: stderr: ""
Jan 13 07:47:09.712: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:09.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5521" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":43,"skipped":609,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:09.718: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 13 07:47:09.763: INFO: The status of Pod labelsupdate2415f9c9-d5dc-4a31-ad7b-ca7b94c05854 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:11.769: INFO: The status of Pod labelsupdate2415f9c9-d5dc-4a31-ad7b-ca7b94c05854 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:13.767: INFO: The status of Pod labelsupdate2415f9c9-d5dc-4a31-ad7b-ca7b94c05854 is Running (Ready = true)
Jan 13 07:47:14.282: INFO: Successfully updated pod "labelsupdate2415f9c9-d5dc-4a31-ad7b-ca7b94c05854"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:16.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2963" for this suite.

• [SLOW TEST:6.586 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":44,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:16.305: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:47:16.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb" in namespace "projected-6649" to be "Succeeded or Failed"
Jan 13 07:47:16.349: INFO: Pod "downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.529212ms
Jan 13 07:47:18.355: INFO: Pod "downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008652631s
STEP: Saw pod success
Jan 13 07:47:18.355: INFO: Pod "downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb" satisfied condition "Succeeded or Failed"
Jan 13 07:47:18.358: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb container client-container: <nil>
STEP: delete the pod
Jan 13 07:47:18.386: INFO: Waiting for pod downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb to disappear
Jan 13 07:47:18.390: INFO: Pod downwardapi-volume-24246e25-4d9c-4a2c-92e1-c8839a73c7bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:18.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6649" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":45,"skipped":648,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:18.395: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:47:18.421: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:21.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3272" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":46,"skipped":668,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:21.546: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-6478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6478 to expose endpoints map[]
Jan 13 07:47:21.617: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 13 07:47:22.621: INFO: successfully validated that service multi-endpoint-test in namespace services-6478 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6478
Jan 13 07:47:22.630: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:24.633: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:26.636: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6478 to expose endpoints map[pod1:[100]]
Jan 13 07:47:26.644: INFO: successfully validated that service multi-endpoint-test in namespace services-6478 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6478
Jan 13 07:47:26.657: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:47:28.660: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6478 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 13 07:47:28.670: INFO: successfully validated that service multi-endpoint-test in namespace services-6478 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-6478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6478 to expose endpoints map[pod2:[101]]
Jan 13 07:47:28.690: INFO: successfully validated that service multi-endpoint-test in namespace services-6478 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6478 to expose endpoints map[]
Jan 13 07:47:28.707: INFO: successfully validated that service multi-endpoint-test in namespace services-6478 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:28.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6478" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:7.184 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":47,"skipped":672,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:28.730: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jan 13 07:47:28.759: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jan 13 07:47:29.019: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 13 07:47:31.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777656849, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777656849, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777656849, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777656849, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 07:47:34.669: INFO: Waited 1.615803854s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 13 07:47:35.822: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:36.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3185" for this suite.

• [SLOW TEST:7.989 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":48,"skipped":681,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:36.719: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:47:44.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9510" for this suite.

• [SLOW TEST:8.044 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":49,"skipped":690,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:47:44.763: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-7975
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jan 13 07:47:44.821: INFO: Found 0 stateful pods, waiting for 3
Jan 13 07:47:54.825: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 07:47:54.825: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 07:47:54.825: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 07:47:54.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-7975 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 07:47:54.962: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 07:47:54.962: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 07:47:54.962: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jan 13 07:48:04.992: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 13 07:48:15.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-7975 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 07:48:15.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 07:48:15.132: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 07:48:15.132: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 07:48:35.148: INFO: Waiting for StatefulSet statefulset-7975/ss2 to complete update
Jan 13 07:48:35.148: INFO: Waiting for Pod statefulset-7975/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Jan 13 07:48:45.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-7975 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 07:48:45.297: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 07:48:45.297: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 07:48:45.297: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 07:48:55.321: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 13 07:49:05.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-7975 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 07:49:05.475: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 07:49:05.475: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 07:49:05.475: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 07:49:15.491: INFO: Waiting for StatefulSet statefulset-7975/ss2 to complete update
Jan 13 07:49:15.491: INFO: Waiting for Pod statefulset-7975/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jan 13 07:49:15.491: INFO: Waiting for Pod statefulset-7975/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jan 13 07:49:15.491: INFO: Waiting for Pod statefulset-7975/ss2-2 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jan 13 07:49:25.499: INFO: Waiting for StatefulSet statefulset-7975/ss2 to complete update
Jan 13 07:49:25.499: INFO: Waiting for Pod statefulset-7975/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jan 13 07:49:35.496: INFO: Deleting all statefulset in ns statefulset-7975
Jan 13 07:49:35.498: INFO: Scaling statefulset ss2 to 0
Jan 13 07:49:45.514: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 07:49:45.516: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:49:45.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7975" for this suite.

• [SLOW TEST:120.775 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":50,"skipped":701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:49:45.539: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:49:49.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1677" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":51,"skipped":750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:49:49.622: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9194
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9194
STEP: creating replication controller externalsvc in namespace services-9194
I0113 07:49:49.673789      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9194, replica count: 2
I0113 07:49:52.724763      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 13 07:49:52.736: INFO: Creating new exec pod
Jan 13 07:49:54.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9194 exec execpodpnzkr -- /bin/sh -x -c nslookup clusterip-service.services-9194.svc.cluster.local'
Jan 13 07:49:54.903: INFO: stderr: "+ nslookup clusterip-service.services-9194.svc.cluster.local\n"
Jan 13 07:49:54.903: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9194.svc.cluster.local\tcanonical name = externalsvc.services-9194.svc.cluster.local.\nName:\texternalsvc.services-9194.svc.cluster.local\nAddress: 10.96.137.38\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9194, will wait for the garbage collector to delete the pods
Jan 13 07:49:54.959: INFO: Deleting ReplicationController externalsvc took: 3.194114ms
Jan 13 07:49:55.059: INFO: Terminating ReplicationController externalsvc pods took: 100.368917ms
Jan 13 07:50:08.969: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:50:08.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9194" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:19.368 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":52,"skipped":777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:50:08.990: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:50:09.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5" in namespace "projected-5786" to be "Succeeded or Failed"
Jan 13 07:50:09.034: INFO: Pod "downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896533ms
Jan 13 07:50:11.038: INFO: Pod "downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00633922s
STEP: Saw pod success
Jan 13 07:50:11.038: INFO: Pod "downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5" satisfied condition "Succeeded or Failed"
Jan 13 07:50:11.039: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5 container client-container: <nil>
STEP: delete the pod
Jan 13 07:50:11.061: INFO: Waiting for pod downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5 to disappear
Jan 13 07:50:11.064: INFO: Pod downwardapi-volume-4a7c0c65-e4e3-4170-8406-86add723d3b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:50:11.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5786" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":53,"skipped":840,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:50:11.069: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0113 07:50:19.675909      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 13 07:51:21.689: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:51:21.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-703" for this suite.

• [SLOW TEST:70.634 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":54,"skipped":883,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:51:21.703: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:51:21.748: INFO: The status of Pod busybox-readonly-fs5172625d-ed43-43f7-8774-a9035653b468 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:51:23.752: INFO: The status of Pod busybox-readonly-fs5172625d-ed43-43f7-8774-a9035653b468 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:51:23.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9117" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":55,"skipped":887,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:51:23.772: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-9650
STEP: creating replication controller nodeport-test in namespace services-9650
I0113 07:51:23.818484      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9650, replica count: 2
I0113 07:51:26.869841      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:51:29.870: INFO: Creating new exec pod
I0113 07:51:29.870942      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:51:32.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9650 exec execpoddtvsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 13 07:51:33.013: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 13 07:51:33.013: INFO: stdout: ""
Jan 13 07:51:34.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9650 exec execpoddtvsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 13 07:51:34.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 13 07:51:34.147: INFO: stdout: "nodeport-test-gxbsd"
Jan 13 07:51:34.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9650 exec execpoddtvsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.108.236 80'
Jan 13 07:51:34.288: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.108.236 80\nConnection to 10.96.108.236 80 port [tcp/http] succeeded!\n"
Jan 13 07:51:34.288: INFO: stdout: ""
Jan 13 07:51:35.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9650 exec execpoddtvsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.108.236 80'
Jan 13 07:51:35.416: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.108.236 80\nConnection to 10.96.108.236 80 port [tcp/http] succeeded!\n"
Jan 13 07:51:35.416: INFO: stdout: "nodeport-test-gxbsd"
Jan 13 07:51:35.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9650 exec execpoddtvsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.59 30521'
Jan 13 07:51:35.564: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.59 30521\nConnection to 192.168.100.59 30521 port [tcp/*] succeeded!\n"
Jan 13 07:51:35.564: INFO: stdout: "nodeport-test-b2zcq"
Jan 13 07:51:35.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9650 exec execpoddtvsg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.60 30521'
Jan 13 07:51:35.696: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.60 30521\nConnection to 192.168.100.60 30521 port [tcp/*] succeeded!\n"
Jan 13 07:51:35.696: INFO: stdout: "nodeport-test-b2zcq"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:51:35.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9650" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:11.935 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":56,"skipped":899,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:51:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1064
Jan 13 07:51:35.762: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:51:37.767: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 13 07:51:37.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 13 07:51:37.892: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 13 07:51:37.892: INFO: stdout: "ipvs"
Jan 13 07:51:37.892: INFO: proxyMode: ipvs
Jan 13 07:51:37.914: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 13 07:51:37.916: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1064
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1064
I0113 07:51:37.936030      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1064, replica count: 3
I0113 07:51:40.987074      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:51:40.993: INFO: Creating new exec pod
Jan 13 07:51:46.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 13 07:51:46.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 13 07:51:46.153: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:51:46.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.60.133 80'
Jan 13 07:51:46.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.60.133 80\nConnection to 10.96.60.133 80 port [tcp/http] succeeded!\n"
Jan 13 07:51:46.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:51:46.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.59 30949'
Jan 13 07:51:46.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.59 30949\nConnection to 192.168.100.59 30949 port [tcp/*] succeeded!\n"
Jan 13 07:51:46.423: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:51:46.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.60 30949'
Jan 13 07:51:46.559: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.60 30949\nConnection to 192.168.100.60 30949 port [tcp/*] succeeded!\n"
Jan 13 07:51:46.559: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:51:46.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.59:30949/ ; done'
Jan 13 07:51:46.740: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n"
Jan 13 07:51:46.740: INFO: stdout: "\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg\naffinity-nodeport-timeout-k5mjg"
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Received response from host: affinity-nodeport-timeout-k5mjg
Jan 13 07:51:46.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.100.59:30949/'
Jan 13 07:51:46.882: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n"
Jan 13 07:51:46.882: INFO: stdout: "affinity-nodeport-timeout-k5mjg"
Jan 13 07:53:56.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1064 exec execpod-affinitygq9tq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.100.59:30949/'
Jan 13 07:53:57.017: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.100.59:30949/\n"
Jan 13 07:53:57.017: INFO: stdout: "affinity-nodeport-timeout-trtt9"
Jan 13 07:53:57.017: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1064, will wait for the garbage collector to delete the pods
Jan 13 07:53:57.089: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.831027ms
Jan 13 07:53:57.190: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.98561ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1064" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:153.313 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":57,"skipped":904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:09.020: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-8t8b
STEP: Creating a pod to test atomic-volume-subpath
Jan 13 07:54:09.079: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8t8b" in namespace "subpath-1687" to be "Succeeded or Failed"
Jan 13 07:54:09.081: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958009ms
Jan 13 07:54:11.086: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006244563s
Jan 13 07:54:13.093: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 4.013753994s
Jan 13 07:54:15.098: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 6.018597616s
Jan 13 07:54:17.105: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 8.025435484s
Jan 13 07:54:19.110: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 10.030399146s
Jan 13 07:54:21.114: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 12.03452078s
Jan 13 07:54:23.119: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 14.039750718s
Jan 13 07:54:25.123: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 16.043584512s
Jan 13 07:54:27.128: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 18.048324212s
Jan 13 07:54:29.130: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Running", Reason="", readiness=true. Elapsed: 20.050592469s
Jan 13 07:54:31.134: INFO: Pod "pod-subpath-test-configmap-8t8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054235753s
STEP: Saw pod success
Jan 13 07:54:31.134: INFO: Pod "pod-subpath-test-configmap-8t8b" satisfied condition "Succeeded or Failed"
Jan 13 07:54:31.135: INFO: Trying to get logs from node worker-s001 pod pod-subpath-test-configmap-8t8b container test-container-subpath-configmap-8t8b: <nil>
STEP: delete the pod
Jan 13 07:54:31.160: INFO: Waiting for pod pod-subpath-test-configmap-8t8b to disappear
Jan 13 07:54:31.163: INFO: Pod pod-subpath-test-configmap-8t8b no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8t8b
Jan 13 07:54:31.163: INFO: Deleting pod "pod-subpath-test-configmap-8t8b" in namespace "subpath-1687"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:31.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1687" for this suite.

• [SLOW TEST:22.150 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":58,"skipped":932,"failed":0}
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:31.170: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:54:31.206: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-da02aed4-2eae-43a1-b43c-ed07c8249849" in namespace "security-context-test-6077" to be "Succeeded or Failed"
Jan 13 07:54:31.209: INFO: Pod "alpine-nnp-false-da02aed4-2eae-43a1-b43c-ed07c8249849": Phase="Pending", Reason="", readiness=false. Elapsed: 3.190935ms
Jan 13 07:54:33.214: INFO: Pod "alpine-nnp-false-da02aed4-2eae-43a1-b43c-ed07c8249849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008074521s
Jan 13 07:54:33.214: INFO: Pod "alpine-nnp-false-da02aed4-2eae-43a1-b43c-ed07c8249849" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:33.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6077" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":59,"skipped":934,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:33.230: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 13 07:54:33.263: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:37.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9585" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":60,"skipped":945,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:37.686: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 13 07:54:39.748: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:41.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9975" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":61,"skipped":964,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:41.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9120" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":62,"skipped":969,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:41.832: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:54:41.907: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-907cb755-d23b-4c85-8b42-6842a1e135f5" in namespace "security-context-test-6224" to be "Succeeded or Failed"
Jan 13 07:54:41.910: INFO: Pod "busybox-privileged-false-907cb755-d23b-4c85-8b42-6842a1e135f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457019ms
Jan 13 07:54:43.913: INFO: Pod "busybox-privileged-false-907cb755-d23b-4c85-8b42-6842a1e135f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006184993s
Jan 13 07:54:43.913: INFO: Pod "busybox-privileged-false-907cb755-d23b-4c85-8b42-6842a1e135f5" satisfied condition "Succeeded or Failed"
Jan 13 07:54:43.917: INFO: Got logs for pod "busybox-privileged-false-907cb755-d23b-4c85-8b42-6842a1e135f5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:43.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6224" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":63,"skipped":973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:54:43.945: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 13 07:54:48.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 create -f -'
Jan 13 07:54:49.167: INFO: stderr: ""
Jan 13 07:54:49.167: INFO: stdout: "e2e-test-crd-publish-openapi-5052-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 13 07:54:49.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 delete e2e-test-crd-publish-openapi-5052-crds test-foo'
Jan 13 07:54:49.220: INFO: stderr: ""
Jan 13 07:54:49.220: INFO: stdout: "e2e-test-crd-publish-openapi-5052-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 13 07:54:49.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 apply -f -'
Jan 13 07:54:49.439: INFO: stderr: ""
Jan 13 07:54:49.439: INFO: stdout: "e2e-test-crd-publish-openapi-5052-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 13 07:54:49.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 delete e2e-test-crd-publish-openapi-5052-crds test-foo'
Jan 13 07:54:49.494: INFO: stderr: ""
Jan 13 07:54:49.494: INFO: stdout: "e2e-test-crd-publish-openapi-5052-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 13 07:54:49.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 create -f -'
Jan 13 07:54:49.694: INFO: rc: 1
Jan 13 07:54:49.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 apply -f -'
Jan 13 07:54:49.901: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 13 07:54:49.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 create -f -'
Jan 13 07:54:50.101: INFO: rc: 1
Jan 13 07:54:50.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 --namespace=crd-publish-openapi-7075 apply -f -'
Jan 13 07:54:50.308: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 13 07:54:50.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 explain e2e-test-crd-publish-openapi-5052-crds'
Jan 13 07:54:50.533: INFO: stderr: ""
Jan 13 07:54:50.533: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5052-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 13 07:54:50.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 explain e2e-test-crd-publish-openapi-5052-crds.metadata'
Jan 13 07:54:50.747: INFO: stderr: ""
Jan 13 07:54:50.747: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5052-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 13 07:54:50.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 explain e2e-test-crd-publish-openapi-5052-crds.spec'
Jan 13 07:54:50.987: INFO: stderr: ""
Jan 13 07:54:50.987: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5052-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 13 07:54:50.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 explain e2e-test-crd-publish-openapi-5052-crds.spec.bars'
Jan 13 07:54:51.183: INFO: stderr: ""
Jan 13 07:54:51.183: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5052-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 13 07:54:51.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-7075 explain e2e-test-crd-publish-openapi-5052-crds.spec.bars2'
Jan 13 07:54:51.386: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:55.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7075" for this suite.

• [SLOW TEST:11.782 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":64,"skipped":1000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:55.705: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 07:54:56.218: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 07:54:59.235: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:54:59.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9689" for this suite.
STEP: Destroying namespace "webhook-9689-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":65,"skipped":1041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:54:59.315: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7820
STEP: creating service affinity-nodeport in namespace services-7820
STEP: creating replication controller affinity-nodeport in namespace services-7820
I0113 07:54:59.370681      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7820, replica count: 3
I0113 07:55:02.422573      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:55:02.429: INFO: Creating new exec pod
Jan 13 07:55:05.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7820 exec execpod-affinity89qhl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 13 07:55:05.580: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 13 07:55:05.580: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:55:05.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7820 exec execpod-affinity89qhl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.252.92 80'
Jan 13 07:55:05.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.252.92 80\nConnection to 10.96.252.92 80 port [tcp/http] succeeded!\n"
Jan 13 07:55:05.731: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:55:05.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7820 exec execpod-affinity89qhl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.59 31210'
Jan 13 07:55:05.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.59 31210\nConnection to 192.168.100.59 31210 port [tcp/*] succeeded!\n"
Jan 13 07:55:05.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:55:05.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7820 exec execpod-affinity89qhl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.60 31210'
Jan 13 07:55:06.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.60 31210\nConnection to 192.168.100.60 31210 port [tcp/*] succeeded!\n"
Jan 13 07:55:06.024: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:55:06.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7820 exec execpod-affinity89qhl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.59:31210/ ; done'
Jan 13 07:55:06.192: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.59:31210/\n"
Jan 13 07:55:06.192: INFO: stdout: "\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85\naffinity-nodeport-mxp85"
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Received response from host: affinity-nodeport-mxp85
Jan 13 07:55:06.192: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7820, will wait for the garbage collector to delete the pods
Jan 13 07:55:06.269: INFO: Deleting ReplicationController affinity-nodeport took: 3.860293ms
Jan 13 07:55:06.370: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.835755ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:55:18.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7820" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:19.687 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":66,"skipped":1095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:55:19.002: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:55:19.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2580" for this suite.
STEP: Destroying namespace "nspatchtest-fd16cfe4-20bc-4300-a68e-62fb61a60f59-6104" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":67,"skipped":1127,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:55:19.066: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2317
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 13 07:55:19.087: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 07:55:19.112: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:55:21.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:23.117: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:25.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:27.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:29.117: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:31.117: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:33.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:35.116: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:37.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:39.118: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 07:55:41.117: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 13 07:55:41.121: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 13 07:55:43.149: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 07:55:43.149: INFO: Going to poll 10.10.28.22 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 13 07:55:43.150: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.28.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2317 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:55:43.150: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:55:44.239: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 13 07:55:44.239: INFO: Going to poll 10.10.131.174 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 13 07:55:44.241: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.131.174 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2317 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 07:55:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 07:55:45.319: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:55:45.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2317" for this suite.

• [SLOW TEST:26.269 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":68,"skipped":1127,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:55:45.335: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:55:45.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09" in namespace "downward-api-4738" to be "Succeeded or Failed"
Jan 13 07:55:45.374: INFO: Pod "downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.612974ms
Jan 13 07:55:47.379: INFO: Pod "downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009722952s
STEP: Saw pod success
Jan 13 07:55:47.379: INFO: Pod "downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09" satisfied condition "Succeeded or Failed"
Jan 13 07:55:47.381: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09 container client-container: <nil>
STEP: delete the pod
Jan 13 07:55:47.411: INFO: Waiting for pod downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09 to disappear
Jan 13 07:55:47.414: INFO: Pod downwardapi-volume-f286080d-6e9e-4a40-a1e0-053f6cc4bd09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:55:47.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4738" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":69,"skipped":1132,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:55:47.419: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0113 07:55:47.448141      24 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 13 07:55:47.454: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 13 07:55:47.457: INFO: starting watch
STEP: patching
STEP: updating
Jan 13 07:55:47.467: INFO: waiting for watch events with expected annotations
Jan 13 07:55:47.467: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:55:47.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7878" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":70,"skipped":1142,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:55:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:55:47.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5841" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":71,"skipped":1158,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:55:47.557: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0113 07:55:58.937900      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 13 07:57:00.951: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:00.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8920" for this suite.

• [SLOW TEST:73.403 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":72,"skipped":1159,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:00.961: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 13 07:57:01.263: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 07:57:04.282: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:57:04.283: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:07.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8762" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.505 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":73,"skipped":1177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:07.466: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:57:07.594: INFO: The status of Pod busybox-host-aliasesbef1b197-adc7-4104-873f-8e182725dd9e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:57:09.599: INFO: The status of Pod busybox-host-aliasesbef1b197-adc7-4104-873f-8e182725dd9e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:57:11.598: INFO: The status of Pod busybox-host-aliasesbef1b197-adc7-4104-873f-8e182725dd9e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:11.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1594" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":74,"skipped":1210,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:11.617: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-nrnt9 in namespace proxy-1808
I0113 07:57:11.658209      24 runners.go:190] Created replication controller with name: proxy-service-nrnt9, namespace: proxy-1808, replica count: 1
I0113 07:57:12.709925      24 runners.go:190] proxy-service-nrnt9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 07:57:13.710469      24 runners.go:190] proxy-service-nrnt9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:57:13.714: INFO: setup took 2.06734761s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 13 07:57:13.719: INFO: (0) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 5.268045ms)
Jan 13 07:57:13.721: INFO: (0) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 6.829554ms)
Jan 13 07:57:13.722: INFO: (0) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 7.796041ms)
Jan 13 07:57:13.722: INFO: (0) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 7.801972ms)
Jan 13 07:57:13.722: INFO: (0) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 7.938597ms)
Jan 13 07:57:13.722: INFO: (0) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 8.171706ms)
Jan 13 07:57:13.722: INFO: (0) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 8.567479ms)
Jan 13 07:57:13.722: INFO: (0) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 8.605711ms)
Jan 13 07:57:13.723: INFO: (0) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 8.797564ms)
Jan 13 07:57:13.723: INFO: (0) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 8.747885ms)
Jan 13 07:57:13.723: INFO: (0) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 8.692894ms)
Jan 13 07:57:13.724: INFO: (0) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 9.704553ms)
Jan 13 07:57:13.724: INFO: (0) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 9.708355ms)
Jan 13 07:57:13.724: INFO: (0) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 10.276918ms)
Jan 13 07:57:13.724: INFO: (0) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 10.677965ms)
Jan 13 07:57:13.726: INFO: (0) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 12.717273ms)
Jan 13 07:57:13.729: INFO: (1) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 2.239369ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.654131ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.726098ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.649165ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.727711ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.690251ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.755654ms)
Jan 13 07:57:13.731: INFO: (1) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.753757ms)
Jan 13 07:57:13.732: INFO: (1) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.938878ms)
Jan 13 07:57:13.732: INFO: (1) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 5.138344ms)
Jan 13 07:57:13.732: INFO: (1) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 5.386225ms)
Jan 13 07:57:13.732: INFO: (1) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.743565ms)
Jan 13 07:57:13.732: INFO: (1) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 5.8925ms)
Jan 13 07:57:13.733: INFO: (1) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 6.087867ms)
Jan 13 07:57:13.733: INFO: (1) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 6.325328ms)
Jan 13 07:57:13.733: INFO: (1) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 6.30693ms)
Jan 13 07:57:13.735: INFO: (2) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 2.187675ms)
Jan 13 07:57:13.736: INFO: (2) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 2.634814ms)
Jan 13 07:57:13.736: INFO: (2) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 2.85846ms)
Jan 13 07:57:13.737: INFO: (2) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 4.286906ms)
Jan 13 07:57:13.738: INFO: (2) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.53635ms)
Jan 13 07:57:13.738: INFO: (2) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.52209ms)
Jan 13 07:57:13.739: INFO: (2) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 5.395035ms)
Jan 13 07:57:13.739: INFO: (2) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 5.574534ms)
Jan 13 07:57:13.739: INFO: (2) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.560112ms)
Jan 13 07:57:13.739: INFO: (2) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 6.303833ms)
Jan 13 07:57:13.739: INFO: (2) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 6.273178ms)
Jan 13 07:57:13.740: INFO: (2) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 6.694092ms)
Jan 13 07:57:13.740: INFO: (2) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 6.626284ms)
Jan 13 07:57:13.740: INFO: (2) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 6.543892ms)
Jan 13 07:57:13.740: INFO: (2) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 6.866295ms)
Jan 13 07:57:13.740: INFO: (2) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 7.283534ms)
Jan 13 07:57:13.743: INFO: (3) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 2.358293ms)
Jan 13 07:57:13.743: INFO: (3) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 2.757626ms)
Jan 13 07:57:13.744: INFO: (3) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 3.306864ms)
Jan 13 07:57:13.744: INFO: (3) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 3.62508ms)
Jan 13 07:57:13.744: INFO: (3) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.962535ms)
Jan 13 07:57:13.744: INFO: (3) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.935384ms)
Jan 13 07:57:13.744: INFO: (3) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.035845ms)
Jan 13 07:57:13.744: INFO: (3) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.011764ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.035975ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.372187ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 4.539514ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.869683ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.866138ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.850976ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.849467ms)
Jan 13 07:57:13.745: INFO: (3) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.942118ms)
Jan 13 07:57:13.748: INFO: (4) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 2.434617ms)
Jan 13 07:57:13.748: INFO: (4) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 2.511273ms)
Jan 13 07:57:13.748: INFO: (4) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 2.790668ms)
Jan 13 07:57:13.748: INFO: (4) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 2.898851ms)
Jan 13 07:57:13.749: INFO: (4) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.423187ms)
Jan 13 07:57:13.749: INFO: (4) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 3.494057ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.11166ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 4.286089ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.540112ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.607162ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.586579ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.806492ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.826043ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.889533ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.878304ms)
Jan 13 07:57:13.750: INFO: (4) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.940206ms)
Jan 13 07:57:13.754: INFO: (5) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.101911ms)
Jan 13 07:57:13.754: INFO: (5) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 3.178364ms)
Jan 13 07:57:13.754: INFO: (5) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.687489ms)
Jan 13 07:57:13.754: INFO: (5) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.948978ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.404975ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.39242ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.365719ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 4.362594ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.360652ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.393395ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.419388ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.492265ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.552966ms)
Jan 13 07:57:13.755: INFO: (5) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.686726ms)
Jan 13 07:57:13.756: INFO: (5) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.714936ms)
Jan 13 07:57:13.756: INFO: (5) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 5.753452ms)
Jan 13 07:57:13.760: INFO: (6) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 3.773911ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.282224ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.400264ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.431176ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 4.589908ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.858728ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.886534ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.850717ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.829554ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 5.056873ms)
Jan 13 07:57:13.761: INFO: (6) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.084303ms)
Jan 13 07:57:13.762: INFO: (6) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 5.224121ms)
Jan 13 07:57:13.762: INFO: (6) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 5.320962ms)
Jan 13 07:57:13.762: INFO: (6) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 5.525743ms)
Jan 13 07:57:13.762: INFO: (6) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 5.537096ms)
Jan 13 07:57:13.762: INFO: (6) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 5.770792ms)
Jan 13 07:57:13.765: INFO: (7) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 2.467709ms)
Jan 13 07:57:13.765: INFO: (7) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 2.671334ms)
Jan 13 07:57:13.765: INFO: (7) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 2.914795ms)
Jan 13 07:57:13.765: INFO: (7) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 3.129371ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.448321ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.528736ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.787733ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.957209ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.958164ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 5.104551ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 5.064181ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 5.143229ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.15947ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.114109ms)
Jan 13 07:57:13.767: INFO: (7) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 5.136993ms)
Jan 13 07:57:13.768: INFO: (7) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 6.348963ms)
Jan 13 07:57:13.771: INFO: (8) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 2.498303ms)
Jan 13 07:57:13.771: INFO: (8) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 2.761118ms)
Jan 13 07:57:13.771: INFO: (8) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 2.955321ms)
Jan 13 07:57:13.772: INFO: (8) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.519888ms)
Jan 13 07:57:13.772: INFO: (8) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.489168ms)
Jan 13 07:57:13.772: INFO: (8) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.533987ms)
Jan 13 07:57:13.772: INFO: (8) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 3.687562ms)
Jan 13 07:57:13.772: INFO: (8) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 3.83064ms)
Jan 13 07:57:13.773: INFO: (8) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 4.37326ms)
Jan 13 07:57:13.773: INFO: (8) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.479449ms)
Jan 13 07:57:13.773: INFO: (8) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.522486ms)
Jan 13 07:57:13.774: INFO: (8) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.080933ms)
Jan 13 07:57:13.774: INFO: (8) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 5.170562ms)
Jan 13 07:57:13.774: INFO: (8) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 5.153053ms)
Jan 13 07:57:13.774: INFO: (8) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 5.177596ms)
Jan 13 07:57:13.775: INFO: (8) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 6.174001ms)
Jan 13 07:57:13.778: INFO: (9) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.480936ms)
Jan 13 07:57:13.779: INFO: (9) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.504234ms)
Jan 13 07:57:13.779: INFO: (9) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.56789ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.623119ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.838107ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.956735ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 4.994472ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 5.048134ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 5.120624ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 5.162292ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.229384ms)
Jan 13 07:57:13.780: INFO: (9) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.444737ms)
Jan 13 07:57:13.781: INFO: (9) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 5.643453ms)
Jan 13 07:57:13.781: INFO: (9) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 5.703787ms)
Jan 13 07:57:13.781: INFO: (9) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 5.826885ms)
Jan 13 07:57:13.781: INFO: (9) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 6.091452ms)
Jan 13 07:57:13.784: INFO: (10) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 2.612243ms)
Jan 13 07:57:13.784: INFO: (10) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 3.371774ms)
Jan 13 07:57:13.784: INFO: (10) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 3.343845ms)
Jan 13 07:57:13.784: INFO: (10) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.330671ms)
Jan 13 07:57:13.784: INFO: (10) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 3.3685ms)
Jan 13 07:57:13.785: INFO: (10) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.187911ms)
Jan 13 07:57:13.785: INFO: (10) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 4.228348ms)
Jan 13 07:57:13.785: INFO: (10) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.145974ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.655996ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.688482ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.84116ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.838399ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.819991ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.946189ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.986861ms)
Jan 13 07:57:13.786: INFO: (10) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.082645ms)
Jan 13 07:57:13.789: INFO: (11) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 2.302912ms)
Jan 13 07:57:13.789: INFO: (11) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 2.525198ms)
Jan 13 07:57:13.789: INFO: (11) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 2.888076ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.343592ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 3.200374ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 3.517208ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 3.469534ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 3.737474ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.092922ms)
Jan 13 07:57:13.790: INFO: (11) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.117538ms)
Jan 13 07:57:13.791: INFO: (11) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.099822ms)
Jan 13 07:57:13.791: INFO: (11) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.255199ms)
Jan 13 07:57:13.791: INFO: (11) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.668998ms)
Jan 13 07:57:13.791: INFO: (11) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.914694ms)
Jan 13 07:57:13.792: INFO: (11) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.283635ms)
Jan 13 07:57:13.792: INFO: (11) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.339816ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 3.831215ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.831328ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 3.777408ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 3.855844ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 3.799292ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 3.803324ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 3.857275ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.883746ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 3.923395ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.146156ms)
Jan 13 07:57:13.796: INFO: (12) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.187047ms)
Jan 13 07:57:13.797: INFO: (12) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.89281ms)
Jan 13 07:57:13.797: INFO: (12) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 5.022093ms)
Jan 13 07:57:13.797: INFO: (12) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.090653ms)
Jan 13 07:57:13.797: INFO: (12) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 5.233938ms)
Jan 13 07:57:13.797: INFO: (12) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 5.229642ms)
Jan 13 07:57:13.801: INFO: (13) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.143714ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 5.6109ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 5.826696ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 5.755442ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 5.781464ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 5.687629ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 5.767448ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 5.752885ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 5.723778ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.99836ms)
Jan 13 07:57:13.803: INFO: (13) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 6.248499ms)
Jan 13 07:57:13.804: INFO: (13) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 6.2939ms)
Jan 13 07:57:13.804: INFO: (13) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 6.446087ms)
Jan 13 07:57:13.804: INFO: (13) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 6.654814ms)
Jan 13 07:57:13.804: INFO: (13) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 6.690878ms)
Jan 13 07:57:13.804: INFO: (13) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 6.76933ms)
Jan 13 07:57:13.808: INFO: (14) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 3.505739ms)
Jan 13 07:57:13.808: INFO: (14) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 3.489853ms)
Jan 13 07:57:13.808: INFO: (14) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 3.49103ms)
Jan 13 07:57:13.808: INFO: (14) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.509221ms)
Jan 13 07:57:13.808: INFO: (14) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 3.722542ms)
Jan 13 07:57:13.809: INFO: (14) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 5.030943ms)
Jan 13 07:57:13.809: INFO: (14) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 5.196161ms)
Jan 13 07:57:13.810: INFO: (14) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 5.624719ms)
Jan 13 07:57:13.810: INFO: (14) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 5.61604ms)
Jan 13 07:57:13.810: INFO: (14) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.747599ms)
Jan 13 07:57:13.815: INFO: (14) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 10.722598ms)
Jan 13 07:57:13.815: INFO: (14) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 10.965829ms)
Jan 13 07:57:13.819: INFO: (14) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 15.400341ms)
Jan 13 07:57:13.820: INFO: (14) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 15.434287ms)
Jan 13 07:57:13.820: INFO: (14) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 15.509144ms)
Jan 13 07:57:13.820: INFO: (14) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 15.545016ms)
Jan 13 07:57:13.823: INFO: (15) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.403528ms)
Jan 13 07:57:13.823: INFO: (15) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 3.645059ms)
Jan 13 07:57:13.824: INFO: (15) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.84573ms)
Jan 13 07:57:13.824: INFO: (15) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.042337ms)
Jan 13 07:57:13.824: INFO: (15) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.306361ms)
Jan 13 07:57:13.824: INFO: (15) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.478041ms)
Jan 13 07:57:13.824: INFO: (15) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.531391ms)
Jan 13 07:57:13.824: INFO: (15) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.62658ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 6.286166ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 6.280172ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 6.275842ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 6.292755ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 6.367383ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 6.29083ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 6.33706ms)
Jan 13 07:57:13.826: INFO: (15) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 6.289832ms)
Jan 13 07:57:13.829: INFO: (16) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 3.065486ms)
Jan 13 07:57:13.829: INFO: (16) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.250667ms)
Jan 13 07:57:13.829: INFO: (16) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.223842ms)
Jan 13 07:57:13.829: INFO: (16) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.311192ms)
Jan 13 07:57:13.830: INFO: (16) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.121612ms)
Jan 13 07:57:13.830: INFO: (16) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.150686ms)
Jan 13 07:57:13.830: INFO: (16) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.315832ms)
Jan 13 07:57:13.830: INFO: (16) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.238212ms)
Jan 13 07:57:13.830: INFO: (16) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.319938ms)
Jan 13 07:57:13.830: INFO: (16) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 4.28571ms)
Jan 13 07:57:13.831: INFO: (16) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.398399ms)
Jan 13 07:57:13.831: INFO: (16) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.539723ms)
Jan 13 07:57:13.831: INFO: (16) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.577173ms)
Jan 13 07:57:13.831: INFO: (16) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.652502ms)
Jan 13 07:57:13.831: INFO: (16) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.693803ms)
Jan 13 07:57:13.831: INFO: (16) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 4.708243ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 2.85171ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 2.926393ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 3.358396ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.419498ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.399355ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 3.455214ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 3.392408ms)
Jan 13 07:57:13.834: INFO: (17) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.432863ms)
Jan 13 07:57:13.835: INFO: (17) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.124967ms)
Jan 13 07:57:13.835: INFO: (17) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 4.207415ms)
Jan 13 07:57:13.836: INFO: (17) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.60514ms)
Jan 13 07:57:13.836: INFO: (17) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.736996ms)
Jan 13 07:57:13.836: INFO: (17) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.744314ms)
Jan 13 07:57:13.836: INFO: (17) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.806293ms)
Jan 13 07:57:13.836: INFO: (17) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 4.99437ms)
Jan 13 07:57:13.836: INFO: (17) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 5.083482ms)
Jan 13 07:57:13.839: INFO: (18) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 2.987503ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.476878ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.63976ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.698678ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.588723ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.657981ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 4.634477ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.64983ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 4.683487ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.733709ms)
Jan 13 07:57:13.841: INFO: (18) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 5.33896ms)
Jan 13 07:57:13.845: INFO: (18) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 8.951497ms)
Jan 13 07:57:13.845: INFO: (18) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 8.936659ms)
Jan 13 07:57:13.845: INFO: (18) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 8.949928ms)
Jan 13 07:57:13.845: INFO: (18) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 8.99024ms)
Jan 13 07:57:13.845: INFO: (18) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 9.025548ms)
Jan 13 07:57:13.847: INFO: (19) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:443/proxy/tlsrewritem... (200; 1.878679ms)
Jan 13 07:57:13.848: INFO: (19) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">... (200; 2.722031ms)
Jan 13 07:57:13.848: INFO: (19) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:1080/proxy/rewriteme">test<... (200; 3.289989ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/: <a href="/api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs/proxy/rewriteme">test</a> (200; 3.320657ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 3.664253ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 3.920753ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:460/proxy/: tls baz (200; 4.003216ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname2/proxy/: tls qux (200; 4.089974ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname2/proxy/: bar (200; 4.089779ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:162/proxy/: bar (200; 4.056045ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/http:proxy-service-nrnt9-spdjs:160/proxy/: foo (200; 4.049304ms)
Jan 13 07:57:13.849: INFO: (19) /api/v1/namespaces/proxy-1808/pods/https:proxy-service-nrnt9-spdjs:462/proxy/: tls qux (200; 4.266341ms)
Jan 13 07:57:13.850: INFO: (19) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname2/proxy/: bar (200; 4.71654ms)
Jan 13 07:57:13.850: INFO: (19) /api/v1/namespaces/proxy-1808/services/http:proxy-service-nrnt9:portname1/proxy/: foo (200; 4.750833ms)
Jan 13 07:57:13.850: INFO: (19) /api/v1/namespaces/proxy-1808/services/https:proxy-service-nrnt9:tlsportname1/proxy/: tls baz (200; 4.845139ms)
Jan 13 07:57:13.851: INFO: (19) /api/v1/namespaces/proxy-1808/services/proxy-service-nrnt9:portname1/proxy/: foo (200; 5.63272ms)
STEP: deleting ReplicationController proxy-service-nrnt9 in namespace proxy-1808, will wait for the garbage collector to delete the pods
Jan 13 07:57:13.906: INFO: Deleting ReplicationController proxy-service-nrnt9 took: 3.120205ms
Jan 13 07:57:14.006: INFO: Terminating ReplicationController proxy-service-nrnt9 pods took: 100.483634ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1808" for this suite.

• [SLOW TEST:6.098 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":75,"skipped":1218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jan 13 07:57:17.795: INFO: Waiting up to 5m0s for pod "test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5" in namespace "svcaccounts-9826" to be "Succeeded or Failed"
Jan 13 07:57:17.800: INFO: Pod "test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184751ms
Jan 13 07:57:19.806: INFO: Pod "test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010321542s
STEP: Saw pod success
Jan 13 07:57:19.806: INFO: Pod "test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5" satisfied condition "Succeeded or Failed"
Jan 13 07:57:19.808: INFO: Trying to get logs from node worker-s001 pod test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 07:57:19.818: INFO: Waiting for pod test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5 to disappear
Jan 13 07:57:19.828: INFO: Pod test-pod-fd36016e-2f3a-4fad-8692-1a86a53cd8a5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:19.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9826" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":76,"skipped":1299,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:19.832: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:21.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6693" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":77,"skipped":1309,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:21.894: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:57:21.943: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6" in namespace "downward-api-1707" to be "Succeeded or Failed"
Jan 13 07:57:21.952: INFO: Pod "downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.275615ms
Jan 13 07:57:23.955: INFO: Pod "downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012399446s
STEP: Saw pod success
Jan 13 07:57:23.955: INFO: Pod "downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6" satisfied condition "Succeeded or Failed"
Jan 13 07:57:23.957: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6 container client-container: <nil>
STEP: delete the pod
Jan 13 07:57:23.970: INFO: Waiting for pod downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6 to disappear
Jan 13 07:57:23.973: INFO: Pod downwardapi-volume-d922fd56-6c58-4840-b70e-b153bb6fd0c6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1707" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":78,"skipped":1309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:23.981: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jan 13 07:57:24.013: INFO: created test-event-1
Jan 13 07:57:24.018: INFO: created test-event-2
Jan 13 07:57:24.022: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 13 07:57:24.027: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 13 07:57:24.039: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:24.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2857" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":79,"skipped":1339,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:24.048: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:57:24.121: INFO: The status of Pod busybox-scheduling-cd3a4d76-f80c-41ba-8886-e88ac669943f is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:57:26.125: INFO: The status of Pod busybox-scheduling-cd3a4d76-f80c-41ba-8886-e88ac669943f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:26.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-789" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":80,"skipped":1341,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:26.140: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:26.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3968" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":81,"skipped":1350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:26.176: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:57:26.223: INFO: The status of Pod pod-secrets-c691f898-ca70-42b5-b4e9-000cab06f175 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:57:28.228: INFO: The status of Pod pod-secrets-c691f898-ca70-42b5-b4e9-000cab06f175 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3937" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":82,"skipped":1385,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:28.264: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 13 07:57:28.299: INFO: Waiting up to 5m0s for pod "security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2" in namespace "security-context-5113" to be "Succeeded or Failed"
Jan 13 07:57:28.301: INFO: Pod "security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255719ms
Jan 13 07:57:30.305: INFO: Pod "security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005557379s
STEP: Saw pod success
Jan 13 07:57:30.305: INFO: Pod "security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2" satisfied condition "Succeeded or Failed"
Jan 13 07:57:30.307: INFO: Trying to get logs from node worker-s001 pod security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2 container test-container: <nil>
STEP: delete the pod
Jan 13 07:57:30.321: INFO: Waiting for pod security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2 to disappear
Jan 13 07:57:30.332: INFO: Pod security-context-94c5b212-0b1f-4c19-8334-9fd204e924e2 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:30.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5113" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":83,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:30.340: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 07:57:30.727: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 07:57:33.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:57:33.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7486" for this suite.
STEP: Destroying namespace "webhook-7486-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":84,"skipped":1443,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:57:33.894: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:01.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1931" for this suite.

• [SLOW TEST:28.088 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":85,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:01.982: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 13 07:58:04.529: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3087 pod-service-account-1c6c2d9a-21ea-4fd1-a691-3e3e32ddb4d3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 13 07:58:04.655: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3087 pod-service-account-1c6c2d9a-21ea-4fd1-a691-3e3e32ddb4d3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 13 07:58:04.786: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3087 pod-service-account-1c6c2d9a-21ea-4fd1-a691-3e3e32ddb4d3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:04.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3087" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":86,"skipped":1471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:04.919: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 07:58:04.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264" in namespace "projected-6680" to be "Succeeded or Failed"
Jan 13 07:58:04.953: INFO: Pod "downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264": Phase="Pending", Reason="", readiness=false. Elapsed: 3.787562ms
Jan 13 07:58:06.956: INFO: Pod "downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00682671s
STEP: Saw pod success
Jan 13 07:58:06.956: INFO: Pod "downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264" satisfied condition "Succeeded or Failed"
Jan 13 07:58:06.958: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264 container client-container: <nil>
STEP: delete the pod
Jan 13 07:58:06.974: INFO: Waiting for pod downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264 to disappear
Jan 13 07:58:06.977: INFO: Pod downwardapi-volume-f5dfed41-aeb6-4b7e-8eca-9e00bd404264 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:06.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6680" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":87,"skipped":1517,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:06.982: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 13 07:58:07.037: INFO: Waiting up to 5m0s for pod "downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768" in namespace "downward-api-6460" to be "Succeeded or Failed"
Jan 13 07:58:07.056: INFO: Pod "downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768": Phase="Pending", Reason="", readiness=false. Elapsed: 19.440124ms
Jan 13 07:58:09.060: INFO: Pod "downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022909352s
STEP: Saw pod success
Jan 13 07:58:09.060: INFO: Pod "downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768" satisfied condition "Succeeded or Failed"
Jan 13 07:58:09.061: INFO: Trying to get logs from node worker-s001 pod downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768 container dapi-container: <nil>
STEP: delete the pod
Jan 13 07:58:09.077: INFO: Waiting for pod downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768 to disappear
Jan 13 07:58:09.084: INFO: Pod downward-api-4507ee11-a2f4-40a2-8c84-e61c1397c768 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:09.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6460" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":88,"skipped":1518,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:09.088: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 13 07:58:09.118: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 07:58:09.122: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 07:58:09.124: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Jan 13 07:58:09.130: INFO: calico-node-vbqlj from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 07:58:09.130: INFO: csi-qingcloud-node-hxljj from kube-system started at 2022-01-13 07:38:08 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 07:58:09.130: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 07:58:09.130: INFO: kube-proxy-r2zxx from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 07:58:09.130: INFO: node-exporter-987g4 from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:58:09.130: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 07:58:09.130: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2022-01-13 07:38:48 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:58:09.130: INFO: 	Container prometheus ready: true, restart count 1
Jan 13 07:58:09.130: INFO: sonobuoy from sonobuoy started at 2022-01-13 07:33:31 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 07:58:09.130: INFO: sonobuoy-e2e-job-3cc758d0b1b14270 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container e2e ready: true, restart count 0
Jan 13 07:58:09.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 07:58:09.130: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-shz85 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 07:58:09.130: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 07:58:09.130: INFO: pod-service-account-1c6c2d9a-21ea-4fd1-a691-3e3e32ddb4d3 from svcaccounts-3087 started at 2022-01-13 07:58:02 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.130: INFO: 	Container test ready: true, restart count 0
Jan 13 07:58:09.130: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Jan 13 07:58:09.143: INFO: calico-node-nvdsl from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 07:58:09.143: INFO: csi-qingcloud-controller-6f8f6458f7-vkmtw from kube-system started at 2022-01-12 18:28:18 +0000 UTC (5 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 13 07:58:09.143: INFO: csi-qingcloud-node-84m8s from kube-system started at 2022-01-12 06:37:21 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 07:58:09.143: INFO: kube-proxy-gdx9n from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 07:58:09.143: INFO: metrics-server-687cb5444-gwn4h from kube-system started at 2022-01-12 06:37:45 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container metrics-server ready: true, restart count 0
Jan 13 07:58:09.143: INFO: snapshot-controller-0 from kube-system started at 2022-01-12 06:38:20 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 13 07:58:09.143: INFO: default-http-backend-5bf68ff9b8-gt7w6 from kubesphere-controls-system started at 2022-01-12 06:39:06 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 13 07:58:09.143: INFO: kubectl-admin-6667774bb-lfhx9 from kubesphere-controls-system started at 2022-01-12 06:41:27 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container kubectl ready: true, restart count 0
Jan 13 07:58:09.143: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2022-01-12 18:28:22 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:58:09.143: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2022-01-12 06:40:50 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:58:09.143: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2022-01-12 17:16:42 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.143: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 07:58:09.143: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:58:09.143: INFO: kube-state-metrics-5547ddd4cc-79dkj from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (3 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 13 07:58:09.144: INFO: node-exporter-wjvtx from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 07:58:09.144: INFO: notification-manager-deployment-78664576cb-7cqhm from kubesphere-monitoring-system started at 2022-01-12 18:28:18 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container tenant ready: true, restart count 0
Jan 13 07:58:09.144: INFO: notification-manager-deployment-78664576cb-pppk6 from kubesphere-monitoring-system started at 2022-01-12 06:41:16 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container tenant ready: true, restart count 0
Jan 13 07:58:09.144: INFO: notification-manager-operator-7d44854f54-97b8n from kubesphere-monitoring-system started at 2022-01-12 06:41:10 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container notification-manager-operator ready: true, restart count 0
Jan 13 07:58:09.144: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2022-01-12 06:41:05 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container prometheus ready: true, restart count 0
Jan 13 07:58:09.144: INFO: prometheus-operator-5c5db79546-gl2sj from kubesphere-monitoring-system started at 2022-01-12 06:40:40 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 13 07:58:09.144: INFO: ks-installer-6cf7f47896-d9fqp from kubesphere-system started at 2022-01-12 06:37:23 +0000 UTC (1 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container installer ready: true, restart count 0
Jan 13 07:58:09.144: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-6f2nd from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 07:58:09.144: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 07:58:09.144: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16c9c57f2d438c0d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:10.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9484" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":89,"skipped":1523,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:10.187: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:25.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3286" for this suite.
STEP: Destroying namespace "nsdeletetest-2276" for this suite.
Jan 13 07:58:25.288: INFO: Namespace nsdeletetest-2276 was already deleted
STEP: Destroying namespace "nsdeletetest-3079" for this suite.

• [SLOW TEST:15.105 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":90,"skipped":1527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:25.292: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:58:25.330: INFO: The status of Pod server-envvars-f91c2da2-0c12-4924-a778-94c25302d60e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:58:27.337: INFO: The status of Pod server-envvars-f91c2da2-0c12-4924-a778-94c25302d60e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:58:29.334: INFO: The status of Pod server-envvars-f91c2da2-0c12-4924-a778-94c25302d60e is Running (Ready = true)
Jan 13 07:58:29.351: INFO: Waiting up to 5m0s for pod "client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01" in namespace "pods-7046" to be "Succeeded or Failed"
Jan 13 07:58:29.355: INFO: Pod "client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50467ms
Jan 13 07:58:31.359: INFO: Pod "client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007348768s
Jan 13 07:58:33.362: INFO: Pod "client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011087871s
STEP: Saw pod success
Jan 13 07:58:33.362: INFO: Pod "client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01" satisfied condition "Succeeded or Failed"
Jan 13 07:58:33.364: INFO: Trying to get logs from node worker-s001 pod client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01 container env3cont: <nil>
STEP: delete the pod
Jan 13 07:58:33.382: INFO: Waiting for pod client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01 to disappear
Jan 13 07:58:33.385: INFO: Pod client-envvars-4d2ad8bd-47d2-4cdf-9103-cc03db73cb01 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:33.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7046" for this suite.

• [SLOW TEST:8.098 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":91,"skipped":1562,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:33.390: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:58:33.413: INFO: Creating deployment "test-recreate-deployment"
Jan 13 07:58:33.416: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 13 07:58:33.423: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 13 07:58:35.428: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 13 07:58:35.430: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 13 07:58:35.436: INFO: Updating deployment test-recreate-deployment
Jan 13 07:58:35.436: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 07:58:35.511: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9883  acf759c7-7cb7-4b88-885b-541915e4caf4 392111 2 2022-01-13 07:58:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-13 07:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 07:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056634f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-01-13 07:58:35 +0000 UTC,LastTransitionTime:2022-01-13 07:58:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-01-13 07:58:35 +0000 UTC,LastTransitionTime:2022-01-13 07:58:33 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 13 07:58:35.513: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-9883  b9e6dad0-cf53-4cfb-8f61-ab697e9c26e0 392109 1 2022-01-13 07:58:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment acf759c7-7cb7-4b88-885b-541915e4caf4 0xc0056639b0 0xc0056639b1}] []  [{kube-controller-manager Update apps/v1 2022-01-13 07:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acf759c7-7cb7-4b88-885b-541915e4caf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005663a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 07:58:35.513: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 13 07:58:35.513: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-9883  cba91ae5-b25e-4432-ae67-0a72a0d143b2 392098 2 2022-01-13 07:58:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment acf759c7-7cb7-4b88-885b-541915e4caf4 0xc0056638a7 0xc0056638a8}] []  [{kube-controller-manager Update apps/v1 2022-01-13 07:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acf759c7-7cb7-4b88-885b-541915e4caf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005663938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 07:58:35.516: INFO: Pod "test-recreate-deployment-85d47dcb4-pq2gb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-pq2gb test-recreate-deployment-85d47dcb4- deployment-9883  4fd16e3b-ac34-491f-8040-66f05d4f1eb5 392108 0 2022-01-13 07:58:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 b9e6dad0-cf53-4cfb-8f61-ab697e9c26e0 0xc0053a4170 0xc0053a4171}] []  [{kube-controller-manager Update v1 2022-01-13 07:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9e6dad0-cf53-4cfb-8f61-ab697e9c26e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-01-13 07:58:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rr22h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rr22h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:,StartTime:2022-01-13 07:58:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:35.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9883" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":92,"skipped":1568,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:35.521: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-f144ef17-dd14-40f3-9cab-e77e015bf538
STEP: Creating a pod to test consume secrets
Jan 13 07:58:35.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f" in namespace "projected-5241" to be "Succeeded or Failed"
Jan 13 07:58:35.557: INFO: Pod "pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159797ms
Jan 13 07:58:37.562: INFO: Pod "pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00642459s
Jan 13 07:58:39.567: INFO: Pod "pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011581365s
STEP: Saw pod success
Jan 13 07:58:39.567: INFO: Pod "pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f" satisfied condition "Succeeded or Failed"
Jan 13 07:58:39.569: INFO: Trying to get logs from node worker-s001 pod pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 13 07:58:39.592: INFO: Waiting for pod pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f to disappear
Jan 13 07:58:39.596: INFO: Pod pod-projected-secrets-11ed577f-a974-4af2-94f7-956bfb94637f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:39.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5241" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":93,"skipped":1578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:39.602: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 13 07:58:39.642: INFO: starting watch
STEP: patching
STEP: updating
Jan 13 07:58:39.647: INFO: waiting for watch events with expected annotations
Jan 13 07:58:39.647: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:39.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9479" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":94,"skipped":1613,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:39.660: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:39.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2716" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":95,"skipped":1621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:39.704: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 13 07:58:39.754: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5880  26efddc7-80a0-4fc8-9f33-1718428c642b 392221 0 2022-01-13 07:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-13 07:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 07:58:39.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5880  26efddc7-80a0-4fc8-9f33-1718428c642b 392222 0 2022-01-13 07:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-13 07:58:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 07:58:39.755: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5880  26efddc7-80a0-4fc8-9f33-1718428c642b 392223 0 2022-01-13 07:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-13 07:58:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 13 07:58:49.772: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5880  26efddc7-80a0-4fc8-9f33-1718428c642b 392309 0 2022-01-13 07:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-13 07:58:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 07:58:49.773: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5880  26efddc7-80a0-4fc8-9f33-1718428c642b 392310 0 2022-01-13 07:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-13 07:58:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 07:58:49.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5880  26efddc7-80a0-4fc8-9f33-1718428c642b 392311 0 2022-01-13 07:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-13 07:58:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:49.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5880" for this suite.

• [SLOW TEST:10.073 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":96,"skipped":1651,"failed":0}
SS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:49.777: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 13 07:58:53.827: INFO: &Pod{ObjectMeta:{send-events-3353f79b-8d28-4d72-9684-9b04f080a179  events-5354  1cb4eb84-8d8f-47d8-855c-035b26f787f4 392344 0 2022-01-13 07:58:49 +0000 UTC <nil> <nil> map[name:foo time:809534467] map[cni.projectcalico.org/podIP:10.10.28.62/32 cni.projectcalico.org/podIPs:10.10.28.62/32] [] []  [{e2e.test Update v1 2022-01-13 07:58:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 07:58:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 07:58:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tv42b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tv42b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 07:58:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.62,StartTime:2022-01-13 07:58:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 07:58:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900,ContainerID:docker://9556619abf8bdf42f25417227068ea4508bfefec1ec50d587bd708af8ecb3967,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan 13 07:58:55.832: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 13 07:58:57.839: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:58:57.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5354" for this suite.

• [SLOW TEST:8.086 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":97,"skipped":1653,"failed":0}
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:58:57.863: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:58:57.898: INFO: Waiting up to 5m0s for pod "busybox-user-65534-22d6e9b0-d7c7-4fbd-92d7-28d802a4386a" in namespace "security-context-test-3755" to be "Succeeded or Failed"
Jan 13 07:58:57.901: INFO: Pod "busybox-user-65534-22d6e9b0-d7c7-4fbd-92d7-28d802a4386a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479332ms
Jan 13 07:58:59.907: INFO: Pod "busybox-user-65534-22d6e9b0-d7c7-4fbd-92d7-28d802a4386a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009235716s
Jan 13 07:59:01.912: INFO: Pod "busybox-user-65534-22d6e9b0-d7c7-4fbd-92d7-28d802a4386a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014164912s
Jan 13 07:59:01.912: INFO: Pod "busybox-user-65534-22d6e9b0-d7c7-4fbd-92d7-28d802a4386a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:01.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3755" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":98,"skipped":1653,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:01.918: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 07:59:02.290: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 07:59:05.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:59:05.317: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:08.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4129" for this suite.
STEP: Destroying namespace "webhook-4129-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.633 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":99,"skipped":1659,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:08.551: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 13 07:59:08.752: INFO: Waiting up to 5m0s for pod "pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e" in namespace "emptydir-9904" to be "Succeeded or Failed"
Jan 13 07:59:08.784: INFO: Pod "pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 31.981331ms
Jan 13 07:59:10.787: INFO: Pod "pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035588075s
Jan 13 07:59:12.793: INFO: Pod "pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041369628s
STEP: Saw pod success
Jan 13 07:59:12.793: INFO: Pod "pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e" satisfied condition "Succeeded or Failed"
Jan 13 07:59:12.795: INFO: Trying to get logs from node worker-s001 pod pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e container test-container: <nil>
STEP: delete the pod
Jan 13 07:59:12.808: INFO: Waiting for pod pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e to disappear
Jan 13 07:59:12.811: INFO: Pod pod-28729a02-7ed9-4b43-abe2-9244c8c18e7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:12.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9904" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":100,"skipped":1664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:12.816: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-51e3fc3f-06e7-456f-9313-5a5d6edbebdb
STEP: Creating a pod to test consume secrets
Jan 13 07:59:12.856: INFO: Waiting up to 5m0s for pod "pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052" in namespace "secrets-5949" to be "Succeeded or Failed"
Jan 13 07:59:12.858: INFO: Pod "pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052": Phase="Pending", Reason="", readiness=false. Elapsed: 1.421931ms
Jan 13 07:59:14.862: INFO: Pod "pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005369502s
STEP: Saw pod success
Jan 13 07:59:14.862: INFO: Pod "pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052" satisfied condition "Succeeded or Failed"
Jan 13 07:59:14.863: INFO: Trying to get logs from node worker-s001 pod pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052 container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 07:59:14.878: INFO: Waiting for pod pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052 to disappear
Jan 13 07:59:14.880: INFO: Pod pod-secrets-d32c823d-56f2-4766-bd70-5d19dbfad052 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:14.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5949" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":101,"skipped":1687,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:14.888: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-2c7173ea-6af9-4d5a-b7c3-1b66a4845ac5
STEP: Creating secret with name s-test-opt-upd-b2f4dde5-ac42-4cd6-8ff2-f9ccc84f2329
STEP: Creating the pod
Jan 13 07:59:14.932: INFO: The status of Pod pod-projected-secrets-b8aa47d9-ce89-4dfd-b473-d9e72b214cb6 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:59:16.935: INFO: The status of Pod pod-projected-secrets-b8aa47d9-ce89-4dfd-b473-d9e72b214cb6 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-2c7173ea-6af9-4d5a-b7c3-1b66a4845ac5
STEP: Updating secret s-test-opt-upd-b2f4dde5-ac42-4cd6-8ff2-f9ccc84f2329
STEP: Creating secret with name s-test-opt-create-8345defb-5a61-40ed-95f6-893255292314
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:18.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4725" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":102,"skipped":1692,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:19.007: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 13 07:59:19.043: INFO: Waiting up to 5m0s for pod "downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0" in namespace "downward-api-6849" to be "Succeeded or Failed"
Jan 13 07:59:19.045: INFO: Pod "downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.921411ms
Jan 13 07:59:21.047: INFO: Pod "downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004184276s
STEP: Saw pod success
Jan 13 07:59:21.048: INFO: Pod "downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0" satisfied condition "Succeeded or Failed"
Jan 13 07:59:21.049: INFO: Trying to get logs from node worker-s001 pod downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0 container dapi-container: <nil>
STEP: delete the pod
Jan 13 07:59:21.068: INFO: Waiting for pod downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0 to disappear
Jan 13 07:59:21.072: INFO: Pod downward-api-6684baf3-a2f0-4eeb-9aa1-58997193a5b0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:21.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6849" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":103,"skipped":1710,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:21.079: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 07:59:21.117: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 07:59:26.120: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 13 07:59:26.126: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 13 07:59:26.134: INFO: observed ReplicaSet test-rs in namespace replicaset-886 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 07:59:26.146: INFO: observed ReplicaSet test-rs in namespace replicaset-886 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 07:59:26.173: INFO: observed ReplicaSet test-rs in namespace replicaset-886 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 07:59:26.184: INFO: observed ReplicaSet test-rs in namespace replicaset-886 with ReadyReplicas 1, AvailableReplicas 1
Jan 13 07:59:28.081: INFO: observed ReplicaSet test-rs in namespace replicaset-886 with ReadyReplicas 2, AvailableReplicas 2
Jan 13 07:59:28.130: INFO: observed Replicaset test-rs in namespace replicaset-886 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 07:59:28.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-886" for this suite.

• [SLOW TEST:7.057 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":104,"skipped":1717,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 07:59:28.136: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7542
Jan 13 07:59:28.171: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 13 07:59:30.180: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 13 07:59:30.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7542 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 13 07:59:30.334: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 13 07:59:30.334: INFO: stdout: "ipvs"
Jan 13 07:59:30.334: INFO: proxyMode: ipvs
Jan 13 07:59:30.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 13 07:59:30.363: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7542
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7542
I0113 07:59:30.383217      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7542, replica count: 3
I0113 07:59:33.434947      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 07:59:33.438: INFO: Creating new exec pod
Jan 13 07:59:38.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7542 exec execpod-affinityjpqpl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 13 07:59:38.609: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 13 07:59:38.609: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:59:38.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7542 exec execpod-affinityjpqpl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.72.72 80'
Jan 13 07:59:38.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.72.72 80\nConnection to 10.96.72.72 80 port [tcp/http] succeeded!\n"
Jan 13 07:59:38.735: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 07:59:38.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7542 exec execpod-affinityjpqpl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.72.72:80/ ; done'
Jan 13 07:59:38.909: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n"
Jan 13 07:59:38.909: INFO: stdout: "\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss\naffinity-clusterip-timeout-cjxss"
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Received response from host: affinity-clusterip-timeout-cjxss
Jan 13 07:59:38.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7542 exec execpod-affinityjpqpl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.72.72:80/'
Jan 13 07:59:39.041: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n"
Jan 13 07:59:39.041: INFO: stdout: "affinity-clusterip-timeout-cjxss"
Jan 13 08:01:49.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-7542 exec execpod-affinityjpqpl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.72.72:80/'
Jan 13 08:01:49.168: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.72.72:80/\n"
Jan 13 08:01:49.168: INFO: stdout: "affinity-clusterip-timeout-r72qd"
Jan 13 08:01:49.168: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7542, will wait for the garbage collector to delete the pods
Jan 13 08:01:49.238: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 3.084994ms
Jan 13 08:01:49.339: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.693005ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:01:58.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7542" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:150.819 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":105,"skipped":1731,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:01:58.955: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9230 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9230;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9230 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9230;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9230.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9230.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9230.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9230.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9230.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9230.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9230.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 120.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.120_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9230 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9230;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9230 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9230;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9230.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9230.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9230.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9230.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9230.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9230.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9230.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9230.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9230.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 120.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.184.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.184.120_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:02:01.021: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.023: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.024: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.028: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.029: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.031: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.033: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.046: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.051: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.052: INFO: Unable to read jessie_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.053: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.055: INFO: Unable to read jessie_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.059: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.060: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:01.071: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9230 wheezy_tcp@dns-test-service.dns-9230 wheezy_udp@dns-test-service.dns-9230.svc wheezy_tcp@dns-test-service.dns-9230.svc wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9230 jessie_tcp@dns-test-service.dns-9230 jessie_udp@dns-test-service.dns-9230.svc jessie_tcp@dns-test-service.dns-9230.svc jessie_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:06.083: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.085: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.087: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.108: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.110: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.112: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.114: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.128: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.130: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.134: INFO: Unable to read jessie_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.138: INFO: Unable to read jessie_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.144: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.145: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:06.157: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9230 wheezy_tcp@dns-test-service.dns-9230 wheezy_udp@dns-test-service.dns-9230.svc wheezy_tcp@dns-test-service.dns-9230.svc wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9230 jessie_tcp@dns-test-service.dns-9230 jessie_udp@dns-test-service.dns-9230.svc jessie_tcp@dns-test-service.dns-9230.svc jessie_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:11.082: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.084: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.086: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.089: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.091: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.094: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.110: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.111: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.113: INFO: Unable to read jessie_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.116: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.118: INFO: Unable to read jessie_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.120: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.121: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.123: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:11.133: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9230 wheezy_tcp@dns-test-service.dns-9230 wheezy_udp@dns-test-service.dns-9230.svc wheezy_tcp@dns-test-service.dns-9230.svc wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9230 jessie_tcp@dns-test-service.dns-9230 jessie_udp@dns-test-service.dns-9230.svc jessie_tcp@dns-test-service.dns-9230.svc jessie_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:16.075: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.078: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.080: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.083: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.085: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.086: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.089: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.095: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.116: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.118: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.119: INFO: Unable to read jessie_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.121: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.124: INFO: Unable to read jessie_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.126: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.145: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.149: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:16.173: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9230 wheezy_tcp@dns-test-service.dns-9230 wheezy_udp@dns-test-service.dns-9230.svc wheezy_tcp@dns-test-service.dns-9230.svc wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9230 jessie_tcp@dns-test-service.dns-9230 jessie_udp@dns-test-service.dns-9230.svc jessie_tcp@dns-test-service.dns-9230.svc jessie_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:21.075: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.077: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.086: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.088: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.089: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.091: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.093: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.094: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.105: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.106: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.108: INFO: Unable to read jessie_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.109: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.111: INFO: Unable to read jessie_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.112: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.114: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:21.125: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9230 wheezy_tcp@dns-test-service.dns-9230 wheezy_udp@dns-test-service.dns-9230.svc wheezy_tcp@dns-test-service.dns-9230.svc wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9230 jessie_tcp@dns-test-service.dns-9230 jessie_udp@dns-test-service.dns-9230.svc jessie_tcp@dns-test-service.dns-9230.svc jessie_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:26.076: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.087: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.092: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.094: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.097: INFO: Unable to read wheezy_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.099: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.101: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.103: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.118: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.121: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.123: INFO: Unable to read jessie_udp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230 from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.127: INFO: Unable to read jessie_udp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.129: INFO: Unable to read jessie_tcp@dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.131: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.133: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:26.143: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9230 wheezy_tcp@dns-test-service.dns-9230 wheezy_udp@dns-test-service.dns-9230.svc wheezy_tcp@dns-test-service.dns-9230.svc wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9230.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9230 jessie_tcp@dns-test-service.dns-9230 jessie_udp@dns-test-service.dns-9230.svc jessie_tcp@dns-test-service.dns-9230.svc jessie_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:31.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:31.130: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc from pod dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888: the server could not find the requested resource (get pods dns-test-8035fb20-4441-4176-95f1-9d73df14e888)
Jan 13 08:02:31.141: INFO: Lookups using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-9230.svc jessie_tcp@_http._tcp.dns-test-service.dns-9230.svc]

Jan 13 08:02:36.131: INFO: DNS probes using dns-9230/dns-test-8035fb20-4441-4176-95f1-9d73df14e888 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:02:36.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9230" for this suite.

• [SLOW TEST:37.263 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":106,"skipped":1733,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:02:36.218: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jan 13 08:02:36.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2664 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jan 13 08:02:36.323: INFO: stderr: ""
Jan 13 08:02:36.323: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 13 08:02:41.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2664 get pod e2e-test-httpd-pod -o json'
Jan 13 08:02:41.427: INFO: stderr: ""
Jan 13 08:02:41.427: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.10.28.58/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.10.28.58/32\"\n        },\n        \"creationTimestamp\": \"2022-01-13T08:02:36Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2664\",\n        \"resourceVersion\": \"393879\",\n        \"uid\": \"aa458735-118d-46b5-bb34-0c185f8f7297\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-2v7gn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-s001\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-2v7gn\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-13T08:02:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-13T08:02:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-13T08:02:38Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-13T08:02:36Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ae332a9970d85670018111916a7a89ecd1ba3b26576da10c8115315f3439cd34\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-01-13T08:02:37Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.100.59\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.28.58\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.28.58\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-01-13T08:02:36Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 13 08:02:41.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2664 replace -f -'
Jan 13 08:02:41.722: INFO: stderr: ""
Jan 13 08:02:41.723: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Jan 13 08:02:41.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2664 delete pods e2e-test-httpd-pod'
Jan 13 08:02:48.881: INFO: stderr: ""
Jan 13 08:02:48.881: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:02:48.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2664" for this suite.

• [SLOW TEST:12.673 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":107,"skipped":1735,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:02:48.891: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jan 13 08:02:48.920: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 13 08:02:48.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 create -f -'
Jan 13 08:02:49.137: INFO: stderr: ""
Jan 13 08:02:49.137: INFO: stdout: "service/agnhost-replica created\n"
Jan 13 08:02:49.137: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 13 08:02:49.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 create -f -'
Jan 13 08:02:49.358: INFO: stderr: ""
Jan 13 08:02:49.358: INFO: stdout: "service/agnhost-primary created\n"
Jan 13 08:02:49.358: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 13 08:02:49.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 create -f -'
Jan 13 08:02:49.599: INFO: stderr: ""
Jan 13 08:02:49.599: INFO: stdout: "service/frontend created\n"
Jan 13 08:02:49.599: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 13 08:02:49.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 create -f -'
Jan 13 08:02:49.824: INFO: stderr: ""
Jan 13 08:02:49.824: INFO: stdout: "deployment.apps/frontend created\n"
Jan 13 08:02:49.824: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 13 08:02:49.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 create -f -'
Jan 13 08:02:50.039: INFO: stderr: ""
Jan 13 08:02:50.039: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 13 08:02:50.039: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 13 08:02:50.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 create -f -'
Jan 13 08:02:50.279: INFO: stderr: ""
Jan 13 08:02:50.279: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 13 08:02:50.279: INFO: Waiting for all frontend pods to be Running.
Jan 13 08:02:55.330: INFO: Waiting for frontend to serve content.
Jan 13 08:02:55.336: INFO: Trying to add a new entry to the guestbook.
Jan 13 08:02:55.343: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 13 08:02:55.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 delete --grace-period=0 --force -f -'
Jan 13 08:02:55.418: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:02:55.418: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 13 08:02:55.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 delete --grace-period=0 --force -f -'
Jan 13 08:02:55.490: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:02:55.490: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 13 08:02:55.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 delete --grace-period=0 --force -f -'
Jan 13 08:02:55.560: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:02:55.560: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 13 08:02:55.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 delete --grace-period=0 --force -f -'
Jan 13 08:02:55.626: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:02:55.626: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 13 08:02:55.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 delete --grace-period=0 --force -f -'
Jan 13 08:02:55.705: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:02:55.705: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 13 08:02:55.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2617 delete --grace-period=0 --force -f -'
Jan 13 08:02:55.786: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:02:55.786: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:02:55.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2617" for this suite.

• [SLOW TEST:6.900 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":108,"skipped":1741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:02:55.792: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:02:59.857: INFO: Deleting pod "var-expansion-9bc4a229-8515-47c1-9576-8e4508dd8485" in namespace "var-expansion-7708"
Jan 13 08:02:59.863: INFO: Wait up to 5m0s for pod "var-expansion-9bc4a229-8515-47c1-9576-8e4508dd8485" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:09.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7708" for this suite.

• [SLOW TEST:14.089 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":109,"skipped":1836,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:09.881: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:22.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5" for this suite.

• [SLOW TEST:13.104 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":110,"skipped":1855,"failed":0}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:22.986: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Jan 13 08:03:23.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 create -f -'
Jan 13 08:03:23.233: INFO: stderr: ""
Jan 13 08:03:23.233: INFO: stdout: "pod/pause created\n"
Jan 13 08:03:23.233: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 13 08:03:23.233: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6828" to be "running and ready"
Jan 13 08:03:23.239: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.833931ms
Jan 13 08:03:25.243: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010166576s
Jan 13 08:03:25.243: INFO: Pod "pause" satisfied condition "running and ready"
Jan 13 08:03:25.243: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 13 08:03:25.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 label pods pause testing-label=testing-label-value'
Jan 13 08:03:25.302: INFO: stderr: ""
Jan 13 08:03:25.302: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 13 08:03:25.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 get pod pause -L testing-label'
Jan 13 08:03:25.355: INFO: stderr: ""
Jan 13 08:03:25.355: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 13 08:03:25.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 label pods pause testing-label-'
Jan 13 08:03:25.417: INFO: stderr: ""
Jan 13 08:03:25.417: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 13 08:03:25.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 get pod pause -L testing-label'
Jan 13 08:03:25.468: INFO: stderr: ""
Jan 13 08:03:25.468: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Jan 13 08:03:25.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 delete --grace-period=0 --force -f -'
Jan 13 08:03:25.540: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:03:25.540: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 13 08:03:25.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 get rc,svc -l name=pause --no-headers'
Jan 13 08:03:25.600: INFO: stderr: "No resources found in kubectl-6828 namespace.\n"
Jan 13 08:03:25.600: INFO: stdout: ""
Jan 13 08:03:25.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6828 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 13 08:03:25.658: INFO: stderr: ""
Jan 13 08:03:25.658: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:25.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6828" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":111,"skipped":1855,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:25.664: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-e1640570-acb4-4dfc-9e06-7734c42e356d
STEP: Creating secret with name s-test-opt-upd-982d357a-efa6-4ab2-82ab-0e07728b8c32
STEP: Creating the pod
Jan 13 08:03:25.746: INFO: The status of Pod pod-secrets-d685041e-2436-4000-b7ef-9fd3d81f193c is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:03:27.752: INFO: The status of Pod pod-secrets-d685041e-2436-4000-b7ef-9fd3d81f193c is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-e1640570-acb4-4dfc-9e06-7734c42e356d
STEP: Updating secret s-test-opt-upd-982d357a-efa6-4ab2-82ab-0e07728b8c32
STEP: Creating secret with name s-test-opt-create-7f1454ba-645f-475d-921f-f289c0d635d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:29.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8499" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":112,"skipped":1873,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:29.822: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:29.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3282" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":113,"skipped":1894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:29.871: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:03:29.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 create -f -'
Jan 13 08:03:30.116: INFO: stderr: ""
Jan 13 08:03:30.116: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 13 08:03:30.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 create -f -'
Jan 13 08:03:30.335: INFO: stderr: ""
Jan 13 08:03:30.335: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 13 08:03:31.342: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:03:31.342: INFO: Found 0 / 1
Jan 13 08:03:32.339: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:03:32.339: INFO: Found 1 / 1
Jan 13 08:03:32.339: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 13 08:03:32.341: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:03:32.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 13 08:03:32.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 describe pod agnhost-primary-btkfl'
Jan 13 08:03:32.406: INFO: stderr: ""
Jan 13 08:03:32.406: INFO: stdout: "Name:         agnhost-primary-btkfl\nNamespace:    kubectl-7169\nPriority:     0\nNode:         worker-s001/192.168.100.59\nStart Time:   Thu, 13 Jan 2022 08:03:30 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.10.28.3/32\n              cni.projectcalico.org/podIPs: 10.10.28.3/32\nStatus:       Running\nIP:           10.10.28.3\nIPs:\n  IP:           10.10.28.3\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://97de0a48667ef7dda934203b3f4a352592f4ed8350b716fc12b8bbd8031c8060\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 13 Jan 2022 08:03:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2v2cw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2v2cw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7169/agnhost-primary-btkfl to worker-s001\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 13 08:03:32.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 describe rc agnhost-primary'
Jan 13 08:03:32.479: INFO: stderr: ""
Jan 13 08:03:32.479: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7169\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-btkfl\n"
Jan 13 08:03:32.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 describe service agnhost-primary'
Jan 13 08:03:32.536: INFO: stderr: ""
Jan 13 08:03:32.536: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7169\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.41.42\nIPs:               10.96.41.42\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.28.3:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 13 08:03:32.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 describe node master1'
Jan 13 08:03:32.622: INFO: stderr: ""
Jan 13 08:03:32.622: INFO: stdout: "Name:               master1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    topology.csi-qingcloud/instance-type=Enterprise2\n                    topology.csi-qingcloud/zone=pek3d\n                    topology.kubernetes.io/zone=pek3d\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi-qingcloud\":\"i-t6lak3vx\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.beta.kubernetes.io/instance-id: i-t6lak3vx\n                    projectcalico.org/IPv4Address: 192.168.100.61/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.10.137.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 12 Jan 2022 06:36:09 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 13 Jan 2022 08:03:23 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 12 Jan 2022 06:36:34 +0000   Wed, 12 Jan 2022 06:36:34 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 13 Jan 2022 07:59:53 +0000   Wed, 12 Jan 2022 06:36:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 13 Jan 2022 07:59:53 +0000   Wed, 12 Jan 2022 06:36:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 13 Jan 2022 07:59:53 +0000   Wed, 12 Jan 2022 06:36:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 13 Jan 2022 07:59:53 +0000   Wed, 12 Jan 2022 06:36:38 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.100.61\n  Hostname:    master1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  82045336Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8167492Ki\n  pods:               120\nAllocatable:\n  cpu:                3600m\n  ephemeral-storage:  82045336Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6921336212\n  pods:               120\nSystem Info:\n  Machine ID:                   f5cc6b8935db5ac44ca1f05961de7647\n  System UUID:                  A5963A5F-6202-394C-9D69-884754B363DE\n  Boot ID:                      b40095a4-1f0c-4c85-9562-6ae49c0d469c\n  Kernel Version:               4.15.0-166-generic\n  OS Image:                     Ubuntu 18.04.6 LTS\n  Operating System:             linux\n  Architecture:                 amd64\n  Container Runtime Version:    docker://20.10.6\n  Kubelet Version:              v1.21.5\n  Kube-Proxy Version:           v1.21.5\nPodCIDR:                        10.10.0.0/24\nPodCIDRs:                       10.10.0.0/24\nNon-terminated Pods:            (15 in total)\n  Namespace                     Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                     ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                   calico-kube-controllers-5544f8d8d9-kh7wm                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         25h\n  kube-system                   calico-node-vvqkl                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         25h\n  kube-system                   cloud-controller-manager-767df6cb6c-r9w7r                  100m (2%)     200m (5%)   50Mi (0%)        100Mi (1%)     25h\n  kube-system                   coredns-6fc5ccb768-6dbrg                                   100m (2%)     0 (0%)      70Mi (1%)        170Mi (2%)     25h\n  kube-system                   coredns-6fc5ccb768-nkj78                                   100m (2%)     0 (0%)      70Mi (1%)        170Mi (2%)     25h\n  kube-system                   csi-qingcloud-node-7hd8s                                   110m (3%)     110m (3%)   120Mi (1%)       120Mi (1%)     25h\n  kube-system                   kube-apiserver-master1                                     250m (6%)     0 (0%)      0 (0%)           0 (0%)         25h\n  kube-system                   kube-controller-manager-master1                            200m (5%)     0 (0%)      0 (0%)           0 (0%)         25h\n  kube-system                   kube-proxy-9h64z                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         25h\n  kube-system                   kube-scheduler-master1                                     100m (2%)     0 (0%)      0 (0%)           0 (0%)         25h\n  kubesphere-monitoring-system  node-exporter-wdcsp                                        112m (3%)     2 (55%)     200Mi (3%)       600Mi (9%)     25h\n  kubesphere-system             ks-apiserver-7c57fd48b-h6rqz                               20m (0%)      1 (27%)     100Mi (1%)       1Gi (15%)      25h\n  kubesphere-system             ks-console-5f6c6bb6bb-r96nc                                20m (0%)      1 (27%)     100Mi (1%)       1Gi (15%)      25h\n  kubesphere-system             ks-controller-manager-d5444d8f-ffrhl                       30m (0%)      1 (27%)     50Mi (0%)        1000Mi (15%)   25h\n  sonobuoy                      sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-g6rjj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1392m (38%)  5310m (147%)\n  memory             760Mi (11%)  4208Mi (63%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Jan 13 08:03:32.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7169 describe namespace kubectl-7169'
Jan 13 08:03:32.679: INFO: stderr: ""
Jan 13 08:03:32.679: INFO: stdout: "Name:         kubectl-7169\nLabels:       e2e-framework=kubectl\n              e2e-run=2c9cc809-df4c-4713-88fa-a5e7697f5ce9\n              kubernetes.io/metadata.name=kubectl-7169\n              kubesphere.io/namespace=kubectl-7169\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:32.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7169" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":114,"skipped":1947,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:32.685: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jan 13 08:03:32.730: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:03:34.733: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 13 08:03:34.743: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:03:36.749: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 13 08:03:36.756: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:36.759: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:38.760: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:38.764: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:40.760: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:40.764: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:42.760: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:42.763: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:44.760: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:44.764: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:46.760: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:46.763: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:48.760: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:48.764: INFO: Pod pod-with-prestop-http-hook still exists
Jan 13 08:03:50.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 13 08:03:50.763: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:50.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3458" for this suite.

• [SLOW TEST:18.089 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":115,"skipped":1951,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:52.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5347" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":116,"skipped":1960,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:52.845: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jan 13 08:03:52.894: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jan 13 08:03:54.909: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jan 13 08:03:56.932: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:03:58.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8366" for this suite.

• [SLOW TEST:6.095 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":117,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:03:58.940: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-929e2fa4-bca4-49ae-9d40-058aef487908
STEP: Creating a pod to test consume configMaps
Jan 13 08:03:58.975: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0" in namespace "configmap-1070" to be "Succeeded or Failed"
Jan 13 08:03:58.978: INFO: Pod "pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.231972ms
Jan 13 08:04:00.982: INFO: Pod "pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007011018s
STEP: Saw pod success
Jan 13 08:04:00.982: INFO: Pod "pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0" satisfied condition "Succeeded or Failed"
Jan 13 08:04:00.984: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:04:01.013: INFO: Waiting for pod pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0 to disappear
Jan 13 08:04:01.021: INFO: Pod pod-configmaps-eb418cdc-e607-400d-b7d7-efec8be17ce0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:04:01.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1070" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":118,"skipped":1981,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:04:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 13 08:04:01.059: INFO: Waiting up to 5m0s for pod "security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c" in namespace "security-context-5878" to be "Succeeded or Failed"
Jan 13 08:04:01.063: INFO: Pod "security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.199833ms
Jan 13 08:04:03.066: INFO: Pod "security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007803985s
STEP: Saw pod success
Jan 13 08:04:03.066: INFO: Pod "security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c" satisfied condition "Succeeded or Failed"
Jan 13 08:04:03.068: INFO: Trying to get logs from node worker-s001 pod security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c container test-container: <nil>
STEP: delete the pod
Jan 13 08:04:03.099: INFO: Waiting for pod security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c to disappear
Jan 13 08:04:03.100: INFO: Pod security-context-687e7180-30d9-4323-9bf6-52a7d1351e7c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:04:03.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5878" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":119,"skipped":1982,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:04:03.105: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-2607d1a5-4967-483c-90ce-946af216ea48
STEP: Creating a pod to test consume configMaps
Jan 13 08:04:03.156: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0" in namespace "projected-2653" to be "Succeeded or Failed"
Jan 13 08:04:03.162: INFO: Pod "pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.352602ms
Jan 13 08:04:05.166: INFO: Pod "pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009509668s
STEP: Saw pod success
Jan 13 08:04:05.166: INFO: Pod "pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0" satisfied condition "Succeeded or Failed"
Jan 13 08:04:05.168: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:04:05.181: INFO: Waiting for pod pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0 to disappear
Jan 13 08:04:05.186: INFO: Pod pod-projected-configmaps-94a24eb0-194a-483d-a9e7-b5a3a7564fe0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:04:05.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2653" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":120,"skipped":1994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:04:05.191: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:04:16.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2259" for this suite.

• [SLOW TEST:11.089 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":121,"skipped":2022,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:04:16.281: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6495
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jan 13 08:04:16.314: INFO: Found 0 stateful pods, waiting for 3
Jan 13 08:04:26.318: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:04:26.318: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:04:26.318: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jan 13 08:04:26.342: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 13 08:04:36.370: INFO: Updating stateful set ss2
Jan 13 08:04:36.381: INFO: Waiting for Pod statefulset-6495/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jan 13 08:04:46.387: INFO: Waiting for Pod statefulset-6495/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Jan 13 08:04:56.445: INFO: Found 2 stateful pods, waiting for 3
Jan 13 08:05:06.449: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:05:06.449: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:05:06.449: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 13 08:05:06.468: INFO: Updating stateful set ss2
Jan 13 08:05:06.480: INFO: Waiting for Pod statefulset-6495/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jan 13 08:05:16.485: INFO: Waiting for Pod statefulset-6495/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jan 13 08:05:26.521: INFO: Updating stateful set ss2
Jan 13 08:05:26.545: INFO: Waiting for StatefulSet statefulset-6495/ss2 to complete update
Jan 13 08:05:26.545: INFO: Waiting for Pod statefulset-6495/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jan 13 08:05:36.550: INFO: Waiting for StatefulSet statefulset-6495/ss2 to complete update
Jan 13 08:05:36.550: INFO: Waiting for Pod statefulset-6495/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jan 13 08:05:46.563: INFO: Deleting all statefulset in ns statefulset-6495
Jan 13 08:05:46.569: INFO: Scaling statefulset ss2 to 0
Jan 13 08:06:06.581: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:06:06.583: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:06:06.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6495" for this suite.

• [SLOW TEST:110.351 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":122,"skipped":2033,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:06:06.632: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:06:07.016: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 13 08:06:09.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777657967, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777657967, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777657967, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777657967, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:06:12.034: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:06:24.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1913" for this suite.
STEP: Destroying namespace "webhook-1913-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.587 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":123,"skipped":2037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:06:24.219: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jan 13 08:06:24.271: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:06:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8088" for this suite.

• [SLOW TEST:26.553 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":124,"skipped":2113,"failed":0}
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:06:50.772: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:06:54.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2351" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":125,"skipped":2113,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:06:54.828: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:07:05.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8238" for this suite.

• [SLOW TEST:11.156 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":126,"skipped":2119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:07:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-ebb404af-761d-49fd-a975-77ae5a23e6aa
STEP: Creating a pod to test consume secrets
Jan 13 08:07:06.027: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898" in namespace "projected-0" to be "Succeeded or Failed"
Jan 13 08:07:06.032: INFO: Pod "pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201151ms
Jan 13 08:07:08.037: INFO: Pod "pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009548718s
STEP: Saw pod success
Jan 13 08:07:08.037: INFO: Pod "pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898" satisfied condition "Succeeded or Failed"
Jan 13 08:07:08.052: INFO: Trying to get logs from node worker-s001 pod pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:07:08.081: INFO: Waiting for pod pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898 to disappear
Jan 13 08:07:08.083: INFO: Pod pod-projected-secrets-8fbedf63-6bcd-4676-bedc-36a4d649e898 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:07:08.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-0" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":127,"skipped":2148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:07:08.090: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 13 08:07:10.174: INFO: running pods: 0 < 3
Jan 13 08:07:12.178: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:07:14.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9971" for this suite.

• [SLOW TEST:6.101 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":128,"skipped":2180,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:07:14.191: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-967e36cf-ae74-4196-8df5-c1cb755c5bec
STEP: Creating the pod
Jan 13 08:07:14.233: INFO: The status of Pod pod-projected-configmaps-8d13f56a-0d58-444d-99f0-961fb14eacc5 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:07:16.240: INFO: The status of Pod pod-projected-configmaps-8d13f56a-0d58-444d-99f0-961fb14eacc5 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:07:18.239: INFO: The status of Pod pod-projected-configmaps-8d13f56a-0d58-444d-99f0-961fb14eacc5 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-967e36cf-ae74-4196-8df5-c1cb755c5bec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:08:20.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8928" for this suite.

• [SLOW TEST:66.363 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":129,"skipped":2193,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:08:20.555: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Jan 13 08:08:20.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 13 08:08:20.775: INFO: stderr: ""
Jan 13 08:08:20.775: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jan 13 08:08:20.775: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 13 08:08:20.775: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2332" to be "running and ready, or succeeded"
Jan 13 08:08:20.782: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.47996ms
Jan 13 08:08:22.786: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010784918s
Jan 13 08:08:24.792: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.016292819s
Jan 13 08:08:24.792: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 13 08:08:24.792: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 13 08:08:24.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 logs logs-generator logs-generator'
Jan 13 08:08:24.854: INFO: stderr: ""
Jan 13 08:08:24.854: INFO: stdout: "I0113 08:08:22.035205       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/26bb 308\nI0113 08:08:22.235312       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/tqj 270\nI0113 08:08:22.435975       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dmn6 227\nI0113 08:08:22.636172       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/8m22 344\nI0113 08:08:22.835432       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/f8j 512\nI0113 08:08:23.035771       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vj69 579\nI0113 08:08:23.236124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/4xnh 340\nI0113 08:08:23.435367       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/p627 281\nI0113 08:08:23.635679       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v4l 258\nI0113 08:08:23.836010       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/gjt 344\nI0113 08:08:24.035348       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/8rhs 307\nI0113 08:08:24.235725       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/cv8 477\nI0113 08:08:24.436104       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/9gf 280\nI0113 08:08:24.635398       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lzcd 282\nI0113 08:08:24.835583       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/cpcd 597\n"
STEP: limiting log lines
Jan 13 08:08:24.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 logs logs-generator logs-generator --tail=1'
Jan 13 08:08:24.912: INFO: stderr: ""
Jan 13 08:08:24.912: INFO: stdout: "I0113 08:08:24.835583       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/cpcd 597\n"
Jan 13 08:08:24.912: INFO: got output "I0113 08:08:24.835583       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/cpcd 597\n"
STEP: limiting log bytes
Jan 13 08:08:24.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 logs logs-generator logs-generator --limit-bytes=1'
Jan 13 08:08:24.968: INFO: stderr: ""
Jan 13 08:08:24.968: INFO: stdout: "I"
Jan 13 08:08:24.968: INFO: got output "I"
STEP: exposing timestamps
Jan 13 08:08:24.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 13 08:08:25.025: INFO: stderr: ""
Jan 13 08:08:25.025: INFO: stdout: "2022-01-13T08:08:24.835682335Z I0113 08:08:24.835583       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/cpcd 597\n"
Jan 13 08:08:25.025: INFO: got output "2022-01-13T08:08:24.835682335Z I0113 08:08:24.835583       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/cpcd 597\n"
STEP: restricting to a time range
Jan 13 08:08:27.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 logs logs-generator logs-generator --since=1s'
Jan 13 08:08:27.592: INFO: stderr: ""
Jan 13 08:08:27.592: INFO: stdout: "I0113 08:08:26.635965       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/5nv6 332\nI0113 08:08:26.835250       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/m67w 295\nI0113 08:08:27.035901       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/d2s 464\nI0113 08:08:27.235265       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/2h8 549\nI0113 08:08:27.435602       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/mrxq 374\n"
Jan 13 08:08:27.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 logs logs-generator logs-generator --since=24h'
Jan 13 08:08:27.649: INFO: stderr: ""
Jan 13 08:08:27.649: INFO: stdout: "I0113 08:08:22.035205       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/26bb 308\nI0113 08:08:22.235312       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/tqj 270\nI0113 08:08:22.435975       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dmn6 227\nI0113 08:08:22.636172       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/8m22 344\nI0113 08:08:22.835432       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/f8j 512\nI0113 08:08:23.035771       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vj69 579\nI0113 08:08:23.236124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/4xnh 340\nI0113 08:08:23.435367       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/p627 281\nI0113 08:08:23.635679       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/v4l 258\nI0113 08:08:23.836010       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/gjt 344\nI0113 08:08:24.035348       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/8rhs 307\nI0113 08:08:24.235725       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/cv8 477\nI0113 08:08:24.436104       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/9gf 280\nI0113 08:08:24.635398       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lzcd 282\nI0113 08:08:24.835583       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/cpcd 597\nI0113 08:08:25.035914       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/896 363\nI0113 08:08:25.236244       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/5866 422\nI0113 08:08:25.435608       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/tn5 276\nI0113 08:08:25.635995       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/nhvn 470\nI0113 08:08:25.835297       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/hr9 360\nI0113 08:08:26.035668       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/s8nd 581\nI0113 08:08:26.236193       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/lz2x 452\nI0113 08:08:26.435565       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/nxj 350\nI0113 08:08:26.635965       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/5nv6 332\nI0113 08:08:26.835250       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/m67w 295\nI0113 08:08:27.035901       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/d2s 464\nI0113 08:08:27.235265       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/2h8 549\nI0113 08:08:27.435602       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/mrxq 374\nI0113 08:08:27.635920       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/zzrz 250\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Jan 13 08:08:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-2332 delete pod logs-generator'
Jan 13 08:08:38.880: INFO: stderr: ""
Jan 13 08:08:38.880: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:08:38.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2332" for this suite.

• [SLOW TEST:18.331 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":130,"skipped":2194,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:08:38.886: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 13 08:08:38.915: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 08:08:38.919: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 08:08:38.921: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Jan 13 08:08:38.926: INFO: calico-node-vbqlj from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 08:08:38.926: INFO: csi-qingcloud-node-hxljj from kube-system started at 2022-01-13 07:38:08 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 08:08:38.926: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 08:08:38.926: INFO: kube-proxy-r2zxx from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 08:08:38.926: INFO: node-exporter-987g4 from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:08:38.926: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 08:08:38.926: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2022-01-13 07:38:48 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:08:38.926: INFO: 	Container prometheus ready: true, restart count 1
Jan 13 08:08:38.926: INFO: sonobuoy from sonobuoy started at 2022-01-13 07:33:31 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 08:08:38.926: INFO: sonobuoy-e2e-job-3cc758d0b1b14270 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container e2e ready: true, restart count 0
Jan 13 08:08:38.926: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 08:08:38.926: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-shz85 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.926: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 08:08:38.926: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 08:08:38.926: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Jan 13 08:08:38.931: INFO: calico-node-nvdsl from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 08:08:38.931: INFO: csi-qingcloud-controller-6f8f6458f7-vkmtw from kube-system started at 2022-01-12 18:28:18 +0000 UTC (5 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 13 08:08:38.931: INFO: csi-qingcloud-node-84m8s from kube-system started at 2022-01-12 06:37:21 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 08:08:38.931: INFO: kube-proxy-gdx9n from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 08:08:38.931: INFO: metrics-server-687cb5444-gwn4h from kube-system started at 2022-01-12 06:37:45 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container metrics-server ready: true, restart count 0
Jan 13 08:08:38.931: INFO: snapshot-controller-0 from kube-system started at 2022-01-12 06:38:20 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 13 08:08:38.931: INFO: default-http-backend-5bf68ff9b8-gt7w6 from kubesphere-controls-system started at 2022-01-12 06:39:06 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 13 08:08:38.931: INFO: kubectl-admin-6667774bb-lfhx9 from kubesphere-controls-system started at 2022-01-12 06:41:27 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container kubectl ready: true, restart count 0
Jan 13 08:08:38.931: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2022-01-12 18:28:22 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:08:38.931: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2022-01-12 06:40:50 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:08:38.931: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2022-01-12 17:16:42 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:08:38.931: INFO: kube-state-metrics-5547ddd4cc-79dkj from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (3 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 13 08:08:38.931: INFO: node-exporter-wjvtx from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 08:08:38.931: INFO: notification-manager-deployment-78664576cb-7cqhm from kubesphere-monitoring-system started at 2022-01-12 18:28:18 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container tenant ready: true, restart count 0
Jan 13 08:08:38.931: INFO: notification-manager-deployment-78664576cb-pppk6 from kubesphere-monitoring-system started at 2022-01-12 06:41:16 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container tenant ready: true, restart count 0
Jan 13 08:08:38.931: INFO: notification-manager-operator-7d44854f54-97b8n from kubesphere-monitoring-system started at 2022-01-12 06:41:10 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container notification-manager-operator ready: true, restart count 0
Jan 13 08:08:38.931: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2022-01-12 06:41:05 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container prometheus ready: true, restart count 0
Jan 13 08:08:38.931: INFO: prometheus-operator-5c5db79546-gl2sj from kubesphere-monitoring-system started at 2022-01-12 06:40:40 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 13 08:08:38.931: INFO: ks-installer-6cf7f47896-d9fqp from kubesphere-system started at 2022-01-12 06:37:23 +0000 UTC (1 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container installer ready: true, restart count 0
Jan 13 08:08:38.931: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-6f2nd from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 08:08:38.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 08:08:38.931: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node worker-s001
STEP: verifying the node has the label node worker-s002
Jan 13 08:08:38.971: INFO: Pod calico-node-nvdsl requesting resource cpu=250m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod calico-node-vbqlj requesting resource cpu=250m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod csi-qingcloud-controller-6f8f6458f7-vkmtw requesting resource cpu=340m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod csi-qingcloud-node-84m8s requesting resource cpu=110m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod csi-qingcloud-node-hxljj requesting resource cpu=110m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod kube-proxy-gdx9n requesting resource cpu=0m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod kube-proxy-r2zxx requesting resource cpu=0m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod metrics-server-687cb5444-gwn4h requesting resource cpu=0m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod snapshot-controller-0 requesting resource cpu=0m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod default-http-backend-5bf68ff9b8-gt7w6 requesting resource cpu=10m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod kubectl-admin-6667774bb-lfhx9 requesting resource cpu=0m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod alertmanager-main-0 requesting resource cpu=20m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod alertmanager-main-1 requesting resource cpu=20m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod alertmanager-main-2 requesting resource cpu=20m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod kube-state-metrics-5547ddd4cc-79dkj requesting resource cpu=120m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod node-exporter-987g4 requesting resource cpu=112m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod node-exporter-wjvtx requesting resource cpu=112m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod notification-manager-deployment-78664576cb-7cqhm requesting resource cpu=5m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod notification-manager-deployment-78664576cb-pppk6 requesting resource cpu=5m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod notification-manager-operator-7d44854f54-97b8n requesting resource cpu=10m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod prometheus-k8s-0 requesting resource cpu=200m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod prometheus-k8s-1 requesting resource cpu=200m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod prometheus-operator-5c5db79546-gl2sj requesting resource cpu=110m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod ks-installer-6cf7f47896-d9fqp requesting resource cpu=20m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod sonobuoy-e2e-job-3cc758d0b1b14270 requesting resource cpu=0m on Node worker-s001
Jan 13 08:08:38.971: INFO: Pod sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-6f2nd requesting resource cpu=0m on Node worker-s002
Jan 13 08:08:38.971: INFO: Pod sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-shz85 requesting resource cpu=0m on Node worker-s001
STEP: Starting Pods to consume most of the cluster CPU.
Jan 13 08:08:38.971: INFO: Creating a pod which consumes cpu=4849m on Node worker-s001
Jan 13 08:08:38.979: INFO: Creating a pod which consumes cpu=4373m on Node worker-s002
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1.16c9c611d1328be2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8292/filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1 to worker-s001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1.16c9c6120f3fba60], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1.16c9c6121269653d], Reason = [Created], Message = [Created container filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1.16c9c6121cd116a8], Reason = [Started], Message = [Started container filler-pod-090c46bd-e052-4270-8c95-f7eaa39f0fe1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50475ee2-a404-4959-a250-f166556fd051.16c9c611d1de7c27], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8292/filler-pod-50475ee2-a404-4959-a250-f166556fd051 to worker-s002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50475ee2-a404-4959-a250-f166556fd051.16c9c6121254a9b1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50475ee2-a404-4959-a250-f166556fd051.16c9c61216cfb806], Reason = [Created], Message = [Created container filler-pod-50475ee2-a404-4959-a250-f166556fd051]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50475ee2-a404-4959-a250-f166556fd051.16c9c6122084f85f], Reason = [Started], Message = [Started container filler-pod-50475ee2-a404-4959-a250-f166556fd051]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16c9c612c14a873c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node worker-s001
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-s002
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:08:44.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8292" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.151 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":131,"skipped":2205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:08:44.038: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:08:48.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2012" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":132,"skipped":2294,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:08:48.501: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0113 08:08:48.525291      24 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:13:48.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2058" for this suite.

• [SLOW TEST:300.042 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":133,"skipped":2368,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:13:48.543: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-dce1351e-5c04-4da7-b77e-0af495d946bf
STEP: Creating a pod to test consume secrets
Jan 13 08:13:48.621: INFO: Waiting up to 5m0s for pod "pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498" in namespace "secrets-7185" to be "Succeeded or Failed"
Jan 13 08:13:48.624: INFO: Pod "pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390509ms
Jan 13 08:13:50.627: INFO: Pod "pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498": Phase="Running", Reason="", readiness=true. Elapsed: 2.005842802s
Jan 13 08:13:52.631: INFO: Pod "pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009643748s
STEP: Saw pod success
Jan 13 08:13:52.631: INFO: Pod "pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498" satisfied condition "Succeeded or Failed"
Jan 13 08:13:52.633: INFO: Trying to get logs from node worker-s001 pod pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498 container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:13:52.658: INFO: Waiting for pod pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498 to disappear
Jan 13 08:13:52.665: INFO: Pod pod-secrets-1d4a77b3-a7e5-497f-8712-65e6a76f7498 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:13:52.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7185" for this suite.
STEP: Destroying namespace "secret-namespace-7431" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":134,"skipped":2378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:13:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:13:52.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803" in namespace "downward-api-59" to be "Succeeded or Failed"
Jan 13 08:13:52.710: INFO: Pod "downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.754094ms
Jan 13 08:13:54.714: INFO: Pod "downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006731167s
Jan 13 08:13:56.718: INFO: Pod "downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010302291s
STEP: Saw pod success
Jan 13 08:13:56.718: INFO: Pod "downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803" satisfied condition "Succeeded or Failed"
Jan 13 08:13:56.720: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803 container client-container: <nil>
STEP: delete the pod
Jan 13 08:13:56.742: INFO: Waiting for pod downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803 to disappear
Jan 13 08:13:56.744: INFO: Pod downwardapi-volume-92cca545-4818-4a86-a96e-f618ff619803 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:13:56.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-59" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":135,"skipped":2400,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:13:56.749: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:13:56.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff" in namespace "downward-api-442" to be "Succeeded or Failed"
Jan 13 08:13:56.795: INFO: Pod "downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff": Phase="Pending", Reason="", readiness=false. Elapsed: 9.52142ms
Jan 13 08:13:58.800: INFO: Pod "downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014769075s
Jan 13 08:14:00.805: INFO: Pod "downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019204901s
STEP: Saw pod success
Jan 13 08:14:00.805: INFO: Pod "downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff" satisfied condition "Succeeded or Failed"
Jan 13 08:14:00.807: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff container client-container: <nil>
STEP: delete the pod
Jan 13 08:14:00.830: INFO: Waiting for pod downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff to disappear
Jan 13 08:14:00.834: INFO: Pod downwardapi-volume-e4291396-c279-4936-97ba-69ef412874ff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:00.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-442" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":136,"skipped":2405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:00.840: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8339.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8339.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:14:04.929: INFO: DNS probes using dns-8339/dns-test-b2c8c6c3-0ac7-490c-ab95-77019cbe6a32 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:04.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8339" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":137,"skipped":2431,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:04.948: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 13 08:14:09.500: INFO: Successfully updated pod "adopt-release-g6k2f"
STEP: Checking that the Job readopts the Pod
Jan 13 08:14:09.500: INFO: Waiting up to 15m0s for pod "adopt-release-g6k2f" in namespace "job-2623" to be "adopted"
Jan 13 08:14:09.512: INFO: Pod "adopt-release-g6k2f": Phase="Running", Reason="", readiness=true. Elapsed: 12.046513ms
Jan 13 08:14:11.516: INFO: Pod "adopt-release-g6k2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016779894s
Jan 13 08:14:11.516: INFO: Pod "adopt-release-g6k2f" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 13 08:14:12.024: INFO: Successfully updated pod "adopt-release-g6k2f"
STEP: Checking that the Job releases the Pod
Jan 13 08:14:12.024: INFO: Waiting up to 15m0s for pod "adopt-release-g6k2f" in namespace "job-2623" to be "released"
Jan 13 08:14:12.042: INFO: Pod "adopt-release-g6k2f": Phase="Running", Reason="", readiness=true. Elapsed: 17.209416ms
Jan 13 08:14:14.047: INFO: Pod "adopt-release-g6k2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.02210538s
Jan 13 08:14:14.047: INFO: Pod "adopt-release-g6k2f" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:14.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2623" for this suite.

• [SLOW TEST:9.112 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":138,"skipped":2436,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:14.060: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 13 08:14:14.100: INFO: The status of Pod labelsupdatee028dec2-cd92-4756-9913-f86e4d6da254 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:14:16.105: INFO: The status of Pod labelsupdatee028dec2-cd92-4756-9913-f86e4d6da254 is Running (Ready = true)
Jan 13 08:14:16.633: INFO: Successfully updated pod "labelsupdatee028dec2-cd92-4756-9913-f86e4d6da254"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:20.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4822" for this suite.

• [SLOW TEST:6.603 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":2446,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:20.663: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 13 08:14:22.716: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:22.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5652" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":140,"skipped":2465,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:22.735: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jan 13 08:14:22.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-1607 create -f -'
Jan 13 08:14:23.061: INFO: stderr: ""
Jan 13 08:14:23.061: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 13 08:14:23.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-1607 diff -f -'
Jan 13 08:14:23.279: INFO: rc: 1
Jan 13 08:14:23.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-1607 delete -f -'
Jan 13 08:14:23.335: INFO: stderr: ""
Jan 13 08:14:23.335: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:23.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1607" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":141,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:23.351: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:14:23.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-1807 version'
Jan 13 08:14:23.439: INFO: stderr: ""
Jan 13 08:14:23.439: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5\", GitCommit:\"aea7bbadd2fc0cd689de94a54e5b7b758869d691\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:10:45Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5\", GitCommit:\"aea7bbadd2fc0cd689de94a54e5b7b758869d691\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:04:16Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:23.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1807" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":142,"skipped":2484,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:23.444: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 13 08:14:23.475: INFO: Waiting up to 5m0s for pod "pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9" in namespace "emptydir-1610" to be "Succeeded or Failed"
Jan 13 08:14:23.478: INFO: Pod "pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523429ms
Jan 13 08:14:25.482: INFO: Pod "pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007100637s
STEP: Saw pod success
Jan 13 08:14:25.482: INFO: Pod "pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9" satisfied condition "Succeeded or Failed"
Jan 13 08:14:25.484: INFO: Trying to get logs from node worker-s001 pod pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9 container test-container: <nil>
STEP: delete the pod
Jan 13 08:14:25.497: INFO: Waiting for pod pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9 to disappear
Jan 13 08:14:25.512: INFO: Pod pod-b0d5705c-7ebe-44a7-85ab-4ad41f13b9f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:25.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1610" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":143,"skipped":2498,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:25.524: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9d2fcc51-8ad6-4176-ad50-d8c758747c99
STEP: Creating a pod to test consume configMaps
Jan 13 08:14:25.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8" in namespace "projected-5035" to be "Succeeded or Failed"
Jan 13 08:14:25.559: INFO: Pod "pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802986ms
Jan 13 08:14:27.563: INFO: Pod "pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009079039s
STEP: Saw pod success
Jan 13 08:14:27.563: INFO: Pod "pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8" satisfied condition "Succeeded or Failed"
Jan 13 08:14:27.565: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:14:27.580: INFO: Waiting for pod pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8 to disappear
Jan 13 08:14:27.586: INFO: Pod pod-projected-configmaps-8f632d8b-6878-44db-8c5a-80060fa35cd8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:27.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5035" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":144,"skipped":2500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:27.595: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:38.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4308" for this suite.

• [SLOW TEST:11.084 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":145,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:38.678: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:14:38.722: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105" in namespace "downward-api-4490" to be "Succeeded or Failed"
Jan 13 08:14:38.723: INFO: Pod "downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105": Phase="Pending", Reason="", readiness=false. Elapsed: 1.696211ms
Jan 13 08:14:40.726: INFO: Pod "downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004668762s
STEP: Saw pod success
Jan 13 08:14:40.726: INFO: Pod "downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105" satisfied condition "Succeeded or Failed"
Jan 13 08:14:40.728: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105 container client-container: <nil>
STEP: delete the pod
Jan 13 08:14:40.743: INFO: Waiting for pod downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105 to disappear
Jan 13 08:14:40.748: INFO: Pod downwardapi-volume-9124ad66-2c54-40ad-b42a-6083e2c19105 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:40.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4490" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":146,"skipped":2537,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:40.753: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:14:56.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1024" for this suite.

• [SLOW TEST:16.150 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":147,"skipped":2549,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:14:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1632
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1632
STEP: creating replication controller externalsvc in namespace services-1632
I0113 08:14:56.970823      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1632, replica count: 2
I0113 08:15:00.021553      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 13 08:15:00.048: INFO: Creating new exec pod
Jan 13 08:15:02.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-1632 exec execpodxczdw -- /bin/sh -x -c nslookup nodeport-service.services-1632.svc.cluster.local'
Jan 13 08:15:02.201: INFO: stderr: "+ nslookup nodeport-service.services-1632.svc.cluster.local\n"
Jan 13 08:15:02.201: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1632.svc.cluster.local\tcanonical name = externalsvc.services-1632.svc.cluster.local.\nName:\texternalsvc.services-1632.svc.cluster.local\nAddress: 10.96.146.221\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1632, will wait for the garbage collector to delete the pods
Jan 13 08:15:02.256: INFO: Deleting ReplicationController externalsvc took: 2.883643ms
Jan 13 08:15:02.358: INFO: Terminating ReplicationController externalsvc pods took: 101.12777ms
Jan 13 08:15:08.973: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1632" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.089 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":148,"skipped":2558,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:08.993: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4727
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 13 08:15:09.021: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 08:15:09.049: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:15:11.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:13.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:15.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:17.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:19.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:21.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:23.054: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:25.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:27.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:15:29.053: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 13 08:15:29.071: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 13 08:15:31.074: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 13 08:15:35.109: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 08:15:35.109: INFO: Going to poll 10.10.28.35 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Jan 13 08:15:35.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.28.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4727 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:15:35.111: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:15:35.175: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 13 08:15:35.175: INFO: Going to poll 10.10.131.173 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Jan 13 08:15:35.177: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.131.173:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4727 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:15:35.177: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:15:35.251: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:35.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4727" for this suite.

• [SLOW TEST:26.264 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":149,"skipped":2566,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:35.257: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-41a71c9b-08b2-4af5-a636-12d45aeefe80
STEP: Creating a pod to test consume secrets
Jan 13 08:15:35.301: INFO: Waiting up to 5m0s for pod "pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760" in namespace "secrets-2615" to be "Succeeded or Failed"
Jan 13 08:15:35.304: INFO: Pod "pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448052ms
Jan 13 08:15:37.308: INFO: Pod "pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006639761s
STEP: Saw pod success
Jan 13 08:15:37.308: INFO: Pod "pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760" satisfied condition "Succeeded or Failed"
Jan 13 08:15:37.310: INFO: Trying to get logs from node worker-s001 pod pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760 container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:15:37.324: INFO: Waiting for pod pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760 to disappear
Jan 13 08:15:37.328: INFO: Pod pod-secrets-948d8d35-911e-48c0-8da5-cbb355e47760 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:37.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2615" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":150,"skipped":2580,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:37.333: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-d4b9849f-8201-4522-8cc0-622f29646b7b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:39.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4148" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":151,"skipped":2601,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:39.400: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:15:40.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 08:15:42.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777658540, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777658540, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777658540, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777658540, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:15:45.075: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:45.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2190" for this suite.
STEP: Destroying namespace "webhook-2190-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.851 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":152,"skipped":2618,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:45.251: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:45.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5333" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":153,"skipped":2620,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:45.339: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:15:45.372: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 13 08:15:50.377: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 13 08:15:50.377: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 08:15:50.402: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2853  54ec4dd4-0166-46ec-b0ae-4ef2f7e8c753 399468 1 2022-01-13 08:15:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-01-13 08:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd45e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 13 08:15:50.417: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-2853  1b51def0-b168-4891-81da-576ad16f1289 399470 1 2022-01-13 08:15:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 54ec4dd4-0166-46ec-b0ae-4ef2f7e8c753 0xc0065271a7 0xc0065271a8}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54ec4dd4-0166-46ec-b0ae-4ef2f7e8c753\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006527238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:15:50.417: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 13 08:15:50.417: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2853  4ca698d3-c286-482f-9c44-036da8d9ec07 399469 1 2022-01-13 08:15:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 54ec4dd4-0166-46ec-b0ae-4ef2f7e8c753 0xc006527097 0xc006527098}] []  [{e2e.test Update apps/v1 2022-01-13 08:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"54ec4dd4-0166-46ec-b0ae-4ef2f7e8c753\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006527138 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:15:50.421: INFO: Pod "test-cleanup-controller-vgw7w" is available:
&Pod{ObjectMeta:{test-cleanup-controller-vgw7w test-cleanup-controller- deployment-2853  1dee3704-1abd-4488-85fb-aba26ed4e808 399444 0 2022-01-13 08:15:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.10.28.50/32 cni.projectcalico.org/podIPs:10.10.28.50/32] [{apps/v1 ReplicaSet test-cleanup-controller 4ca698d3-c286-482f-9c44-036da8d9ec07 0xc0076cf087 0xc0076cf088}] []  [{kube-controller-manager Update v1 2022-01-13 08:15:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca698d3-c286-482f-9c44-036da8d9ec07\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:15:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:15:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqhsc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqhsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:15:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:15:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:15:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.50,StartTime:2022-01-13 08:15:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:15:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://ad2c187171adfbd0535b6cc6d7dcffd532c772a920d36f92325af9e8938b6d62,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:15:50.421: INFO: Pod "test-cleanup-deployment-5b4d99b59b-6ft77" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-6ft77 test-cleanup-deployment-5b4d99b59b- deployment-2853  3e50fcee-4b84-4153-b71b-15b354d71530 399474 0 2022-01-13 08:15:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 1b51def0-b168-4891-81da-576ad16f1289 0xc0076cf297 0xc0076cf298}] []  [{kube-controller-manager Update v1 2022-01-13 08:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b51def0-b168-4891-81da-576ad16f1289\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zschx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zschx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:15:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:50.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2853" for this suite.

• [SLOW TEST:5.093 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":154,"skipped":2625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:50.433: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:50.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1454" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":155,"skipped":2674,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:50.615: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:15:50.689: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 13 08:15:52.723: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:15:52.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5653" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":156,"skipped":2680,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:15:52.741: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 13 08:15:52.792: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:52.793: INFO: Number of nodes with available pods: 0
Jan 13 08:15:52.793: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:15:53.796: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:53.799: INFO: Number of nodes with available pods: 0
Jan 13 08:15:53.799: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:15:56.026: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:56.048: INFO: Number of nodes with available pods: 2
Jan 13 08:15:56.048: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 13 08:15:56.091: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:56.093: INFO: Number of nodes with available pods: 1
Jan 13 08:15:56.093: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 08:15:57.098: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:57.100: INFO: Number of nodes with available pods: 1
Jan 13 08:15:57.100: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 08:15:58.096: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:58.099: INFO: Number of nodes with available pods: 1
Jan 13 08:15:58.099: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 08:15:59.098: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:15:59.100: INFO: Number of nodes with available pods: 1
Jan 13 08:15:59.100: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 08:16:00.098: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:16:00.101: INFO: Number of nodes with available pods: 1
Jan 13 08:16:00.101: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 08:16:01.098: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:16:01.100: INFO: Number of nodes with available pods: 2
Jan 13 08:16:01.100: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-278, will wait for the garbage collector to delete the pods
Jan 13 08:16:01.158: INFO: Deleting DaemonSet.extensions daemon-set took: 3.091075ms
Jan 13 08:16:01.259: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.293158ms
Jan 13 08:16:08.961: INFO: Number of nodes with available pods: 0
Jan 13 08:16:08.961: INFO: Number of running nodes: 0, number of available pods: 0
Jan 13 08:16:08.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"399764"},"items":null}

Jan 13 08:16:08.965: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"399764"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-278" for this suite.

• [SLOW TEST:16.234 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":157,"skipped":2683,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:08.976: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:09.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5461" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":158,"skipped":2683,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:16:09.319: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:16:12.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:16:12.340: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7597-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:15.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3340" for this suite.
STEP: Destroying namespace "webhook-3340-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.524 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":159,"skipped":2698,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:16:15.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002" in namespace "downward-api-8535" to be "Succeeded or Failed"
Jan 13 08:16:15.626: INFO: Pod "downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002": Phase="Pending", Reason="", readiness=false. Elapsed: 4.178454ms
Jan 13 08:16:17.631: INFO: Pod "downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009429468s
STEP: Saw pod success
Jan 13 08:16:17.631: INFO: Pod "downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002" satisfied condition "Succeeded or Failed"
Jan 13 08:16:17.634: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002 container client-container: <nil>
STEP: delete the pod
Jan 13 08:16:17.649: INFO: Waiting for pod downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002 to disappear
Jan 13 08:16:17.652: INFO: Pod downwardapi-volume-d6afd935-a795-45be-9fe6-407ddee26002 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:17.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8535" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":160,"skipped":2709,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:17.675: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-2b8a8d9b-5910-4b3e-af26-d0a21860f67a
STEP: Creating the pod
Jan 13 08:16:17.747: INFO: The status of Pod pod-configmaps-159dba39-c560-475d-9f5d-fe2715a94df6 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:16:19.752: INFO: The status of Pod pod-configmaps-159dba39-c560-475d-9f5d-fe2715a94df6 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:16:21.753: INFO: The status of Pod pod-configmaps-159dba39-c560-475d-9f5d-fe2715a94df6 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-2b8a8d9b-5910-4b3e-af26-d0a21860f67a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2386" for this suite.

• [SLOW TEST:6.105 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":161,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:23.780: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:16:23.815: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 13 08:16:28.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-9613 --namespace=crd-publish-openapi-9613 create -f -'
Jan 13 08:16:29.089: INFO: stderr: ""
Jan 13 08:16:29.089: INFO: stdout: "e2e-test-crd-publish-openapi-9356-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 13 08:16:29.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-9613 --namespace=crd-publish-openapi-9613 delete e2e-test-crd-publish-openapi-9356-crds test-cr'
Jan 13 08:16:29.154: INFO: stderr: ""
Jan 13 08:16:29.154: INFO: stdout: "e2e-test-crd-publish-openapi-9356-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 13 08:16:29.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-9613 --namespace=crd-publish-openapi-9613 apply -f -'
Jan 13 08:16:29.367: INFO: stderr: ""
Jan 13 08:16:29.367: INFO: stdout: "e2e-test-crd-publish-openapi-9356-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 13 08:16:29.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-9613 --namespace=crd-publish-openapi-9613 delete e2e-test-crd-publish-openapi-9356-crds test-cr'
Jan 13 08:16:29.425: INFO: stderr: ""
Jan 13 08:16:29.425: INFO: stdout: "e2e-test-crd-publish-openapi-9356-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 13 08:16:29.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-9613 explain e2e-test-crd-publish-openapi-9356-crds'
Jan 13 08:16:29.630: INFO: stderr: ""
Jan 13 08:16:29.630: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9356-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:33.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9613" for this suite.

• [SLOW TEST:10.152 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":162,"skipped":2730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:33.933: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jan 13 08:16:33.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7968 create -f -'
Jan 13 08:16:34.259: INFO: stderr: ""
Jan 13 08:16:34.259: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 13 08:16:35.268: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:16:35.268: INFO: Found 0 / 1
Jan 13 08:16:36.263: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:16:36.263: INFO: Found 0 / 1
Jan 13 08:16:37.267: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:16:37.267: INFO: Found 1 / 1
Jan 13 08:16:37.267: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 13 08:16:37.268: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:16:37.268: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 13 08:16:37.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7968 patch pod agnhost-primary-zbfkv -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 13 08:16:37.343: INFO: stderr: ""
Jan 13 08:16:37.343: INFO: stdout: "pod/agnhost-primary-zbfkv patched\n"
STEP: checking annotations
Jan 13 08:16:37.345: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:16:37.345: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:16:37.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7968" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":163,"skipped":2793,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:16:37.351: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2352
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2352
Jan 13 08:16:37.392: INFO: Found 0 stateful pods, waiting for 1
Jan 13 08:16:47.398: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jan 13 08:16:47.421: INFO: Deleting all statefulset in ns statefulset-2352
Jan 13 08:16:47.422: INFO: Scaling statefulset ss to 0
Jan 13 08:17:17.447: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:17:17.449: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:17.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2352" for this suite.

• [SLOW TEST:40.121 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":164,"skipped":2812,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:17.472: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:17:17.513: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:18.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5477" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":165,"skipped":2812,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:18.066: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-9a46a360-6131-45b7-97e4-e3664df4c663
STEP: Creating a pod to test consume secrets
Jan 13 08:17:18.122: INFO: Waiting up to 5m0s for pod "pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef" in namespace "secrets-5923" to be "Succeeded or Failed"
Jan 13 08:17:18.125: INFO: Pod "pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.323011ms
Jan 13 08:17:20.131: INFO: Pod "pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008965539s
STEP: Saw pod success
Jan 13 08:17:20.131: INFO: Pod "pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef" satisfied condition "Succeeded or Failed"
Jan 13 08:17:20.133: INFO: Trying to get logs from node worker-s001 pod pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:17:20.153: INFO: Waiting for pod pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef to disappear
Jan 13 08:17:20.156: INFO: Pod pod-secrets-fd50d34a-a6a7-40d8-993d-55c823112cef no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:20.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5923" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":166,"skipped":2818,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:20.161: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1142.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1142.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1142.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1142.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:17:22.245: INFO: DNS probes using dns-1142/dns-test-3a1f1639-256e-4165-8b03-d5a92a60c60e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:22.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1142" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":167,"skipped":2831,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:22.262: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:17:22.605: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:17:25.617: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:17:25.620: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2400-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:28.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1256" for this suite.
STEP: Destroying namespace "webhook-1256-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":168,"skipped":2835,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:28.953: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jan 13 08:17:29.037: INFO: created test-podtemplate-1
Jan 13 08:17:29.040: INFO: created test-podtemplate-2
Jan 13 08:17:29.043: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 13 08:17:29.044: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 13 08:17:29.054: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:29.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8673" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":169,"skipped":2845,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:29.065: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:17:29.106: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438" in namespace "downward-api-5784" to be "Succeeded or Failed"
Jan 13 08:17:29.112: INFO: Pod "downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99844ms
Jan 13 08:17:31.115: INFO: Pod "downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009445576s
STEP: Saw pod success
Jan 13 08:17:31.115: INFO: Pod "downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438" satisfied condition "Succeeded or Failed"
Jan 13 08:17:31.117: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438 container client-container: <nil>
STEP: delete the pod
Jan 13 08:17:31.141: INFO: Waiting for pod downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438 to disappear
Jan 13 08:17:31.145: INFO: Pod downwardapi-volume-dcc41ee5-6063-434e-851c-89a4e6fc5438 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:17:31.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5784" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":170,"skipped":2851,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:17:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0113 08:18:11.215448      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 13 08:19:13.228: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jan 13 08:19:13.228: INFO: Deleting pod "simpletest.rc-62ztk" in namespace "gc-8796"
Jan 13 08:19:13.236: INFO: Deleting pod "simpletest.rc-77plk" in namespace "gc-8796"
Jan 13 08:19:13.247: INFO: Deleting pod "simpletest.rc-cd2xm" in namespace "gc-8796"
Jan 13 08:19:13.260: INFO: Deleting pod "simpletest.rc-djjmv" in namespace "gc-8796"
Jan 13 08:19:13.273: INFO: Deleting pod "simpletest.rc-j45db" in namespace "gc-8796"
Jan 13 08:19:13.284: INFO: Deleting pod "simpletest.rc-lpfzw" in namespace "gc-8796"
Jan 13 08:19:13.295: INFO: Deleting pod "simpletest.rc-pzj5h" in namespace "gc-8796"
Jan 13 08:19:13.311: INFO: Deleting pod "simpletest.rc-r2qbk" in namespace "gc-8796"
Jan 13 08:19:13.328: INFO: Deleting pod "simpletest.rc-r7rzk" in namespace "gc-8796"
Jan 13 08:19:13.340: INFO: Deleting pod "simpletest.rc-x462f" in namespace "gc-8796"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:13.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8796" for this suite.

• [SLOW TEST:102.206 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":171,"skipped":2865,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:13.363: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 13 08:19:13.419: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8360  e169397a-b51e-47f9-9fc5-52529601232b 401249 0 2022-01-13 08:19:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-13 08:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 08:19:13.419: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8360  e169397a-b51e-47f9-9fc5-52529601232b 401250 0 2022-01-13 08:19:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-13 08:19:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 13 08:19:13.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8360  e169397a-b51e-47f9-9fc5-52529601232b 401251 0 2022-01-13 08:19:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-13 08:19:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 08:19:13.443: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8360  e169397a-b51e-47f9-9fc5-52529601232b 401252 0 2022-01-13 08:19:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-13 08:19:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:13.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8360" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":172,"skipped":2876,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:13.460: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:13.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3198" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":173,"skipped":2886,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2183
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2183
I0113 08:19:13.589444      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2183, replica count: 2
Jan 13 08:19:16.641: INFO: Creating new exec pod
I0113 08:19:16.641106      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 08:19:21.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2183 exec execpodx88qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 13 08:19:21.784: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 13 08:19:21.784: INFO: stdout: "externalname-service-6m2fs"
Jan 13 08:19:21.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2183 exec execpodx88qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.172.104 80'
Jan 13 08:19:21.908: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.172.104 80\nConnection to 10.96.172.104 80 port [tcp/http] succeeded!\n"
Jan 13 08:19:21.908: INFO: stdout: ""
Jan 13 08:19:22.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2183 exec execpodx88qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.172.104 80'
Jan 13 08:19:23.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.172.104 80\nConnection to 10.96.172.104 80 port [tcp/http] succeeded!\n"
Jan 13 08:19:23.043: INFO: stdout: "externalname-service-6m2fs"
Jan 13 08:19:23.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2183 exec execpodx88qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.59 30157'
Jan 13 08:19:23.166: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.59 30157\nConnection to 192.168.100.59 30157 port [tcp/*] succeeded!\n"
Jan 13 08:19:23.166: INFO: stdout: "externalname-service-6m2fs"
Jan 13 08:19:23.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2183 exec execpodx88qc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.60 30157'
Jan 13 08:19:23.299: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.60 30157\nConnection to 192.168.100.60 30157 port [tcp/*] succeeded!\n"
Jan 13 08:19:23.300: INFO: stdout: "externalname-service-6m2fs"
Jan 13 08:19:23.300: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:23.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2183" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:9.828 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":174,"skipped":2898,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:23.328: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 13 08:19:23.365: INFO: Waiting up to 5m0s for pod "pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567" in namespace "emptydir-5029" to be "Succeeded or Failed"
Jan 13 08:19:23.371: INFO: Pod "pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778554ms
Jan 13 08:19:25.373: INFO: Pod "pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008443094s
STEP: Saw pod success
Jan 13 08:19:25.373: INFO: Pod "pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567" satisfied condition "Succeeded or Failed"
Jan 13 08:19:25.375: INFO: Trying to get logs from node worker-s001 pod pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567 container test-container: <nil>
STEP: delete the pod
Jan 13 08:19:25.395: INFO: Waiting for pod pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567 to disappear
Jan 13 08:19:25.398: INFO: Pod pod-e0eec57b-8d12-49d6-b1eb-2e7a7052e567 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:25.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5029" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":175,"skipped":2912,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:25.402: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 13 08:19:25.460: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 13 08:19:25.462: INFO: starting watch
STEP: patching
STEP: updating
Jan 13 08:19:25.470: INFO: waiting for watch events with expected annotations
Jan 13 08:19:25.470: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:25.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2495" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":176,"skipped":2922,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:25.496: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-88eed3a7-9c1a-4daa-b34e-9e4e78c65094
STEP: Creating a pod to test consume secrets
Jan 13 08:19:25.525: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b" in namespace "projected-4309" to be "Succeeded or Failed"
Jan 13 08:19:25.528: INFO: Pod "pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238323ms
Jan 13 08:19:27.531: INFO: Pod "pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006312726s
STEP: Saw pod success
Jan 13 08:19:27.531: INFO: Pod "pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b" satisfied condition "Succeeded or Failed"
Jan 13 08:19:27.533: INFO: Trying to get logs from node worker-s001 pod pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:19:27.546: INFO: Waiting for pod pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b to disappear
Jan 13 08:19:27.568: INFO: Pod pod-projected-secrets-5d6a83ae-902a-4a73-9b4b-9da3b6c5c12b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:27.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4309" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":177,"skipped":2926,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:27.572: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jan 13 08:19:27.608: INFO: Waiting up to 5m0s for pod "var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8" in namespace "var-expansion-8651" to be "Succeeded or Failed"
Jan 13 08:19:27.619: INFO: Pod "var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016356ms
Jan 13 08:19:29.623: INFO: Pod "var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014218703s
Jan 13 08:19:31.626: INFO: Pod "var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018167701s
STEP: Saw pod success
Jan 13 08:19:31.626: INFO: Pod "var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8" satisfied condition "Succeeded or Failed"
Jan 13 08:19:31.629: INFO: Trying to get logs from node worker-s001 pod var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8 container dapi-container: <nil>
STEP: delete the pod
Jan 13 08:19:31.648: INFO: Waiting for pod var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8 to disappear
Jan 13 08:19:31.652: INFO: Pod var-expansion-fdfafdea-0e5b-4fed-9bb6-e9f350ab15a8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:31.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8651" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":178,"skipped":2935,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:31.657: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8823" for this suite.

• [SLOW TEST:16.139 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":179,"skipped":2939,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:47.796: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 13 08:19:47.831: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 13 08:19:47.835: INFO: Waiting for terminating namespaces to be deleted...
Jan 13 08:19:47.837: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Jan 13 08:19:47.847: INFO: calico-node-vbqlj from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 08:19:47.847: INFO: csi-qingcloud-node-hxljj from kube-system started at 2022-01-13 07:38:08 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 08:19:47.847: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 08:19:47.847: INFO: kube-proxy-r2zxx from kube-system started at 2022-01-12 06:36:44 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 08:19:47.847: INFO: node-exporter-987g4 from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:19:47.847: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 08:19:47.847: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2022-01-13 07:38:48 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:19:47.847: INFO: 	Container prometheus ready: true, restart count 1
Jan 13 08:19:47.847: INFO: sonobuoy from sonobuoy started at 2022-01-13 07:33:31 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 13 08:19:47.847: INFO: sonobuoy-e2e-job-3cc758d0b1b14270 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container e2e ready: true, restart count 0
Jan 13 08:19:47.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 08:19:47.847: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-shz85 from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 08:19:47.847: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 13 08:19:47.847: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Jan 13 08:19:47.854: INFO: calico-node-nvdsl from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container calico-node ready: true, restart count 0
Jan 13 08:19:47.854: INFO: csi-qingcloud-controller-6f8f6458f7-vkmtw from kube-system started at 2022-01-12 18:28:18 +0000 UTC (5 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container csi-attacher ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container csi-provisioner ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container csi-resizer ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jan 13 08:19:47.854: INFO: csi-qingcloud-node-84m8s from kube-system started at 2022-01-12 06:37:21 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container csi-qingcloud ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container node-registrar ready: true, restart count 0
Jan 13 08:19:47.854: INFO: kube-proxy-gdx9n from kube-system started at 2022-01-12 06:36:37 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 13 08:19:47.854: INFO: metrics-server-687cb5444-gwn4h from kube-system started at 2022-01-12 06:37:45 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container metrics-server ready: true, restart count 0
Jan 13 08:19:47.854: INFO: snapshot-controller-0 from kube-system started at 2022-01-12 06:38:20 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 13 08:19:47.854: INFO: default-http-backend-5bf68ff9b8-gt7w6 from kubesphere-controls-system started at 2022-01-12 06:39:06 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container default-http-backend ready: true, restart count 0
Jan 13 08:19:47.854: INFO: kubectl-admin-6667774bb-lfhx9 from kubesphere-controls-system started at 2022-01-12 06:41:27 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container kubectl ready: true, restart count 0
Jan 13 08:19:47.854: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2022-01-12 18:28:22 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:19:47.854: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2022-01-12 06:40:50 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:19:47.854: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2022-01-12 17:16:42 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container alertmanager ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:19:47.854: INFO: kube-state-metrics-5547ddd4cc-79dkj from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (3 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 13 08:19:47.854: INFO: node-exporter-wjvtx from kubesphere-monitoring-system started at 2022-01-12 06:40:43 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container node-exporter ready: true, restart count 0
Jan 13 08:19:47.854: INFO: notification-manager-deployment-78664576cb-7cqhm from kubesphere-monitoring-system started at 2022-01-12 18:28:18 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container tenant ready: true, restart count 0
Jan 13 08:19:47.854: INFO: notification-manager-deployment-78664576cb-pppk6 from kubesphere-monitoring-system started at 2022-01-12 06:41:16 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container notification-manager ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container tenant ready: true, restart count 0
Jan 13 08:19:47.854: INFO: notification-manager-operator-7d44854f54-97b8n from kubesphere-monitoring-system started at 2022-01-12 06:41:10 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container notification-manager-operator ready: true, restart count 0
Jan 13 08:19:47.854: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2022-01-12 06:41:05 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container config-reloader ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container prometheus ready: true, restart count 0
Jan 13 08:19:47.854: INFO: prometheus-operator-5c5db79546-gl2sj from kubesphere-monitoring-system started at 2022-01-12 06:40:40 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 13 08:19:47.854: INFO: ks-installer-6cf7f47896-d9fqp from kubesphere-system started at 2022-01-12 06:37:23 +0000 UTC (1 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container installer ready: true, restart count 0
Jan 13 08:19:47.854: INFO: sonobuoy-systemd-logs-daemon-set-378748f8c3b74963-6f2nd from sonobuoy started at 2022-01-13 07:33:33 +0000 UTC (2 container statuses recorded)
Jan 13 08:19:47.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 13 08:19:47.854: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-24b7547c-a7a0-4ba6-8bac-f97e5e9f9f77 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-24b7547c-a7a0-4ba6-8bac-f97e5e9f9f77 off the node worker-s001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-24b7547c-a7a0-4ba6-8bac-f97e5e9f9f77
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:19:55.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2681" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.156 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":180,"skipped":2939,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:19:55.953: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:19:55.986: INFO: Creating ReplicaSet my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06
Jan 13 08:19:55.993: INFO: Pod name my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06: Found 0 pods out of 1
Jan 13 08:20:00.998: INFO: Pod name my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06: Found 1 pods out of 1
Jan 13 08:20:00.998: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06" is running
Jan 13 08:20:01.000: INFO: Pod "my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06-t69cw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 08:19:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 08:19:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 08:19:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 08:19:56 +0000 UTC Reason: Message:}])
Jan 13 08:20:01.000: INFO: Trying to dial the pod
Jan 13 08:20:06.013: INFO: Controller my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06: Got expected result from replica 1 [my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06-t69cw]: "my-hostname-basic-17cf007d-cdd9-4be6-8c6a-82a5d829ef06-t69cw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:20:06.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4023" for this suite.

• [SLOW TEST:10.066 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":181,"skipped":2943,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:20:06.019: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2893.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2893.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:20:10.093: INFO: DNS probes using dns-test-76d411b7-2a98-4e9c-8fc1-b5bc42704fb9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2893.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2893.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:20:14.148: INFO: File wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:14.150: INFO: File jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:14.150: INFO: Lookups using dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f failed for: [wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local]

Jan 13 08:20:19.156: INFO: File wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:19.159: INFO: File jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:19.159: INFO: Lookups using dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f failed for: [wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local]

Jan 13 08:20:24.158: INFO: File wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:24.160: INFO: File jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:24.160: INFO: Lookups using dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f failed for: [wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local]

Jan 13 08:20:29.154: INFO: File wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:29.156: INFO: File jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:29.156: INFO: Lookups using dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f failed for: [wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local]

Jan 13 08:20:34.155: INFO: File wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:34.157: INFO: File jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local from pod  dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 13 08:20:34.157: INFO: Lookups using dns-2893/dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f failed for: [wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local]

Jan 13 08:20:39.156: INFO: DNS probes using dns-test-8f7320d9-8a50-4a70-91ed-60ee895d3d7f succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2893.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2893.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2893.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2893.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:20:41.211: INFO: DNS probes using dns-test-8993b1e3-3a8a-467c-a0c1-a18a60ec7f25 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:20:41.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2893" for this suite.

• [SLOW TEST:35.232 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":182,"skipped":2961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:20:41.252: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 13 08:20:41.295: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 08:20:46.300: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:20:46.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8625" for this suite.

• [SLOW TEST:5.090 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":183,"skipped":2991,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:20:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:20:46.375: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6431
I0113 08:20:46.382586      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6431, replica count: 1
I0113 08:20:47.433542      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0113 08:20:48.434062      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 08:20:48.545: INFO: Created: latency-svc-fvwdw
Jan 13 08:20:48.552: INFO: Got endpoints: latency-svc-fvwdw [17.833769ms]
Jan 13 08:20:48.564: INFO: Created: latency-svc-9gtff
Jan 13 08:20:48.573: INFO: Created: latency-svc-pztkd
Jan 13 08:20:48.578: INFO: Got endpoints: latency-svc-9gtff [26.023998ms]
Jan 13 08:20:48.580: INFO: Created: latency-svc-mr8vf
Jan 13 08:20:48.583: INFO: Created: latency-svc-xt7fw
Jan 13 08:20:48.598: INFO: Got endpoints: latency-svc-xt7fw [45.387528ms]
Jan 13 08:20:48.598: INFO: Created: latency-svc-cbkwz
Jan 13 08:20:48.598: INFO: Got endpoints: latency-svc-pztkd [45.291823ms]
Jan 13 08:20:48.598: INFO: Got endpoints: latency-svc-mr8vf [45.760668ms]
Jan 13 08:20:48.603: INFO: Got endpoints: latency-svc-cbkwz [50.797321ms]
Jan 13 08:20:48.606: INFO: Created: latency-svc-gnw8d
Jan 13 08:20:48.612: INFO: Got endpoints: latency-svc-gnw8d [59.430539ms]
Jan 13 08:20:48.616: INFO: Created: latency-svc-hzczw
Jan 13 08:20:48.624: INFO: Created: latency-svc-kksmx
Jan 13 08:20:48.625: INFO: Got endpoints: latency-svc-hzczw [72.496222ms]
Jan 13 08:20:48.627: INFO: Created: latency-svc-ffcgk
Jan 13 08:20:48.631: INFO: Created: latency-svc-w55kc
Jan 13 08:20:48.640: INFO: Got endpoints: latency-svc-ffcgk [87.598177ms]
Jan 13 08:20:48.640: INFO: Got endpoints: latency-svc-w55kc [87.591038ms]
Jan 13 08:20:48.640: INFO: Got endpoints: latency-svc-kksmx [87.675198ms]
Jan 13 08:20:48.640: INFO: Created: latency-svc-c45vl
Jan 13 08:20:48.644: INFO: Created: latency-svc-kt5px
Jan 13 08:20:48.649: INFO: Got endpoints: latency-svc-c45vl [96.430452ms]
Jan 13 08:20:48.649: INFO: Created: latency-svc-xwkvx
Jan 13 08:20:48.652: INFO: Created: latency-svc-4c5sf
Jan 13 08:20:48.652: INFO: Got endpoints: latency-svc-kt5px [99.615351ms]
Jan 13 08:20:48.657: INFO: Created: latency-svc-kkvhl
Jan 13 08:20:48.657: INFO: Got endpoints: latency-svc-xwkvx [105.063784ms]
Jan 13 08:20:48.658: INFO: Got endpoints: latency-svc-4c5sf [105.688528ms]
Jan 13 08:20:48.658: INFO: Got endpoints: latency-svc-kkvhl [105.98543ms]
Jan 13 08:20:48.664: INFO: Created: latency-svc-fc679
Jan 13 08:20:48.667: INFO: Got endpoints: latency-svc-fc679 [88.880099ms]
Jan 13 08:20:48.670: INFO: Created: latency-svc-jxgff
Jan 13 08:20:48.672: INFO: Created: latency-svc-vljt4
Jan 13 08:20:48.675: INFO: Got endpoints: latency-svc-jxgff [76.737197ms]
Jan 13 08:20:48.678: INFO: Got endpoints: latency-svc-vljt4 [80.367005ms]
Jan 13 08:20:48.680: INFO: Created: latency-svc-nxt8n
Jan 13 08:20:48.685: INFO: Created: latency-svc-txgg5
Jan 13 08:20:48.686: INFO: Got endpoints: latency-svc-nxt8n [87.569166ms]
Jan 13 08:20:48.688: INFO: Got endpoints: latency-svc-txgg5 [84.857186ms]
Jan 13 08:20:48.690: INFO: Created: latency-svc-fbn9d
Jan 13 08:20:48.693: INFO: Created: latency-svc-v46bl
Jan 13 08:20:48.705: INFO: Created: latency-svc-pthmn
Jan 13 08:20:48.705: INFO: Got endpoints: latency-svc-fbn9d [92.762899ms]
Jan 13 08:20:48.705: INFO: Got endpoints: latency-svc-v46bl [79.802895ms]
Jan 13 08:20:48.723: INFO: Got endpoints: latency-svc-pthmn [82.873237ms]
Jan 13 08:20:48.724: INFO: Created: latency-svc-hhvc8
Jan 13 08:20:48.730: INFO: Got endpoints: latency-svc-hhvc8 [90.338922ms]
Jan 13 08:20:48.732: INFO: Created: latency-svc-jvg44
Jan 13 08:20:48.734: INFO: Created: latency-svc-6p6q2
Jan 13 08:20:48.736: INFO: Got endpoints: latency-svc-jvg44 [95.934926ms]
Jan 13 08:20:48.740: INFO: Created: latency-svc-zcbqn
Jan 13 08:20:48.741: INFO: Got endpoints: latency-svc-6p6q2 [91.851575ms]
Jan 13 08:20:48.742: INFO: Created: latency-svc-fxqp9
Jan 13 08:20:48.745: INFO: Created: latency-svc-vgpgj
Jan 13 08:20:48.746: INFO: Got endpoints: latency-svc-zcbqn [93.633406ms]
Jan 13 08:20:48.752: INFO: Got endpoints: latency-svc-vgpgj [94.293291ms]
Jan 13 08:20:48.752: INFO: Got endpoints: latency-svc-fxqp9 [94.776123ms]
Jan 13 08:20:48.754: INFO: Created: latency-svc-cktsm
Jan 13 08:20:48.759: INFO: Got endpoints: latency-svc-cktsm [100.235338ms]
Jan 13 08:20:48.761: INFO: Created: latency-svc-v7rln
Jan 13 08:20:48.762: INFO: Got endpoints: latency-svc-v7rln [94.685928ms]
Jan 13 08:20:48.765: INFO: Created: latency-svc-m6jk4
Jan 13 08:20:48.772: INFO: Got endpoints: latency-svc-m6jk4 [97.012624ms]
Jan 13 08:20:48.778: INFO: Created: latency-svc-vr8xv
Jan 13 08:20:48.782: INFO: Got endpoints: latency-svc-vr8xv [103.824164ms]
Jan 13 08:20:48.786: INFO: Created: latency-svc-c9f9n
Jan 13 08:20:48.794: INFO: Created: latency-svc-sc925
Jan 13 08:20:48.795: INFO: Created: latency-svc-hslk2
Jan 13 08:20:48.801: INFO: Created: latency-svc-d9v4r
Jan 13 08:20:48.802: INFO: Got endpoints: latency-svc-c9f9n [116.356804ms]
Jan 13 08:20:48.804: INFO: Created: latency-svc-gdhsb
Jan 13 08:20:48.811: INFO: Created: latency-svc-nkj8b
Jan 13 08:20:48.816: INFO: Created: latency-svc-9kkqr
Jan 13 08:20:48.819: INFO: Created: latency-svc-tcfw5
Jan 13 08:20:48.823: INFO: Created: latency-svc-74rnn
Jan 13 08:20:48.827: INFO: Created: latency-svc-vvt86
Jan 13 08:20:48.829: INFO: Created: latency-svc-6pcf6
Jan 13 08:20:48.843: INFO: Created: latency-svc-mbfgm
Jan 13 08:20:48.844: INFO: Created: latency-svc-fbwkr
Jan 13 08:20:48.850: INFO: Created: latency-svc-w29gt
Jan 13 08:20:48.853: INFO: Got endpoints: latency-svc-sc925 [165.306522ms]
Jan 13 08:20:48.857: INFO: Created: latency-svc-zpwtj
Jan 13 08:20:48.873: INFO: Created: latency-svc-dw54q
Jan 13 08:20:48.873: INFO: Created: latency-svc-888nh
Jan 13 08:20:48.900: INFO: Got endpoints: latency-svc-hslk2 [195.146961ms]
Jan 13 08:20:48.908: INFO: Created: latency-svc-v9r4z
Jan 13 08:20:48.953: INFO: Got endpoints: latency-svc-d9v4r [247.952281ms]
Jan 13 08:20:48.958: INFO: Created: latency-svc-ndf77
Jan 13 08:20:49.000: INFO: Got endpoints: latency-svc-gdhsb [277.459001ms]
Jan 13 08:20:49.011: INFO: Created: latency-svc-7pk45
Jan 13 08:20:49.057: INFO: Got endpoints: latency-svc-nkj8b [326.056228ms]
Jan 13 08:20:49.064: INFO: Created: latency-svc-65kks
Jan 13 08:20:49.100: INFO: Got endpoints: latency-svc-9kkqr [364.319211ms]
Jan 13 08:20:49.120: INFO: Created: latency-svc-rnnww
Jan 13 08:20:49.151: INFO: Got endpoints: latency-svc-tcfw5 [409.88151ms]
Jan 13 08:20:49.156: INFO: Created: latency-svc-w872n
Jan 13 08:20:49.201: INFO: Got endpoints: latency-svc-74rnn [455.343413ms]
Jan 13 08:20:49.207: INFO: Created: latency-svc-87sgr
Jan 13 08:20:49.250: INFO: Got endpoints: latency-svc-vvt86 [497.866388ms]
Jan 13 08:20:49.262: INFO: Created: latency-svc-rvvjl
Jan 13 08:20:49.304: INFO: Got endpoints: latency-svc-6pcf6 [551.709503ms]
Jan 13 08:20:49.310: INFO: Created: latency-svc-9w5zq
Jan 13 08:20:49.360: INFO: Got endpoints: latency-svc-mbfgm [601.925251ms]
Jan 13 08:20:49.370: INFO: Created: latency-svc-9g8d9
Jan 13 08:20:49.403: INFO: Got endpoints: latency-svc-fbwkr [640.601471ms]
Jan 13 08:20:49.411: INFO: Created: latency-svc-xmxzp
Jan 13 08:20:49.450: INFO: Got endpoints: latency-svc-w29gt [678.570821ms]
Jan 13 08:20:49.457: INFO: Created: latency-svc-h4p4w
Jan 13 08:20:49.503: INFO: Got endpoints: latency-svc-zpwtj [720.576072ms]
Jan 13 08:20:49.512: INFO: Created: latency-svc-lzfqh
Jan 13 08:20:49.550: INFO: Got endpoints: latency-svc-888nh [747.880057ms]
Jan 13 08:20:49.558: INFO: Created: latency-svc-zmkrv
Jan 13 08:20:49.606: INFO: Got endpoints: latency-svc-dw54q [752.071036ms]
Jan 13 08:20:49.612: INFO: Created: latency-svc-vxcpm
Jan 13 08:20:49.653: INFO: Got endpoints: latency-svc-v9r4z [753.347014ms]
Jan 13 08:20:49.659: INFO: Created: latency-svc-6v4c9
Jan 13 08:20:49.704: INFO: Got endpoints: latency-svc-ndf77 [751.005615ms]
Jan 13 08:20:49.713: INFO: Created: latency-svc-gwxj5
Jan 13 08:20:49.752: INFO: Got endpoints: latency-svc-7pk45 [751.49137ms]
Jan 13 08:20:49.758: INFO: Created: latency-svc-4cjv7
Jan 13 08:20:49.802: INFO: Got endpoints: latency-svc-65kks [745.105653ms]
Jan 13 08:20:49.808: INFO: Created: latency-svc-ssv8s
Jan 13 08:20:49.853: INFO: Got endpoints: latency-svc-rnnww [752.6984ms]
Jan 13 08:20:49.861: INFO: Created: latency-svc-6948s
Jan 13 08:20:49.900: INFO: Got endpoints: latency-svc-w872n [749.582993ms]
Jan 13 08:20:49.907: INFO: Created: latency-svc-vdgzc
Jan 13 08:20:49.952: INFO: Got endpoints: latency-svc-87sgr [751.339606ms]
Jan 13 08:20:49.961: INFO: Created: latency-svc-9m4k2
Jan 13 08:20:50.011: INFO: Got endpoints: latency-svc-rvvjl [760.741311ms]
Jan 13 08:20:50.018: INFO: Created: latency-svc-wmgx7
Jan 13 08:20:50.053: INFO: Got endpoints: latency-svc-9w5zq [749.164099ms]
Jan 13 08:20:50.062: INFO: Created: latency-svc-whxb5
Jan 13 08:20:50.105: INFO: Got endpoints: latency-svc-9g8d9 [744.042551ms]
Jan 13 08:20:50.112: INFO: Created: latency-svc-mm4kz
Jan 13 08:20:50.152: INFO: Got endpoints: latency-svc-xmxzp [749.562782ms]
Jan 13 08:20:50.161: INFO: Created: latency-svc-2zbzg
Jan 13 08:20:50.200: INFO: Got endpoints: latency-svc-h4p4w [750.177ms]
Jan 13 08:20:50.220: INFO: Created: latency-svc-xzf8g
Jan 13 08:20:50.255: INFO: Got endpoints: latency-svc-lzfqh [752.187117ms]
Jan 13 08:20:50.263: INFO: Created: latency-svc-dn8h9
Jan 13 08:20:50.302: INFO: Got endpoints: latency-svc-zmkrv [752.430522ms]
Jan 13 08:20:50.309: INFO: Created: latency-svc-8d2zv
Jan 13 08:20:50.355: INFO: Got endpoints: latency-svc-vxcpm [749.886195ms]
Jan 13 08:20:50.362: INFO: Created: latency-svc-hw4pc
Jan 13 08:20:50.403: INFO: Got endpoints: latency-svc-6v4c9 [749.66139ms]
Jan 13 08:20:50.412: INFO: Created: latency-svc-lssxg
Jan 13 08:20:50.450: INFO: Got endpoints: latency-svc-gwxj5 [746.600146ms]
Jan 13 08:20:50.472: INFO: Created: latency-svc-rplx2
Jan 13 08:20:50.502: INFO: Got endpoints: latency-svc-4cjv7 [750.414557ms]
Jan 13 08:20:50.509: INFO: Created: latency-svc-vfm5w
Jan 13 08:20:50.557: INFO: Got endpoints: latency-svc-ssv8s [755.058298ms]
Jan 13 08:20:50.565: INFO: Created: latency-svc-gh7w6
Jan 13 08:20:50.603: INFO: Got endpoints: latency-svc-6948s [750.353467ms]
Jan 13 08:20:50.609: INFO: Created: latency-svc-tfbxw
Jan 13 08:20:50.654: INFO: Got endpoints: latency-svc-vdgzc [753.60514ms]
Jan 13 08:20:50.664: INFO: Created: latency-svc-2xpw4
Jan 13 08:20:50.703: INFO: Got endpoints: latency-svc-9m4k2 [750.994791ms]
Jan 13 08:20:50.710: INFO: Created: latency-svc-zw8bq
Jan 13 08:20:50.751: INFO: Got endpoints: latency-svc-wmgx7 [740.371743ms]
Jan 13 08:20:50.758: INFO: Created: latency-svc-cmsw5
Jan 13 08:20:50.802: INFO: Got endpoints: latency-svc-whxb5 [749.18728ms]
Jan 13 08:20:50.809: INFO: Created: latency-svc-jfvl8
Jan 13 08:20:50.851: INFO: Got endpoints: latency-svc-mm4kz [746.026417ms]
Jan 13 08:20:50.857: INFO: Created: latency-svc-6dwp6
Jan 13 08:20:50.901: INFO: Got endpoints: latency-svc-2zbzg [749.159258ms]
Jan 13 08:20:50.922: INFO: Created: latency-svc-lbwtq
Jan 13 08:20:50.951: INFO: Got endpoints: latency-svc-xzf8g [750.665849ms]
Jan 13 08:20:50.957: INFO: Created: latency-svc-25vzq
Jan 13 08:20:51.000: INFO: Got endpoints: latency-svc-dn8h9 [744.733108ms]
Jan 13 08:20:51.007: INFO: Created: latency-svc-lrshs
Jan 13 08:20:51.052: INFO: Got endpoints: latency-svc-8d2zv [749.594613ms]
Jan 13 08:20:51.059: INFO: Created: latency-svc-zlgjd
Jan 13 08:20:51.106: INFO: Got endpoints: latency-svc-hw4pc [750.257508ms]
Jan 13 08:20:51.112: INFO: Created: latency-svc-g67pc
Jan 13 08:20:51.152: INFO: Got endpoints: latency-svc-lssxg [749.253709ms]
Jan 13 08:20:51.158: INFO: Created: latency-svc-6gjhm
Jan 13 08:20:51.203: INFO: Got endpoints: latency-svc-rplx2 [752.205235ms]
Jan 13 08:20:51.217: INFO: Created: latency-svc-9w7vn
Jan 13 08:20:51.250: INFO: Got endpoints: latency-svc-vfm5w [747.640072ms]
Jan 13 08:20:51.259: INFO: Created: latency-svc-m45jr
Jan 13 08:20:51.302: INFO: Got endpoints: latency-svc-gh7w6 [745.518684ms]
Jan 13 08:20:51.312: INFO: Created: latency-svc-9gwt9
Jan 13 08:20:51.369: INFO: Got endpoints: latency-svc-tfbxw [765.279918ms]
Jan 13 08:20:51.387: INFO: Created: latency-svc-tczbf
Jan 13 08:20:51.402: INFO: Got endpoints: latency-svc-2xpw4 [748.073741ms]
Jan 13 08:20:51.412: INFO: Created: latency-svc-fjnnt
Jan 13 08:20:51.455: INFO: Got endpoints: latency-svc-zw8bq [751.446907ms]
Jan 13 08:20:51.473: INFO: Created: latency-svc-b2722
Jan 13 08:20:51.502: INFO: Got endpoints: latency-svc-cmsw5 [751.132723ms]
Jan 13 08:20:51.508: INFO: Created: latency-svc-dlqzb
Jan 13 08:20:51.553: INFO: Got endpoints: latency-svc-jfvl8 [750.631702ms]
Jan 13 08:20:51.562: INFO: Created: latency-svc-r9fnm
Jan 13 08:20:51.602: INFO: Got endpoints: latency-svc-6dwp6 [751.275371ms]
Jan 13 08:20:51.608: INFO: Created: latency-svc-tdjsd
Jan 13 08:20:51.653: INFO: Got endpoints: latency-svc-lbwtq [751.391791ms]
Jan 13 08:20:51.660: INFO: Created: latency-svc-pxmmf
Jan 13 08:20:51.708: INFO: Got endpoints: latency-svc-25vzq [756.573275ms]
Jan 13 08:20:51.715: INFO: Created: latency-svc-hmgm5
Jan 13 08:20:51.753: INFO: Got endpoints: latency-svc-lrshs [752.991322ms]
Jan 13 08:20:51.765: INFO: Created: latency-svc-pvmjj
Jan 13 08:20:51.800: INFO: Got endpoints: latency-svc-zlgjd [747.966582ms]
Jan 13 08:20:51.809: INFO: Created: latency-svc-l4zgd
Jan 13 08:20:51.869: INFO: Got endpoints: latency-svc-g67pc [762.764882ms]
Jan 13 08:20:51.874: INFO: Created: latency-svc-rc7p9
Jan 13 08:20:51.901: INFO: Got endpoints: latency-svc-6gjhm [748.583839ms]
Jan 13 08:20:51.907: INFO: Created: latency-svc-6m2qk
Jan 13 08:20:51.950: INFO: Got endpoints: latency-svc-9w7vn [746.932074ms]
Jan 13 08:20:51.957: INFO: Created: latency-svc-k9wwj
Jan 13 08:20:52.000: INFO: Got endpoints: latency-svc-m45jr [750.023757ms]
Jan 13 08:20:52.008: INFO: Created: latency-svc-6qn72
Jan 13 08:20:52.050: INFO: Got endpoints: latency-svc-9gwt9 [747.840687ms]
Jan 13 08:20:52.058: INFO: Created: latency-svc-zrrqv
Jan 13 08:20:52.100: INFO: Got endpoints: latency-svc-tczbf [731.007619ms]
Jan 13 08:20:52.107: INFO: Created: latency-svc-h5xq8
Jan 13 08:20:52.155: INFO: Got endpoints: latency-svc-fjnnt [752.635666ms]
Jan 13 08:20:52.160: INFO: Created: latency-svc-st9n6
Jan 13 08:20:52.200: INFO: Got endpoints: latency-svc-b2722 [745.350943ms]
Jan 13 08:20:52.207: INFO: Created: latency-svc-77q42
Jan 13 08:20:52.250: INFO: Got endpoints: latency-svc-dlqzb [747.581056ms]
Jan 13 08:20:52.256: INFO: Created: latency-svc-r42sj
Jan 13 08:20:52.300: INFO: Got endpoints: latency-svc-r9fnm [746.887078ms]
Jan 13 08:20:52.311: INFO: Created: latency-svc-2nqth
Jan 13 08:20:52.352: INFO: Got endpoints: latency-svc-tdjsd [749.996613ms]
Jan 13 08:20:52.357: INFO: Created: latency-svc-9tcl6
Jan 13 08:20:52.401: INFO: Got endpoints: latency-svc-pxmmf [748.229672ms]
Jan 13 08:20:52.407: INFO: Created: latency-svc-cn4bw
Jan 13 08:20:52.454: INFO: Got endpoints: latency-svc-hmgm5 [746.414657ms]
Jan 13 08:20:52.461: INFO: Created: latency-svc-vp82h
Jan 13 08:20:52.501: INFO: Got endpoints: latency-svc-pvmjj [747.653357ms]
Jan 13 08:20:52.508: INFO: Created: latency-svc-bmzd5
Jan 13 08:20:52.562: INFO: Got endpoints: latency-svc-l4zgd [762.480496ms]
Jan 13 08:20:52.570: INFO: Created: latency-svc-sdnvm
Jan 13 08:20:52.601: INFO: Got endpoints: latency-svc-rc7p9 [732.118355ms]
Jan 13 08:20:52.611: INFO: Created: latency-svc-s9l82
Jan 13 08:20:52.652: INFO: Got endpoints: latency-svc-6m2qk [750.848812ms]
Jan 13 08:20:52.657: INFO: Created: latency-svc-2475l
Jan 13 08:20:52.703: INFO: Got endpoints: latency-svc-k9wwj [753.477655ms]
Jan 13 08:20:52.710: INFO: Created: latency-svc-2xg46
Jan 13 08:20:52.753: INFO: Got endpoints: latency-svc-6qn72 [752.663266ms]
Jan 13 08:20:52.758: INFO: Created: latency-svc-zp92j
Jan 13 08:20:52.801: INFO: Got endpoints: latency-svc-zrrqv [750.543389ms]
Jan 13 08:20:52.810: INFO: Created: latency-svc-m5pps
Jan 13 08:20:52.851: INFO: Got endpoints: latency-svc-h5xq8 [751.19754ms]
Jan 13 08:20:52.856: INFO: Created: latency-svc-5m7wp
Jan 13 08:20:52.901: INFO: Got endpoints: latency-svc-st9n6 [745.998907ms]
Jan 13 08:20:52.908: INFO: Created: latency-svc-pwbpm
Jan 13 08:20:52.949: INFO: Got endpoints: latency-svc-77q42 [748.952669ms]
Jan 13 08:20:52.955: INFO: Created: latency-svc-lcqft
Jan 13 08:20:53.001: INFO: Got endpoints: latency-svc-r42sj [751.330955ms]
Jan 13 08:20:53.013: INFO: Created: latency-svc-579dc
Jan 13 08:20:53.050: INFO: Got endpoints: latency-svc-2nqth [750.256656ms]
Jan 13 08:20:53.057: INFO: Created: latency-svc-vwjh6
Jan 13 08:20:53.103: INFO: Got endpoints: latency-svc-9tcl6 [751.200502ms]
Jan 13 08:20:53.118: INFO: Created: latency-svc-xnxtb
Jan 13 08:20:53.151: INFO: Got endpoints: latency-svc-cn4bw [750.135381ms]
Jan 13 08:20:53.158: INFO: Created: latency-svc-6cpps
Jan 13 08:20:53.207: INFO: Got endpoints: latency-svc-vp82h [752.259032ms]
Jan 13 08:20:53.215: INFO: Created: latency-svc-tw8qz
Jan 13 08:20:53.255: INFO: Got endpoints: latency-svc-bmzd5 [754.181685ms]
Jan 13 08:20:53.261: INFO: Created: latency-svc-t6fcf
Jan 13 08:20:53.303: INFO: Got endpoints: latency-svc-sdnvm [740.641908ms]
Jan 13 08:20:53.311: INFO: Created: latency-svc-b9wgk
Jan 13 08:20:53.353: INFO: Got endpoints: latency-svc-s9l82 [752.508133ms]
Jan 13 08:20:53.359: INFO: Created: latency-svc-z4nfw
Jan 13 08:20:53.404: INFO: Got endpoints: latency-svc-2475l [752.350165ms]
Jan 13 08:20:53.412: INFO: Created: latency-svc-rbf6p
Jan 13 08:20:53.456: INFO: Got endpoints: latency-svc-2xg46 [753.154804ms]
Jan 13 08:20:53.468: INFO: Created: latency-svc-ggsm6
Jan 13 08:20:53.500: INFO: Got endpoints: latency-svc-zp92j [747.427616ms]
Jan 13 08:20:53.516: INFO: Created: latency-svc-tnwht
Jan 13 08:20:53.552: INFO: Got endpoints: latency-svc-m5pps [751.453392ms]
Jan 13 08:20:53.559: INFO: Created: latency-svc-xkpq4
Jan 13 08:20:53.601: INFO: Got endpoints: latency-svc-5m7wp [750.231877ms]
Jan 13 08:20:53.613: INFO: Created: latency-svc-wjbkz
Jan 13 08:20:53.650: INFO: Got endpoints: latency-svc-pwbpm [749.470603ms]
Jan 13 08:20:53.658: INFO: Created: latency-svc-4k9gk
Jan 13 08:20:53.701: INFO: Got endpoints: latency-svc-lcqft [751.586445ms]
Jan 13 08:20:53.709: INFO: Created: latency-svc-k77ls
Jan 13 08:20:53.750: INFO: Got endpoints: latency-svc-579dc [748.199941ms]
Jan 13 08:20:53.756: INFO: Created: latency-svc-dwt7q
Jan 13 08:20:53.800: INFO: Got endpoints: latency-svc-vwjh6 [749.267129ms]
Jan 13 08:20:53.808: INFO: Created: latency-svc-g29pk
Jan 13 08:20:53.851: INFO: Got endpoints: latency-svc-xnxtb [747.517548ms]
Jan 13 08:20:53.856: INFO: Created: latency-svc-ndhh9
Jan 13 08:20:53.903: INFO: Got endpoints: latency-svc-6cpps [751.637135ms]
Jan 13 08:20:53.913: INFO: Created: latency-svc-2mqxs
Jan 13 08:20:53.950: INFO: Got endpoints: latency-svc-tw8qz [743.244425ms]
Jan 13 08:20:53.956: INFO: Created: latency-svc-vjhw9
Jan 13 08:20:54.002: INFO: Got endpoints: latency-svc-t6fcf [747.27018ms]
Jan 13 08:20:54.010: INFO: Created: latency-svc-lf7ss
Jan 13 08:20:54.050: INFO: Got endpoints: latency-svc-b9wgk [747.05279ms]
Jan 13 08:20:54.060: INFO: Created: latency-svc-msd6n
Jan 13 08:20:54.102: INFO: Got endpoints: latency-svc-z4nfw [748.326808ms]
Jan 13 08:20:54.109: INFO: Created: latency-svc-x5v8c
Jan 13 08:20:54.150: INFO: Got endpoints: latency-svc-rbf6p [746.106809ms]
Jan 13 08:20:54.156: INFO: Created: latency-svc-dq5nv
Jan 13 08:20:54.201: INFO: Got endpoints: latency-svc-ggsm6 [744.764034ms]
Jan 13 08:20:54.210: INFO: Created: latency-svc-vxbhw
Jan 13 08:20:54.256: INFO: Got endpoints: latency-svc-tnwht [755.385708ms]
Jan 13 08:20:54.261: INFO: Created: latency-svc-wd6j5
Jan 13 08:20:54.302: INFO: Got endpoints: latency-svc-xkpq4 [749.261516ms]
Jan 13 08:20:54.309: INFO: Created: latency-svc-8vrcl
Jan 13 08:20:54.352: INFO: Got endpoints: latency-svc-wjbkz [751.020729ms]
Jan 13 08:20:54.361: INFO: Created: latency-svc-bn926
Jan 13 08:20:54.401: INFO: Got endpoints: latency-svc-4k9gk [750.636381ms]
Jan 13 08:20:54.408: INFO: Created: latency-svc-cqwts
Jan 13 08:20:54.450: INFO: Got endpoints: latency-svc-k77ls [748.9277ms]
Jan 13 08:20:54.461: INFO: Created: latency-svc-479zw
Jan 13 08:20:54.503: INFO: Got endpoints: latency-svc-dwt7q [753.114933ms]
Jan 13 08:20:54.510: INFO: Created: latency-svc-kqnn9
Jan 13 08:20:54.554: INFO: Got endpoints: latency-svc-g29pk [754.213459ms]
Jan 13 08:20:54.561: INFO: Created: latency-svc-wxk4r
Jan 13 08:20:54.600: INFO: Got endpoints: latency-svc-ndhh9 [749.031695ms]
Jan 13 08:20:54.608: INFO: Created: latency-svc-2sfk7
Jan 13 08:20:54.652: INFO: Got endpoints: latency-svc-2mqxs [748.631789ms]
Jan 13 08:20:54.660: INFO: Created: latency-svc-vxwj6
Jan 13 08:20:54.700: INFO: Got endpoints: latency-svc-vjhw9 [749.881444ms]
Jan 13 08:20:54.706: INFO: Created: latency-svc-fxjs9
Jan 13 08:20:54.750: INFO: Got endpoints: latency-svc-lf7ss [747.715719ms]
Jan 13 08:20:54.759: INFO: Created: latency-svc-nqsdd
Jan 13 08:20:54.800: INFO: Got endpoints: latency-svc-msd6n [749.876197ms]
Jan 13 08:20:54.806: INFO: Created: latency-svc-7g55t
Jan 13 08:20:54.850: INFO: Got endpoints: latency-svc-x5v8c [748.148699ms]
Jan 13 08:20:54.858: INFO: Created: latency-svc-sv57q
Jan 13 08:20:54.901: INFO: Got endpoints: latency-svc-dq5nv [750.530729ms]
Jan 13 08:20:54.906: INFO: Created: latency-svc-f76r8
Jan 13 08:20:54.950: INFO: Got endpoints: latency-svc-vxbhw [749.264251ms]
Jan 13 08:20:54.956: INFO: Created: latency-svc-cfzb6
Jan 13 08:20:54.999: INFO: Got endpoints: latency-svc-wd6j5 [743.688144ms]
Jan 13 08:20:55.007: INFO: Created: latency-svc-xxsmg
Jan 13 08:20:55.052: INFO: Got endpoints: latency-svc-8vrcl [750.159949ms]
Jan 13 08:20:55.057: INFO: Created: latency-svc-8xgl7
Jan 13 08:20:55.106: INFO: Got endpoints: latency-svc-bn926 [753.306468ms]
Jan 13 08:20:55.112: INFO: Created: latency-svc-qcpbb
Jan 13 08:20:55.150: INFO: Got endpoints: latency-svc-cqwts [748.862749ms]
Jan 13 08:20:55.162: INFO: Created: latency-svc-5dvcm
Jan 13 08:20:55.199: INFO: Got endpoints: latency-svc-479zw [749.420267ms]
Jan 13 08:20:55.206: INFO: Created: latency-svc-gczms
Jan 13 08:20:55.266: INFO: Got endpoints: latency-svc-kqnn9 [763.442091ms]
Jan 13 08:20:55.274: INFO: Created: latency-svc-jmx5j
Jan 13 08:20:55.301: INFO: Got endpoints: latency-svc-wxk4r [747.366718ms]
Jan 13 08:20:55.308: INFO: Created: latency-svc-9wms2
Jan 13 08:20:55.353: INFO: Got endpoints: latency-svc-2sfk7 [753.316461ms]
Jan 13 08:20:55.363: INFO: Created: latency-svc-24kwx
Jan 13 08:20:55.401: INFO: Got endpoints: latency-svc-vxwj6 [749.456688ms]
Jan 13 08:20:55.408: INFO: Created: latency-svc-brrct
Jan 13 08:20:55.455: INFO: Got endpoints: latency-svc-fxjs9 [754.833111ms]
Jan 13 08:20:55.467: INFO: Created: latency-svc-r7vbb
Jan 13 08:20:55.501: INFO: Got endpoints: latency-svc-nqsdd [751.017264ms]
Jan 13 08:20:55.509: INFO: Created: latency-svc-kqdtj
Jan 13 08:20:55.553: INFO: Got endpoints: latency-svc-7g55t [753.212977ms]
Jan 13 08:20:55.561: INFO: Created: latency-svc-4g8bd
Jan 13 08:20:55.604: INFO: Got endpoints: latency-svc-sv57q [754.227669ms]
Jan 13 08:20:55.610: INFO: Created: latency-svc-cq77x
Jan 13 08:20:55.651: INFO: Got endpoints: latency-svc-f76r8 [750.241373ms]
Jan 13 08:20:55.666: INFO: Created: latency-svc-sxj4c
Jan 13 08:20:55.705: INFO: Got endpoints: latency-svc-cfzb6 [754.786972ms]
Jan 13 08:20:55.713: INFO: Created: latency-svc-mtcvv
Jan 13 08:20:55.752: INFO: Got endpoints: latency-svc-xxsmg [752.736293ms]
Jan 13 08:20:55.766: INFO: Created: latency-svc-b69tj
Jan 13 08:20:55.803: INFO: Got endpoints: latency-svc-8xgl7 [751.217585ms]
Jan 13 08:20:55.812: INFO: Created: latency-svc-rb6lz
Jan 13 08:20:55.852: INFO: Got endpoints: latency-svc-qcpbb [746.764582ms]
Jan 13 08:20:55.884: INFO: Created: latency-svc-4bwt8
Jan 13 08:20:55.902: INFO: Got endpoints: latency-svc-5dvcm [752.485014ms]
Jan 13 08:20:55.916: INFO: Created: latency-svc-l56gm
Jan 13 08:20:55.950: INFO: Got endpoints: latency-svc-gczms [751.150376ms]
Jan 13 08:20:55.958: INFO: Created: latency-svc-c84v4
Jan 13 08:20:56.001: INFO: Got endpoints: latency-svc-jmx5j [734.557663ms]
Jan 13 08:20:56.016: INFO: Created: latency-svc-cbw58
Jan 13 08:20:56.050: INFO: Got endpoints: latency-svc-9wms2 [749.075773ms]
Jan 13 08:20:56.060: INFO: Created: latency-svc-28rxp
Jan 13 08:20:56.103: INFO: Got endpoints: latency-svc-24kwx [750.239863ms]
Jan 13 08:20:56.110: INFO: Created: latency-svc-r48ct
Jan 13 08:20:56.161: INFO: Got endpoints: latency-svc-brrct [760.081993ms]
Jan 13 08:20:56.169: INFO: Created: latency-svc-xvhfb
Jan 13 08:20:56.203: INFO: Got endpoints: latency-svc-r7vbb [748.201944ms]
Jan 13 08:20:56.212: INFO: Created: latency-svc-85x2s
Jan 13 08:20:56.250: INFO: Got endpoints: latency-svc-kqdtj [749.248712ms]
Jan 13 08:20:56.260: INFO: Created: latency-svc-nq7b4
Jan 13 08:20:56.304: INFO: Got endpoints: latency-svc-4g8bd [750.539396ms]
Jan 13 08:20:56.317: INFO: Created: latency-svc-m5nxz
Jan 13 08:20:56.353: INFO: Got endpoints: latency-svc-cq77x [748.936127ms]
Jan 13 08:20:56.359: INFO: Created: latency-svc-cn7zb
Jan 13 08:20:56.400: INFO: Got endpoints: latency-svc-sxj4c [749.339864ms]
Jan 13 08:20:56.452: INFO: Got endpoints: latency-svc-mtcvv [746.564889ms]
Jan 13 08:20:56.500: INFO: Got endpoints: latency-svc-b69tj [748.270242ms]
Jan 13 08:20:56.553: INFO: Got endpoints: latency-svc-rb6lz [749.884915ms]
Jan 13 08:20:56.606: INFO: Got endpoints: latency-svc-4bwt8 [753.675933ms]
Jan 13 08:20:56.652: INFO: Got endpoints: latency-svc-l56gm [750.121685ms]
Jan 13 08:20:56.701: INFO: Got endpoints: latency-svc-c84v4 [750.718869ms]
Jan 13 08:20:56.753: INFO: Got endpoints: latency-svc-cbw58 [752.430586ms]
Jan 13 08:20:56.804: INFO: Got endpoints: latency-svc-28rxp [753.935339ms]
Jan 13 08:20:56.853: INFO: Got endpoints: latency-svc-r48ct [749.45668ms]
Jan 13 08:20:56.905: INFO: Got endpoints: latency-svc-xvhfb [743.47196ms]
Jan 13 08:20:56.954: INFO: Got endpoints: latency-svc-85x2s [751.235681ms]
Jan 13 08:20:57.002: INFO: Got endpoints: latency-svc-nq7b4 [751.93523ms]
Jan 13 08:20:57.050: INFO: Got endpoints: latency-svc-m5nxz [746.300328ms]
Jan 13 08:20:57.101: INFO: Got endpoints: latency-svc-cn7zb [748.297351ms]
Jan 13 08:20:57.101: INFO: Latencies: [26.023998ms 45.291823ms 45.387528ms 45.760668ms 50.797321ms 59.430539ms 72.496222ms 76.737197ms 79.802895ms 80.367005ms 82.873237ms 84.857186ms 87.569166ms 87.591038ms 87.598177ms 87.675198ms 88.880099ms 90.338922ms 91.851575ms 92.762899ms 93.633406ms 94.293291ms 94.685928ms 94.776123ms 95.934926ms 96.430452ms 97.012624ms 99.615351ms 100.235338ms 103.824164ms 105.063784ms 105.688528ms 105.98543ms 116.356804ms 165.306522ms 195.146961ms 247.952281ms 277.459001ms 326.056228ms 364.319211ms 409.88151ms 455.343413ms 497.866388ms 551.709503ms 601.925251ms 640.601471ms 678.570821ms 720.576072ms 731.007619ms 732.118355ms 734.557663ms 740.371743ms 740.641908ms 743.244425ms 743.47196ms 743.688144ms 744.042551ms 744.733108ms 744.764034ms 745.105653ms 745.350943ms 745.518684ms 745.998907ms 746.026417ms 746.106809ms 746.300328ms 746.414657ms 746.564889ms 746.600146ms 746.764582ms 746.887078ms 746.932074ms 747.05279ms 747.27018ms 747.366718ms 747.427616ms 747.517548ms 747.581056ms 747.640072ms 747.653357ms 747.715719ms 747.840687ms 747.880057ms 747.966582ms 748.073741ms 748.148699ms 748.199941ms 748.201944ms 748.229672ms 748.270242ms 748.297351ms 748.326808ms 748.583839ms 748.631789ms 748.862749ms 748.9277ms 748.936127ms 748.952669ms 749.031695ms 749.075773ms 749.159258ms 749.164099ms 749.18728ms 749.248712ms 749.253709ms 749.261516ms 749.264251ms 749.267129ms 749.339864ms 749.420267ms 749.45668ms 749.456688ms 749.470603ms 749.562782ms 749.582993ms 749.594613ms 749.66139ms 749.876197ms 749.881444ms 749.884915ms 749.886195ms 749.996613ms 750.023757ms 750.121685ms 750.135381ms 750.159949ms 750.177ms 750.231877ms 750.239863ms 750.241373ms 750.256656ms 750.257508ms 750.353467ms 750.414557ms 750.530729ms 750.539396ms 750.543389ms 750.631702ms 750.636381ms 750.665849ms 750.718869ms 750.848812ms 750.994791ms 751.005615ms 751.017264ms 751.020729ms 751.132723ms 751.150376ms 751.19754ms 751.200502ms 751.217585ms 751.235681ms 751.275371ms 751.330955ms 751.339606ms 751.391791ms 751.446907ms 751.453392ms 751.49137ms 751.586445ms 751.637135ms 751.93523ms 752.071036ms 752.187117ms 752.205235ms 752.259032ms 752.350165ms 752.430522ms 752.430586ms 752.485014ms 752.508133ms 752.635666ms 752.663266ms 752.6984ms 752.736293ms 752.991322ms 753.114933ms 753.154804ms 753.212977ms 753.306468ms 753.316461ms 753.347014ms 753.477655ms 753.60514ms 753.675933ms 753.935339ms 754.181685ms 754.213459ms 754.227669ms 754.786972ms 754.833111ms 755.058298ms 755.385708ms 756.573275ms 760.081993ms 760.741311ms 762.480496ms 762.764882ms 763.442091ms 765.279918ms]
Jan 13 08:20:57.102: INFO: 50 %ile: 749.159258ms
Jan 13 08:20:57.102: INFO: 90 %ile: 753.316461ms
Jan 13 08:20:57.102: INFO: 99 %ile: 763.442091ms
Jan 13 08:20:57.102: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:20:57.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6431" for this suite.

• [SLOW TEST:10.771 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":184,"skipped":3011,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:20:57.114: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:20:57.631: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:21:00.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:00.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7521" for this suite.
STEP: Destroying namespace "webhook-7521-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":185,"skipped":3017,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:00.732: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jan 13 08:21:00.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9614 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Jan 13 08:21:00.875: INFO: stderr: ""
Jan 13 08:21:00.875: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Jan 13 08:21:00.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9614 delete pods e2e-test-httpd-pod'
Jan 13 08:21:08.881: INFO: stderr: ""
Jan 13 08:21:08.881: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9614" for this suite.

• [SLOW TEST:8.162 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":186,"skipped":3021,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:08.895: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5105" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":187,"skipped":3023,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:12.973: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:13.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-129" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":188,"skipped":3029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:13.026: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1423/configmap-test-643f3928-6294-4719-ad74-11d2af6a0f49
STEP: Creating a pod to test consume configMaps
Jan 13 08:21:13.064: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9" in namespace "configmap-1423" to be "Succeeded or Failed"
Jan 13 08:21:13.067: INFO: Pod "pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039686ms
Jan 13 08:21:15.071: INFO: Pod "pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006599274s
Jan 13 08:21:17.077: INFO: Pod "pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013072731s
STEP: Saw pod success
Jan 13 08:21:17.077: INFO: Pod "pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9" satisfied condition "Succeeded or Failed"
Jan 13 08:21:17.080: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9 container env-test: <nil>
STEP: delete the pod
Jan 13 08:21:17.104: INFO: Waiting for pod pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9 to disappear
Jan 13 08:21:17.107: INFO: Pod pod-configmaps-cb5609ec-d886-468b-ba96-7ec19da6a9c9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:17.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1423" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":3078,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:17.113: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jan 13 08:21:17.149: INFO: Waiting up to 5m0s for pod "var-expansion-46b16289-7bf8-414c-a032-001d1731e715" in namespace "var-expansion-8557" to be "Succeeded or Failed"
Jan 13 08:21:17.152: INFO: Pod "var-expansion-46b16289-7bf8-414c-a032-001d1731e715": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5297ms
Jan 13 08:21:19.156: INFO: Pod "var-expansion-46b16289-7bf8-414c-a032-001d1731e715": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007700679s
Jan 13 08:21:21.160: INFO: Pod "var-expansion-46b16289-7bf8-414c-a032-001d1731e715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01190424s
STEP: Saw pod success
Jan 13 08:21:21.160: INFO: Pod "var-expansion-46b16289-7bf8-414c-a032-001d1731e715" satisfied condition "Succeeded or Failed"
Jan 13 08:21:21.162: INFO: Trying to get logs from node worker-s001 pod var-expansion-46b16289-7bf8-414c-a032-001d1731e715 container dapi-container: <nil>
STEP: delete the pod
Jan 13 08:21:21.177: INFO: Waiting for pod var-expansion-46b16289-7bf8-414c-a032-001d1731e715 to disappear
Jan 13 08:21:21.185: INFO: Pod var-expansion-46b16289-7bf8-414c-a032-001d1731e715 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:21.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8557" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":190,"skipped":3090,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:21.202: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-874b71bf-f8af-4b11-9166-e222f0f256ec
STEP: Creating a pod to test consume configMaps
Jan 13 08:21:21.235: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc" in namespace "projected-8538" to be "Succeeded or Failed"
Jan 13 08:21:21.238: INFO: Pod "pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.841851ms
Jan 13 08:21:23.241: INFO: Pod "pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005950388s
STEP: Saw pod success
Jan 13 08:21:23.241: INFO: Pod "pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc" satisfied condition "Succeeded or Failed"
Jan 13 08:21:23.242: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:21:23.260: INFO: Waiting for pod pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc to disappear
Jan 13 08:21:23.261: INFO: Pod pod-projected-configmaps-e3ed3270-5e2a-4989-9599-2709b3a2f1cc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:23.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8538" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":191,"skipped":3094,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:23.266: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jan 13 08:21:23.309: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:21:25.311: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 13 08:21:25.325: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:21:27.330: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 13 08:21:27.342: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 13 08:21:27.351: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 13 08:21:29.352: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 13 08:21:29.356: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 13 08:21:31.351: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 13 08:21:31.354: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:31.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9584" for this suite.

• [SLOW TEST:8.094 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":192,"skipped":3105,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:31.360: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-d03c8f2e-dfff-4410-ac80-b0552315ff10
STEP: Creating a pod to test consume configMaps
Jan 13 08:21:31.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819" in namespace "projected-1054" to be "Succeeded or Failed"
Jan 13 08:21:31.401: INFO: Pod "pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819": Phase="Pending", Reason="", readiness=false. Elapsed: 4.175765ms
Jan 13 08:21:33.405: INFO: Pod "pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008461556s
STEP: Saw pod success
Jan 13 08:21:33.405: INFO: Pod "pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819" satisfied condition "Succeeded or Failed"
Jan 13 08:21:33.407: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 13 08:21:33.426: INFO: Waiting for pod pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819 to disappear
Jan 13 08:21:33.431: INFO: Pod pod-projected-configmaps-6bafad14-9fd1-4111-8f36-2bdfa3943819 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:33.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1054" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":193,"skipped":3114,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:33.436: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:39.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9202" for this suite.
STEP: Destroying namespace "nsdeletetest-1482" for this suite.
Jan 13 08:21:39.537: INFO: Namespace nsdeletetest-1482 was already deleted
STEP: Destroying namespace "nsdeletetest-7454" for this suite.

• [SLOW TEST:6.103 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":194,"skipped":3130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:21:39.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a" in namespace "projected-8383" to be "Succeeded or Failed"
Jan 13 08:21:39.574: INFO: Pod "downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501681ms
Jan 13 08:21:41.588: INFO: Pod "downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01677009s
STEP: Saw pod success
Jan 13 08:21:41.588: INFO: Pod "downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a" satisfied condition "Succeeded or Failed"
Jan 13 08:21:41.590: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a container client-container: <nil>
STEP: delete the pod
Jan 13 08:21:41.605: INFO: Waiting for pod downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a to disappear
Jan 13 08:21:41.614: INFO: Pod downwardapi-volume-5daf85c5-af96-4cba-be9b-487b5d95581a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:41.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8383" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":195,"skipped":3154,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:41.619: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-7797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7797 to expose endpoints map[]
Jan 13 08:21:41.665: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 13 08:21:42.673: INFO: successfully validated that service endpoint-test2 in namespace services-7797 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7797
Jan 13 08:21:42.684: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:21:44.687: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7797 to expose endpoints map[pod1:[80]]
Jan 13 08:21:44.696: INFO: successfully validated that service endpoint-test2 in namespace services-7797 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7797
Jan 13 08:21:44.706: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:21:46.711: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7797 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 13 08:21:46.720: INFO: successfully validated that service endpoint-test2 in namespace services-7797 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7797 to expose endpoints map[pod2:[80]]
Jan 13 08:21:46.740: INFO: successfully validated that service endpoint-test2 in namespace services-7797 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7797
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7797 to expose endpoints map[]
Jan 13 08:21:46.757: INFO: successfully validated that service endpoint-test2 in namespace services-7797 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:46.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7797" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.198 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":196,"skipped":3163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:46.817: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jan 13 08:21:46.878: INFO: Waiting up to 5m0s for pod "client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4" in namespace "containers-798" to be "Succeeded or Failed"
Jan 13 08:21:46.880: INFO: Pod "client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725254ms
Jan 13 08:21:48.885: INFO: Pod "client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006463477s
STEP: Saw pod success
Jan 13 08:21:48.885: INFO: Pod "client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4" satisfied condition "Succeeded or Failed"
Jan 13 08:21:48.887: INFO: Trying to get logs from node worker-s001 pod client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:21:48.907: INFO: Waiting for pod client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4 to disappear
Jan 13 08:21:48.910: INFO: Pod client-containers-0c40c694-51c0-4401-96e9-c2e3a5c469c4 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:21:48.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-798" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":197,"skipped":3206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:21:48.920: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6395
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-6395
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6395
Jan 13 08:21:48.956: INFO: Found 0 stateful pods, waiting for 1
Jan 13 08:21:58.961: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 13 08:21:58.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:21:59.113: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:21:59.113: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:21:59.113: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:21:59.116: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 13 08:22:09.125: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:22:09.125: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:22:09.139: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:09.139: INFO: ss-0  worker-s001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:09.139: INFO: 
Jan 13 08:22:09.139: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 13 08:22:10.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996961495s
Jan 13 08:22:11.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991315908s
Jan 13 08:22:12.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986120679s
Jan 13 08:22:13.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974939885s
Jan 13 08:22:14.171: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970010727s
Jan 13 08:22:15.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96613739s
Jan 13 08:22:16.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96200347s
Jan 13 08:22:17.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957635786s
Jan 13 08:22:18.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.695944ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6395
Jan 13 08:22:19.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:22:19.322: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 08:22:19.323: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 08:22:19.323: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 08:22:19.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:22:19.467: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 13 08:22:19.467: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 08:22:19.467: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 08:22:19.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:22:19.594: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 13 08:22:19.595: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 08:22:19.595: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 08:22:19.612: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 13 08:22:29.644: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:22:29.644: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:22:29.644: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 13 08:22:29.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:22:29.799: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:22:29.799: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:22:29.799: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:22:29.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:22:29.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:22:29.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:22:29.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:22:29.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-6395 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:22:30.072: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:22:30.072: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:22:30.072: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:22:30.072: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:22:30.075: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 13 08:22:40.080: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:22:40.080: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:22:40.080: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:22:40.093: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:40.093: INFO: ss-0  worker-s001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:40.093: INFO: ss-1  worker-s002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:40.093: INFO: ss-2  worker-s001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:40.093: INFO: 
Jan 13 08:22:40.093: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:41.098: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:41.098: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:41.098: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:41.098: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:41.098: INFO: 
Jan 13 08:22:41.098: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:42.101: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:42.101: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:42.101: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:42.101: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:42.101: INFO: 
Jan 13 08:22:42.101: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:43.107: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:43.107: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:43.107: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:43.107: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:43.107: INFO: 
Jan 13 08:22:43.107: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:44.110: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:44.110: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:44.110: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:44.110: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:44.110: INFO: 
Jan 13 08:22:44.110: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:45.116: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:45.116: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:45.116: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:45.116: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:45.116: INFO: 
Jan 13 08:22:45.116: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:46.121: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:46.121: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:46.121: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:46.121: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:46.121: INFO: 
Jan 13 08:22:46.121: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:47.126: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:47.126: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:47.126: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:47.126: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:47.126: INFO: 
Jan 13 08:22:47.126: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:48.130: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jan 13 08:22:48.130: INFO: ss-0  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:21:48 +0000 UTC  }]
Jan 13 08:22:48.130: INFO: ss-1  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:48.130: INFO: ss-2  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:22:09 +0000 UTC  }]
Jan 13 08:22:48.130: INFO: 
Jan 13 08:22:48.130: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 13 08:22:49.134: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.553869ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6395
Jan 13 08:22:50.137: INFO: Scaling statefulset ss to 0
Jan 13 08:22:50.143: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jan 13 08:22:50.146: INFO: Deleting all statefulset in ns statefulset-6395
Jan 13 08:22:50.147: INFO: Scaling statefulset ss to 0
Jan 13 08:22:50.153: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:22:50.154: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:22:50.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6395" for this suite.

• [SLOW TEST:61.254 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":198,"skipped":3257,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:22:50.174: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 13 08:22:50.207: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 13 08:23:06.544: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:23:10.911: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:23:28.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2236" for this suite.

• [SLOW TEST:38.742 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":199,"skipped":3260,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:23:28.917: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:23:28.945: INFO: Creating deployment "webserver-deployment"
Jan 13 08:23:28.947: INFO: Waiting for observed generation 1
Jan 13 08:23:30.953: INFO: Waiting for all required pods to come up
Jan 13 08:23:30.957: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 13 08:23:34.965: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 13 08:23:34.970: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 13 08:23:34.980: INFO: Updating deployment webserver-deployment
Jan 13 08:23:34.980: INFO: Waiting for observed generation 2
Jan 13 08:23:36.986: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 13 08:23:36.988: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 13 08:23:36.989: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 13 08:23:36.993: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 13 08:23:36.993: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 13 08:23:36.994: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 13 08:23:36.996: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 13 08:23:36.996: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 13 08:23:37.000: INFO: Updating deployment webserver-deployment
Jan 13 08:23:37.000: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 13 08:23:37.003: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 13 08:23:37.014: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 08:23:37.029: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9284  b5464e2e-96ae-4337-9efd-7f57d5e617a3 405816 3 2022-01-13 08:23:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-13 08:23:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a1998f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-01-13 08:23:35 +0000 UTC,LastTransitionTime:2022-01-13 08:23:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-01-13 08:23:37 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 13 08:23:37.044: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9284  743f517c-8f4a-4348-be34-18d2cc3310f4 405807 3 2022-01-13 08:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b5464e2e-96ae-4337-9efd-7f57d5e617a3 0xc00a199e17 0xc00a199e18}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5464e2e-96ae-4337-9efd-7f57d5e617a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a199e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:23:37.044: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 13 08:23:37.044: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-9284  9c851976-6932-442b-9f73-55683499ddf4 405805 3 2022-01-13 08:23:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b5464e2e-96ae-4337-9efd-7f57d5e617a3 0xc00a199f07 0xc00a199f08}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5464e2e-96ae-4337-9efd-7f57d5e617a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a199ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:23:37.059: INFO: Pod "webserver-deployment-795d758f88-2mnm9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2mnm9 webserver-deployment-795d758f88- deployment-9284  e81ef704-871d-478f-8191-e6fbd21f8606 405822 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a424757 0xc00a424758}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9bg6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9bg6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.059: INFO: Pod "webserver-deployment-795d758f88-5bnkd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5bnkd webserver-deployment-795d758f88- deployment-9284  335e5e9d-0386-4b22-ae70-1db30ca31fb3 405802 0 2022-01-13 08:23:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.28.49/32 cni.projectcalico.org/podIPs:10.10.28.49/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a424a00 0xc00a424a01}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-01-13 08:23:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pttf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pttf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:,StartTime:2022-01-13 08:23:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.059: INFO: Pod "webserver-deployment-795d758f88-5lwnv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5lwnv webserver-deployment-795d758f88- deployment-9284  5e385cd3-fd2b-4bec-867f-a29ce9b60951 405811 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a424d97 0xc00a424d98}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcz4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcz4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.060: INFO: Pod "webserver-deployment-795d758f88-8gg8k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8gg8k webserver-deployment-795d758f88- deployment-9284  ba2b9d1b-7518-4148-b694-29817b0f0ea0 405798 0 2022-01-13 08:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.28.26/32 cni.projectcalico.org/podIPs:10.10.28.26/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a425130 0xc00a425131}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-01-13 08:23:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w775h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w775h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:,StartTime:2022-01-13 08:23:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.060: INFO: Pod "webserver-deployment-795d758f88-kcrbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kcrbp webserver-deployment-795d758f88- deployment-9284  6ad68a77-287f-4d87-bc23-2d79e76af1c8 405783 0 2022-01-13 08:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.28.46/32 cni.projectcalico.org/podIPs:10.10.28.46/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a4253d7 0xc00a4253d8}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-01-13 08:23:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9t7c2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9t7c2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:,StartTime:2022-01-13 08:23:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.060: INFO: Pod "webserver-deployment-795d758f88-m6268" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m6268 webserver-deployment-795d758f88- deployment-9284  885a8f38-b926-4080-88ba-7284beb29371 405795 0 2022-01-13 08:23:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.131.185/32 cni.projectcalico.org/podIPs:10.10.131.185/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a425827 0xc00a425828}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-01-13 08:23:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8szc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8szc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:,StartTime:2022-01-13 08:23:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.060: INFO: Pod "webserver-deployment-795d758f88-msnzg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-msnzg webserver-deployment-795d758f88- deployment-9284  b392e881-a081-4336-9508-76e6fd6af735 405781 0 2022-01-13 08:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.131.183/32 cni.projectcalico.org/podIPs:10.10.131.183/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a425b77 0xc00a425b78}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-01-13 08:23:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2022-01-13 08:23:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fb8cg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fb8cg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:,StartTime:2022-01-13 08:23:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.060: INFO: Pod "webserver-deployment-795d758f88-t7t9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t7t9c webserver-deployment-795d758f88- deployment-9284  dbcd32ba-24e5-4ff9-96eb-2abd49e9ec2e 405820 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 743f517c-8f4a-4348-be34-18d2cc3310f4 0xc00a425f37 0xc00a425f38}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743f517c-8f4a-4348-be34-18d2cc3310f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6x996,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6x996,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.060: INFO: Pod "webserver-deployment-847dcfb7fb-4s2fm" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-4s2fm webserver-deployment-847dcfb7fb- deployment-9284  7ed2429a-db87-4797-b629-eccdd4f33e50 405688 0 2022-01-13 08:23:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.131.184/32 cni.projectcalico.org/podIPs:10.10.131.184/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a440160 0xc00a440161}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cfkwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cfkwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:10.10.131.184,StartTime:2022-01-13 08:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://50671f00708c273ed4a5e19b5de406aaf06712223a9d12c3bdf66d88a98bf0a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-6fhr6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6fhr6 webserver-deployment-847dcfb7fb- deployment-9284  51c8bdfa-3e9e-435b-8b53-1bc44f706417 405668 0 2022-01-13 08:23:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.28.54/32 cni.projectcalico.org/podIPs:10.10.28.54/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a4404a7 0xc00a4404a8}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvb55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvb55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.54,StartTime:2022-01-13 08:23:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://9073379c1e42325ea5631c21d2eec3ddf384a21e99224c08cb0fc8d986be7de1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-7xkxz" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7xkxz webserver-deployment-847dcfb7fb- deployment-9284  f937c2a7-f006-4aaa-a3f3-55b3fddb2b1b 405638 0 2022-01-13 08:23:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.131.178/32 cni.projectcalico.org/podIPs:10.10.131.178/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a4406d7 0xc00a4406d8}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csjfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csjfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:10.10.131.178,StartTime:2022-01-13 08:23:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://aa2a9f6e0a2020319bce67d3c543a796f62e5649cdf9f644164a0e53a734450e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-8wzm8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8wzm8 webserver-deployment-847dcfb7fb- deployment-9284  8ed94312-5853-4890-9228-efebde70dce2 405825 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a440967 0xc00a440968}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lq9g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lq9g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-cr6zq" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cr6zq webserver-deployment-847dcfb7fb- deployment-9284  35cb8cf0-b07c-49cb-98bb-ff886511121c 405827 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a440ad0 0xc00a440ad1}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkb6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkb6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-gmxn7" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gmxn7 webserver-deployment-847dcfb7fb- deployment-9284  73a391fe-0b7f-41fb-9285-31f2eb5e761b 405683 0 2022-01-13 08:23:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.131.181/32 cni.projectcalico.org/podIPs:10.10.131.181/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a440c30 0xc00a440c31}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9cbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9cbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:10.10.131.181,StartTime:2022-01-13 08:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://467fa31ef675374eb1207e1074b8c5d74d7db255d4f2d3b00f5c8d934dd52eea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-hv5nq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hv5nq webserver-deployment-847dcfb7fb- deployment-9284  1f7acbe5-eb18-4487-9899-799947f05230 405648 0 2022-01-13 08:23:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.28.30/32 cni.projectcalico.org/podIPs:10.10.28.30/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a440e47 0xc00a440e48}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjx6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjx6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.30,StartTime:2022-01-13 08:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://206df853d22fdb1a465c2de02bb4fab0d16a7ecaa2dc4488c6990ded9ded4813,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.061: INFO: Pod "webserver-deployment-847dcfb7fb-p8pkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-p8pkg webserver-deployment-847dcfb7fb- deployment-9284  fafc6df8-c12d-487a-9dad-f3cc1c27bf98 405813 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a441047 0xc00a441048}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8hbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8hbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.062: INFO: Pod "webserver-deployment-847dcfb7fb-q7lwd" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q7lwd webserver-deployment-847dcfb7fb- deployment-9284  4ad44682-0d4f-48ff-9177-1155d044b080 405691 0 2022-01-13 08:23:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.28.38/32 cni.projectcalico.org/podIPs:10.10.28.38/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a4411d0 0xc00a4411d1}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzhrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzhrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.38,StartTime:2022-01-13 08:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://d215fe1b063f4eb76d8ac2e014aa54d71bbbe7499802d1e4b0ffd122efc09384,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.062: INFO: Pod "webserver-deployment-847dcfb7fb-qzvjr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qzvjr webserver-deployment-847dcfb7fb- deployment-9284  3a302fc0-92c1-4af3-a459-63ae5822f307 405700 0 2022-01-13 08:23:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.28.34/32 cni.projectcalico.org/podIPs:10.10.28.34/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a4413e7 0xc00a4413e8}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv9p5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv9p5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.34,StartTime:2022-01-13 08:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://8319b9e8dc07d24348d0b5ef70a92d6f74d99d89780d7e3473aa5a86062a79f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.062: INFO: Pod "webserver-deployment-847dcfb7fb-svhh7" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-svhh7 webserver-deployment-847dcfb7fb- deployment-9284  a6899b8d-c795-44f2-b698-6673e78de633 405681 0 2022-01-13 08:23:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.131.177/32 cni.projectcalico.org/podIPs:10.10.131.177/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a441647 0xc00a441648}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8jj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8jj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.60,PodIP:10.10.131.177,StartTime:2022-01-13 08:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://ea24ca9b5c530b488f412a4fbfdda4e02163dd31c8a1b56a8327f6c2472ecab4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:23:37.062: INFO: Pod "webserver-deployment-847dcfb7fb-wmc8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wmc8n webserver-deployment-847dcfb7fb- deployment-9284  6250ac3a-ddbf-471d-b90b-3de6ac9c9ba3 405826 0 2022-01-13 08:23:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 9c851976-6932-442b-9f73-55683499ddf4 0xc00a4418a7 0xc00a4418a8}] []  [{kube-controller-manager Update v1 2022-01-13 08:23:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c851976-6932-442b-9f73-55683499ddf4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rs6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rs6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:23:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:23:37.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9284" for this suite.

• [SLOW TEST:8.194 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":200,"skipped":3261,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:23:37.111: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 13 08:23:37.566: INFO: starting watch
STEP: patching
STEP: updating
Jan 13 08:23:37.573: INFO: waiting for watch events with expected annotations
Jan 13 08:23:37.573: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:23:37.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-345" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":201,"skipped":3271,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:23:37.623: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-237477c9-c0cc-4580-bf0b-c468c635c8ee
STEP: Creating a pod to test consume secrets
Jan 13 08:23:37.659: INFO: Waiting up to 5m0s for pod "pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e" in namespace "secrets-6736" to be "Succeeded or Failed"
Jan 13 08:23:37.661: INFO: Pod "pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424001ms
Jan 13 08:23:39.666: INFO: Pod "pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007100892s
Jan 13 08:23:41.670: INFO: Pod "pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011082008s
Jan 13 08:23:43.678: INFO: Pod "pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018853399s
STEP: Saw pod success
Jan 13 08:23:43.685: INFO: Pod "pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e" satisfied condition "Succeeded or Failed"
Jan 13 08:23:43.690: INFO: Trying to get logs from node worker-s001 pod pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:23:43.725: INFO: Waiting for pod pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e to disappear
Jan 13 08:23:43.730: INFO: Pod pod-secrets-19c7dfe2-5ef5-4c2a-851c-140c64a8493e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:23:43.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6736" for this suite.

• [SLOW TEST:6.111 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":202,"skipped":3278,"failed":0}
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:23:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0113 08:23:43.770360      24 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:01.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6401" for this suite.

• [SLOW TEST:318.071 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":203,"skipped":3278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:01.805: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 13 08:29:01.862: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 13 08:29:01.864: INFO: starting watch
STEP: patching
STEP: updating
Jan 13 08:29:01.875: INFO: waiting for watch events with expected annotations
Jan 13 08:29:01.875: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:01.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7341" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":204,"skipped":3303,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:01.893: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:29:01.921: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 13 08:29:06.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-2155 --namespace=crd-publish-openapi-2155 create -f -'
Jan 13 08:29:06.649: INFO: stderr: ""
Jan 13 08:29:06.649: INFO: stdout: "e2e-test-crd-publish-openapi-8239-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 13 08:29:06.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-2155 --namespace=crd-publish-openapi-2155 delete e2e-test-crd-publish-openapi-8239-crds test-cr'
Jan 13 08:29:06.705: INFO: stderr: ""
Jan 13 08:29:06.705: INFO: stdout: "e2e-test-crd-publish-openapi-8239-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 13 08:29:06.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-2155 --namespace=crd-publish-openapi-2155 apply -f -'
Jan 13 08:29:06.924: INFO: stderr: ""
Jan 13 08:29:06.924: INFO: stdout: "e2e-test-crd-publish-openapi-8239-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 13 08:29:06.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-2155 --namespace=crd-publish-openapi-2155 delete e2e-test-crd-publish-openapi-8239-crds test-cr'
Jan 13 08:29:06.983: INFO: stderr: ""
Jan 13 08:29:06.983: INFO: stdout: "e2e-test-crd-publish-openapi-8239-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 13 08:29:06.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-2155 explain e2e-test-crd-publish-openapi-8239-crds'
Jan 13 08:29:07.181: INFO: stderr: ""
Jan 13 08:29:07.181: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8239-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:12.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2155" for this suite.

• [SLOW TEST:10.148 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":205,"skipped":3316,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:12.040: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:29:12.082: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 13 08:29:17.089: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 13 08:29:17.089: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 13 08:29:19.095: INFO: Creating deployment "test-rollover-deployment"
Jan 13 08:29:19.102: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 13 08:29:21.106: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 13 08:29:21.110: INFO: Ensure that both replica sets have 1 created replica
Jan 13 08:29:21.112: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 13 08:29:21.117: INFO: Updating deployment test-rollover-deployment
Jan 13 08:29:21.117: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 13 08:29:23.123: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 13 08:29:23.127: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 13 08:29:23.131: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 08:29:23.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659363, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 08:29:25.138: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 08:29:25.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659363, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 08:29:27.138: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 08:29:27.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659363, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 08:29:29.142: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 08:29:29.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659363, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 08:29:31.137: INFO: all replica sets need to contain the pod-template-hash label
Jan 13 08:29:31.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659363, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 08:29:33.145: INFO: 
Jan 13 08:29:33.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659373, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659359, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 08:29:35.140: INFO: 
Jan 13 08:29:35.140: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 08:29:35.146: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4406  b2915567-bda4-4b71-b646-12bc2e3ad118 407574 2 2022-01-13 08:29:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-13 08:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0090fe3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-13 08:29:19 +0000 UTC,LastTransitionTime:2022-01-13 08:29:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-01-13 08:29:33 +0000 UTC,LastTransitionTime:2022-01-13 08:29:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 08:29:35.148: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-4406  e20838b7-4dc7-428d-8fce-1303d996f731 407563 2 2022-01-13 08:29:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b2915567-bda4-4b71-b646-12bc2e3ad118 0xc0090fea30 0xc0090fea31}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2915567-bda4-4b71-b646-12bc2e3ad118\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0090feb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:29:35.148: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 13 08:29:35.148: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4406  37ade8e6-47f7-4a7f-b04f-cec52045152f 407573 2 2022-01-13 08:29:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b2915567-bda4-4b71-b646-12bc2e3ad118 0xc0090fe777 0xc0090fe778}] []  [{e2e.test Update apps/v1 2022-01-13 08:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2915567-bda4-4b71-b646-12bc2e3ad118\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0090fe818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:29:35.148: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4406  4782efe7-8fad-480e-badf-5896bd6d74c8 407508 2 2022-01-13 08:29:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b2915567-bda4-4b71-b646-12bc2e3ad118 0xc0090fe887 0xc0090fe888}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2915567-bda4-4b71-b646-12bc2e3ad118\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0090fe928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:29:35.150: INFO: Pod "test-rollover-deployment-98c5f4599-mnswj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-mnswj test-rollover-deployment-98c5f4599- deployment-4406  cf478328-ea4a-4fa2-b58c-0d0baba8b3ea 407526 0 2022-01-13 08:29:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:10.10.28.35/32 cni.projectcalico.org/podIPs:10.10.28.35/32] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 e20838b7-4dc7-428d-8fce-1303d996f731 0xc0090d6fe0 0xc0090d6fe1}] []  [{kube-controller-manager Update v1 2022-01-13 08:29:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e20838b7-4dc7-428d-8fce-1303d996f731\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:29:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvbgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvbgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:29:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:29:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:29:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.35,StartTime:2022-01-13 08:29:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:29:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900,ContainerID:docker://28383fea8f32c782efac85719f6478675ce041a3bd276b07ad046b6805944e77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:35.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4406" for this suite.

• [SLOW TEST:23.116 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":206,"skipped":3325,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:35.156: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:35.187: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1119
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:41.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5364" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:41.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1119" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":207,"skipped":3327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:41.278: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jan 13 08:29:41.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 create -f -'
Jan 13 08:29:41.621: INFO: stderr: ""
Jan 13 08:29:41.621: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 13 08:29:41.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:29:41.677: INFO: stderr: ""
Jan 13 08:29:41.677: INFO: stdout: "update-demo-nautilus-8ttjd update-demo-nautilus-n64lg "
Jan 13 08:29:41.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods update-demo-nautilus-8ttjd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:29:41.728: INFO: stderr: ""
Jan 13 08:29:41.728: INFO: stdout: ""
Jan 13 08:29:41.728: INFO: update-demo-nautilus-8ttjd is created but not running
Jan 13 08:29:46.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:29:46.788: INFO: stderr: ""
Jan 13 08:29:46.788: INFO: stdout: "update-demo-nautilus-8ttjd update-demo-nautilus-n64lg "
Jan 13 08:29:46.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods update-demo-nautilus-8ttjd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:29:46.851: INFO: stderr: ""
Jan 13 08:29:46.851: INFO: stdout: "true"
Jan 13 08:29:46.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods update-demo-nautilus-8ttjd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:29:46.906: INFO: stderr: ""
Jan 13 08:29:46.906: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:29:46.906: INFO: validating pod update-demo-nautilus-8ttjd
Jan 13 08:29:46.909: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:29:46.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:29:46.909: INFO: update-demo-nautilus-8ttjd is verified up and running
Jan 13 08:29:46.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods update-demo-nautilus-n64lg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:29:46.959: INFO: stderr: ""
Jan 13 08:29:46.959: INFO: stdout: "true"
Jan 13 08:29:46.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods update-demo-nautilus-n64lg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:29:47.009: INFO: stderr: ""
Jan 13 08:29:47.009: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:29:47.009: INFO: validating pod update-demo-nautilus-n64lg
Jan 13 08:29:47.012: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:29:47.012: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:29:47.012: INFO: update-demo-nautilus-n64lg is verified up and running
STEP: using delete to clean up resources
Jan 13 08:29:47.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 delete --grace-period=0 --force -f -'
Jan 13 08:29:47.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:29:47.065: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 13 08:29:47.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get rc,svc -l name=update-demo --no-headers'
Jan 13 08:29:47.125: INFO: stderr: "No resources found in kubectl-9745 namespace.\n"
Jan 13 08:29:47.125: INFO: stdout: ""
Jan 13 08:29:47.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9745 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 13 08:29:47.185: INFO: stderr: ""
Jan 13 08:29:47.185: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:29:47.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9745" for this suite.

• [SLOW TEST:5.913 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":208,"skipped":3457,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:29:47.192: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2918, will wait for the garbage collector to delete the pods
Jan 13 08:29:51.300: INFO: Deleting Job.batch foo took: 5.186457ms
Jan 13 08:29:51.400: INFO: Terminating Job.batch foo pods took: 100.204842ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:30:28.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2918" for this suite.

• [SLOW TEST:41.716 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":209,"skipped":3462,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:30:28.908: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:30:29.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:30:32.383: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:30:32.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-12" for this suite.
STEP: Destroying namespace "webhook-12-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":210,"skipped":3467,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:30:32.484: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9307
STEP: creating service affinity-clusterip in namespace services-9307
STEP: creating replication controller affinity-clusterip in namespace services-9307
I0113 08:30:32.548306      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9307, replica count: 3
I0113 08:30:35.607886      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 08:30:35.612: INFO: Creating new exec pod
Jan 13 08:30:40.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9307 exec execpod-affinityntmrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 13 08:30:40.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 13 08:30:40.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 08:30:40.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9307 exec execpod-affinityntmrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.65.91 80'
Jan 13 08:30:40.927: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.65.91 80\nConnection to 10.96.65.91 80 port [tcp/http] succeeded!\n"
Jan 13 08:30:40.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 13 08:30:40.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-9307 exec execpod-affinityntmrd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.65.91:80/ ; done'
Jan 13 08:30:41.111: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.65.91:80/\n"
Jan 13 08:30:41.111: INFO: stdout: "\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh\naffinity-clusterip-245dh"
Jan 13 08:30:41.111: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.111: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.111: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.111: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.111: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Received response from host: affinity-clusterip-245dh
Jan 13 08:30:41.112: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9307, will wait for the garbage collector to delete the pods
Jan 13 08:30:41.196: INFO: Deleting ReplicationController affinity-clusterip took: 15.69408ms
Jan 13 08:30:41.297: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.08645ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:30:49.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9307" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:16.540 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":211,"skipped":3477,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:30:49.024: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:30:49.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-960" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":212,"skipped":3487,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:30:49.089: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 13 08:30:49.124: INFO: Waiting up to 5m0s for pod "pod-ab48db8c-4adf-4296-bf53-f789ccab2b69" in namespace "emptydir-3739" to be "Succeeded or Failed"
Jan 13 08:30:49.125: INFO: Pod "pod-ab48db8c-4adf-4296-bf53-f789ccab2b69": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806649ms
Jan 13 08:30:51.128: INFO: Pod "pod-ab48db8c-4adf-4296-bf53-f789ccab2b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004622937s
STEP: Saw pod success
Jan 13 08:30:51.128: INFO: Pod "pod-ab48db8c-4adf-4296-bf53-f789ccab2b69" satisfied condition "Succeeded or Failed"
Jan 13 08:30:51.133: INFO: Trying to get logs from node worker-s001 pod pod-ab48db8c-4adf-4296-bf53-f789ccab2b69 container test-container: <nil>
STEP: delete the pod
Jan 13 08:30:51.155: INFO: Waiting for pod pod-ab48db8c-4adf-4296-bf53-f789ccab2b69 to disappear
Jan 13 08:30:51.159: INFO: Pod pod-ab48db8c-4adf-4296-bf53-f789ccab2b69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:30:51.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3739" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":213,"skipped":3496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:30:51.164: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:30:51.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:30:54.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 13 08:30:54.503: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:30:54.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6802" for this suite.
STEP: Destroying namespace "webhook-6802-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":214,"skipped":3550,"failed":0}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:30:54.562: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 13 08:30:54.602: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 08:31:54.634: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jan 13 08:31:54.655: INFO: Created pod: pod0-sched-preemption-low-priority
Jan 13 08:31:54.677: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:32:14.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8196" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:80.173 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":215,"skipped":3550,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:32:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jan 13 08:32:14.763: INFO: Waiting up to 5m0s for pod "var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7" in namespace "var-expansion-3885" to be "Succeeded or Failed"
Jan 13 08:32:14.766: INFO: Pod "var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.181219ms
Jan 13 08:32:16.768: INFO: Pod "var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005100629s
STEP: Saw pod success
Jan 13 08:32:16.768: INFO: Pod "var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7" satisfied condition "Succeeded or Failed"
Jan 13 08:32:16.770: INFO: Trying to get logs from node worker-s001 pod var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7 container dapi-container: <nil>
STEP: delete the pod
Jan 13 08:32:16.781: INFO: Waiting for pod var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7 to disappear
Jan 13 08:32:16.783: INFO: Pod var-expansion-353f4416-5688-4a7f-bfcb-3a2ecc883db7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:32:16.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3885" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":216,"skipped":3559,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:32:16.787: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:32:16.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b" in namespace "projected-6134" to be "Succeeded or Failed"
Jan 13 08:32:16.839: INFO: Pod "downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.883059ms
Jan 13 08:32:18.841: INFO: Pod "downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015099523s
STEP: Saw pod success
Jan 13 08:32:18.841: INFO: Pod "downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b" satisfied condition "Succeeded or Failed"
Jan 13 08:32:18.842: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b container client-container: <nil>
STEP: delete the pod
Jan 13 08:32:18.868: INFO: Waiting for pod downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b to disappear
Jan 13 08:32:18.871: INFO: Pod downwardapi-volume-02d5ed1a-ce5e-49f4-9f9e-c5253a758f2b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:32:18.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6134" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":217,"skipped":3571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:32:18.878: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 13 08:32:18.916: INFO: Waiting up to 5m0s for pod "downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e" in namespace "downward-api-9998" to be "Succeeded or Failed"
Jan 13 08:32:18.920: INFO: Pod "downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168181ms
Jan 13 08:32:20.923: INFO: Pod "downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007184312s
STEP: Saw pod success
Jan 13 08:32:20.923: INFO: Pod "downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e" satisfied condition "Succeeded or Failed"
Jan 13 08:32:20.925: INFO: Trying to get logs from node worker-s001 pod downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e container dapi-container: <nil>
STEP: delete the pod
Jan 13 08:32:20.940: INFO: Waiting for pod downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e to disappear
Jan 13 08:32:20.943: INFO: Pod downward-api-60bc7faf-8e55-4714-8ac9-c919a28df96e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:32:20.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9998" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":218,"skipped":3620,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:32:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:32:21.004: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 13 08:32:21.012: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:21.014: INFO: Number of nodes with available pods: 0
Jan 13 08:32:21.014: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:32:22.017: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:22.019: INFO: Number of nodes with available pods: 0
Jan 13 08:32:22.019: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:32:23.018: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:23.020: INFO: Number of nodes with available pods: 2
Jan 13 08:32:23.020: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 13 08:32:23.039: INFO: Wrong image for pod: daemon-set-97fk2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:23.039: INFO: Wrong image for pod: daemon-set-nzc2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:23.041: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:24.045: INFO: Wrong image for pod: daemon-set-nzc2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:24.048: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:25.045: INFO: Wrong image for pod: daemon-set-nzc2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:25.049: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:26.045: INFO: Wrong image for pod: daemon-set-nzc2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:26.048: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:27.044: INFO: Pod daemon-set-h5btx is not available
Jan 13 08:32:27.044: INFO: Wrong image for pod: daemon-set-nzc2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:27.047: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:28.044: INFO: Pod daemon-set-h5btx is not available
Jan 13 08:32:28.044: INFO: Wrong image for pod: daemon-set-nzc2j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jan 13 08:32:28.046: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:29.051: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:30.051: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:31.049: INFO: Pod daemon-set-kb74p is not available
Jan 13 08:32:31.053: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 13 08:32:31.060: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:31.066: INFO: Number of nodes with available pods: 1
Jan 13 08:32:31.066: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:32:32.081: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:32.084: INFO: Number of nodes with available pods: 1
Jan 13 08:32:32.084: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:32:33.071: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:32:33.073: INFO: Number of nodes with available pods: 2
Jan 13 08:32:33.073: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4413, will wait for the garbage collector to delete the pods
Jan 13 08:32:33.139: INFO: Deleting DaemonSet.extensions daemon-set took: 3.042149ms
Jan 13 08:32:33.240: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.882675ms
Jan 13 08:32:37.143: INFO: Number of nodes with available pods: 0
Jan 13 08:32:37.143: INFO: Number of running nodes: 0, number of available pods: 0
Jan 13 08:32:37.145: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"409243"},"items":null}

Jan 13 08:32:37.146: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"409243"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:32:37.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4413" for this suite.

• [SLOW TEST:16.206 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":219,"skipped":3644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:32:37.157: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:32:59.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2340" for this suite.

• [SLOW TEST:22.299 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":220,"skipped":3680,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:32:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:32:59.486: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:00.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1257" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":221,"skipped":3691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:00.513: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:33:00.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f" in namespace "downward-api-2719" to be "Succeeded or Failed"
Jan 13 08:33:00.579: INFO: Pod "downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.285952ms
Jan 13 08:33:02.583: INFO: Pod "downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013641539s
STEP: Saw pod success
Jan 13 08:33:02.583: INFO: Pod "downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f" satisfied condition "Succeeded or Failed"
Jan 13 08:33:02.585: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f container client-container: <nil>
STEP: delete the pod
Jan 13 08:33:02.602: INFO: Waiting for pod downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f to disappear
Jan 13 08:33:02.604: INFO: Pod downwardapi-volume-73a31f94-5384-40fe-b27c-649d2d79332f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:02.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2719" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":222,"skipped":3721,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:02.609: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:33:03.058: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 13 08:33:05.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659583, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659583, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659583, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777659583, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:33:08.096: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:08.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5522" for this suite.
STEP: Destroying namespace "webhook-5522-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.601 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":223,"skipped":3723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:08.210: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 13 08:33:08.273: INFO: The status of Pod pod-update-590ab0a8-29f8-4c77-a47e-6ec0dd17f376 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:33:10.278: INFO: The status of Pod pod-update-590ab0a8-29f8-4c77-a47e-6ec0dd17f376 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 13 08:33:10.792: INFO: Successfully updated pod "pod-update-590ab0a8-29f8-4c77-a47e-6ec0dd17f376"
STEP: verifying the updated pod is in kubernetes
Jan 13 08:33:10.796: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:10.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-541" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":224,"skipped":3752,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:10.801: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-93e1c928-67f5-4609-a9f7-65830c424df4 in namespace container-probe-9986
Jan 13 08:33:12.872: INFO: Started pod liveness-93e1c928-67f5-4609-a9f7-65830c424df4 in namespace container-probe-9986
STEP: checking the pod's current state and verifying that restartCount is present
Jan 13 08:33:12.873: INFO: Initial restart count of pod liveness-93e1c928-67f5-4609-a9f7-65830c424df4 is 0
Jan 13 08:33:32.936: INFO: Restart count of pod container-probe-9986/liveness-93e1c928-67f5-4609-a9f7-65830c424df4 is now 1 (20.063116779s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:32.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9986" for this suite.

• [SLOW TEST:22.156 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":225,"skipped":3754,"failed":0}
S
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:32.958: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jan 13 08:33:32.993: INFO: Waiting up to 5m0s for pod "client-containers-4da57392-8a25-459e-992a-8ec373471e2c" in namespace "containers-4225" to be "Succeeded or Failed"
Jan 13 08:33:32.997: INFO: Pod "client-containers-4da57392-8a25-459e-992a-8ec373471e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567993ms
Jan 13 08:33:35.003: INFO: Pod "client-containers-4da57392-8a25-459e-992a-8ec373471e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010211364s
STEP: Saw pod success
Jan 13 08:33:35.003: INFO: Pod "client-containers-4da57392-8a25-459e-992a-8ec373471e2c" satisfied condition "Succeeded or Failed"
Jan 13 08:33:35.005: INFO: Trying to get logs from node worker-s001 pod client-containers-4da57392-8a25-459e-992a-8ec373471e2c container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:33:35.021: INFO: Waiting for pod client-containers-4da57392-8a25-459e-992a-8ec373471e2c to disappear
Jan 13 08:33:35.024: INFO: Pod client-containers-4da57392-8a25-459e-992a-8ec373471e2c no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:35.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4225" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":226,"skipped":3755,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:35.030: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jan 13 08:33:35.073: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-6609 proxy --unix-socket=/tmp/kubectl-proxy-unix733581912/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:35.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6609" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":227,"skipped":3755,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:35.125: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:33:35.166: INFO: Create a RollingUpdate DaemonSet
Jan 13 08:33:35.169: INFO: Check that daemon pods launch on every node of the cluster
Jan 13 08:33:35.171: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:35.174: INFO: Number of nodes with available pods: 0
Jan 13 08:33:35.174: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:33:36.179: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:36.183: INFO: Number of nodes with available pods: 0
Jan 13 08:33:36.183: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 08:33:37.179: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:37.188: INFO: Number of nodes with available pods: 2
Jan 13 08:33:37.188: INFO: Number of running nodes: 2, number of available pods: 2
Jan 13 08:33:37.188: INFO: Update the DaemonSet to trigger a rollout
Jan 13 08:33:37.210: INFO: Updating DaemonSet daemon-set
Jan 13 08:33:42.222: INFO: Roll back the DaemonSet before rollout is complete
Jan 13 08:33:42.227: INFO: Updating DaemonSet daemon-set
Jan 13 08:33:42.227: INFO: Make sure DaemonSet rollback is complete
Jan 13 08:33:42.232: INFO: Wrong image for pod: daemon-set-87jdn. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Jan 13 08:33:42.232: INFO: Pod daemon-set-87jdn is not available
Jan 13 08:33:42.234: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:43.240: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:44.241: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:45.240: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:46.250: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:47.241: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:48.240: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 08:33:49.236: INFO: Pod daemon-set-w6vhp is not available
Jan 13 08:33:49.239: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6926, will wait for the garbage collector to delete the pods
Jan 13 08:33:49.298: INFO: Deleting DaemonSet.extensions daemon-set took: 3.590207ms
Jan 13 08:33:49.399: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.68982ms
Jan 13 08:33:58.705: INFO: Number of nodes with available pods: 0
Jan 13 08:33:58.705: INFO: Number of running nodes: 0, number of available pods: 0
Jan 13 08:33:58.707: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"410040"},"items":null}

Jan 13 08:33:58.708: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"410040"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:58.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6926" for this suite.

• [SLOW TEST:23.593 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":228,"skipped":3776,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:58.718: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:33:58.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8317" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":229,"skipped":3794,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:33:58.764: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:33:58.787: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 13 08:33:58.796: INFO: The status of Pod pod-exec-websocket-c908b20b-2bcf-4e2d-acdf-08bd7f59e06d is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:34:00.799: INFO: The status of Pod pod-exec-websocket-c908b20b-2bcf-4e2d-acdf-08bd7f59e06d is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:34:00.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-224" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":230,"skipped":3797,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:34:00.884: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0113 08:34:00.917052      24 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6950" for this suite.

• [SLOW TEST:120.055 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":231,"skipped":3798,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:00.939: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:00.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2606" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":232,"skipped":3846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:00.995: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5750.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5750.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5750.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5750.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5750.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5750.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:36:05.055: INFO: DNS probes using dns-5750/dns-test-60c20438-523b-4a35-9a5f-6c4c44519599 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:05.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5750" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":233,"skipped":3876,"failed":0}

------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:05.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6372" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":234,"skipped":3876,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:05.158: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 13 08:36:05.184: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 13 08:36:10.190: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:11.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3708" for this suite.

• [SLOW TEST:6.058 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":235,"skipped":3883,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:11.216: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 13 08:36:11.250: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:36:16.064: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:35.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2553" for this suite.

• [SLOW TEST:24.129 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":236,"skipped":3889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:35.345: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 13 08:36:35.383: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8045  2f34d7f7-a223-4092-9677-3855fa497364 410940 0 2022-01-13 08:36:35 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-01-13 08:36:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cpfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cpfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:36:35.389: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:36:37.393: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 13 08:36:37.393: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8045 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:36:37.393: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Verifying customized DNS server is configured on pod...
Jan 13 08:36:37.483: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8045 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:36:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:36:37.580: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:36:37.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8045" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":237,"skipped":3914,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:36:37.608: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 13 08:36:39.668: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-191 PodName:var-expansion-117244ad-0366-4321-b544-24ef63890a22 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:36:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: test for file in mounted path
Jan 13 08:36:39.759: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-191 PodName:var-expansion-117244ad-0366-4321-b544-24ef63890a22 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:36:39.759: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: updating the annotation value
Jan 13 08:36:40.343: INFO: Successfully updated pod "var-expansion-117244ad-0366-4321-b544-24ef63890a22"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 13 08:36:40.346: INFO: Deleting pod "var-expansion-117244ad-0366-4321-b544-24ef63890a22" in namespace "var-expansion-191"
Jan 13 08:36:40.357: INFO: Wait up to 5m0s for pod "var-expansion-117244ad-0366-4321-b544-24ef63890a22" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:20.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-191" for this suite.

• [SLOW TEST:42.788 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":238,"skipped":3924,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:20.396: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 13 08:37:22.452: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:22.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4796" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":239,"skipped":3933,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:22.470: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:37:22.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f" in namespace "projected-604" to be "Succeeded or Failed"
Jan 13 08:37:22.504: INFO: Pod "downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804464ms
Jan 13 08:37:24.509: INFO: Pod "downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007643776s
STEP: Saw pod success
Jan 13 08:37:24.509: INFO: Pod "downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f" satisfied condition "Succeeded or Failed"
Jan 13 08:37:24.511: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f container client-container: <nil>
STEP: delete the pod
Jan 13 08:37:24.536: INFO: Waiting for pod downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f to disappear
Jan 13 08:37:24.544: INFO: Pod downwardapi-volume-086c8f5d-e165-40e1-ba75-c6110d45ca4f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:24.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-604" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":240,"skipped":3947,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:24.550: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:37:24.795: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:37:27.809: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:27.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4255" for this suite.
STEP: Destroying namespace "webhook-4255-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":241,"skipped":3948,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 13 08:37:27.964: INFO: observed Pod pod-test in namespace pods-1798 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 13 08:37:27.967: INFO: observed Pod pod-test in namespace pods-1798 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  }]
Jan 13 08:37:27.979: INFO: observed Pod pod-test in namespace pods-1798 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  }]
Jan 13 08:37:28.997: INFO: observed Pod pod-test in namespace pods-1798 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  }]
Jan 13 08:37:29.901: INFO: Found Pod pod-test in namespace pods-1798 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-13 08:37:27 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jan 13 08:37:29.909: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 13 08:37:29.937: INFO: observed event type ADDED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
Jan 13 08:37:29.937: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:29.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1798" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":242,"skipped":3950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:29.943: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jan 13 08:37:29.987: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:37:31.992: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 13 08:37:32.005: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:37:34.009: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 13 08:37:34.014: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:34.021: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:36.021: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:36.025: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:38.022: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:38.025: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:40.022: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:40.027: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:42.022: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:42.026: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:44.022: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:44.025: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:46.021: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:46.026: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:48.021: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:48.026: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 13 08:37:50.022: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 13 08:37:50.027: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:50.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7928" for this suite.

• [SLOW TEST:20.097 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":243,"skipped":3982,"failed":0}
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:50.040: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-ee6a8d9d-5b70-4fcd-b783-9c94dbc4c04d
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:50.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2012" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":244,"skipped":3982,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:50.085: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:37:50.120: INFO: Creating pod...
Jan 13 08:37:50.131: INFO: Pod Quantity: 1 Status: Pending
Jan 13 08:37:51.134: INFO: Pod Quantity: 1 Status: Pending
Jan 13 08:37:52.134: INFO: Pod Status: Running
Jan 13 08:37:52.134: INFO: Creating service...
Jan 13 08:37:52.167: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/DELETE
Jan 13 08:37:52.171: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 13 08:37:52.171: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/GET
Jan 13 08:37:52.174: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 13 08:37:52.174: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/HEAD
Jan 13 08:37:52.177: INFO: http.Client request:HEAD | StatusCode:200
Jan 13 08:37:52.177: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 13 08:37:52.179: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 13 08:37:52.179: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/PATCH
Jan 13 08:37:52.181: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 13 08:37:52.181: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/POST
Jan 13 08:37:52.183: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 13 08:37:52.183: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/pods/agnhost/proxy/some/path/with/PUT
Jan 13 08:37:52.185: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 13 08:37:52.185: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/DELETE
Jan 13 08:37:52.187: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 13 08:37:52.187: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/GET
Jan 13 08:37:52.190: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 13 08:37:52.190: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/HEAD
Jan 13 08:37:52.192: INFO: http.Client request:HEAD | StatusCode:200
Jan 13 08:37:52.192: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/OPTIONS
Jan 13 08:37:52.195: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 13 08:37:52.195: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/PATCH
Jan 13 08:37:52.198: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 13 08:37:52.198: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/POST
Jan 13 08:37:52.200: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 13 08:37:52.200: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4199/services/test-service/proxy/some/path/with/PUT
Jan 13 08:37:52.203: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:52.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4199" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":245,"skipped":3983,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:37:52.252: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fd8fa22e-4299-4980-a196-925bc6da2dd8" in namespace "security-context-test-4530" to be "Succeeded or Failed"
Jan 13 08:37:52.255: INFO: Pod "busybox-readonly-false-fd8fa22e-4299-4980-a196-925bc6da2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349474ms
Jan 13 08:37:54.262: INFO: Pod "busybox-readonly-false-fd8fa22e-4299-4980-a196-925bc6da2dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01051933s
Jan 13 08:37:54.262: INFO: Pod "busybox-readonly-false-fd8fa22e-4299-4980-a196-925bc6da2dd8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:37:54.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4530" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":246,"skipped":3984,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:37:54.268: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:37:54.296: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 13 08:37:54.302: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 13 08:37:59.306: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 13 08:37:59.306: INFO: Creating deployment "test-rolling-update-deployment"
Jan 13 08:37:59.310: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 13 08:37:59.319: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 13 08:38:01.324: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 13 08:38:01.325: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 08:38:01.342: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6191  b334b132-937d-416a-a9c5-7249acf5d7ec 411734 1 2022-01-13 08:37:59 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-01-13 08:37:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f36588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-13 08:37:59 +0000 UTC,LastTransitionTime:2022-01-13 08:37:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-01-13 08:38:01 +0000 UTC,LastTransitionTime:2022-01-13 08:37:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 08:38:01.344: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-6191  ddccbbf6-d797-404a-879b-2423391497aa 411724 1 2022-01-13 08:37:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b334b132-937d-416a-a9c5-7249acf5d7ec 0xc003f36be7 0xc003f36be8}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b334b132-937d-416a-a9c5-7249acf5d7ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f36c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:38:01.344: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 13 08:38:01.344: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6191  be4fd3a7-7217-4fd9-bc23-81c383eccdfd 411733 2 2022-01-13 08:37:54 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b334b132-937d-416a-a9c5-7249acf5d7ec 0xc003f36a67 0xc003f36a68}] []  [{e2e.test Update apps/v1 2022-01-13 08:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b334b132-937d-416a-a9c5-7249acf5d7ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f36b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:38:01.346: INFO: Pod "test-rolling-update-deployment-585b757574-tclt7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-tclt7 test-rolling-update-deployment-585b757574- deployment-6191  55f04457-0d41-4045-8e68-f7b814355df7 411723 0 2022-01-13 08:37:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:10.10.28.39/32 cni.projectcalico.org/podIPs:10.10.28.39/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 ddccbbf6-d797-404a-879b-2423391497aa 0xc003f373e7 0xc003f373e8}] []  [{kube-controller-manager Update v1 2022-01-13 08:37:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddccbbf6-d797-404a-879b-2423391497aa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:38:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:38:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq42b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq42b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:37:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:38:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:38:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:37:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.39,StartTime:2022-01-13 08:37:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:38:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker://sha256:a51db9feaeb94e79df8275b6c4921047efdd40b23de5153754aedde5fd42a900,ContainerID:docker://5f21c85088bfb8d2a2580591b476b39e581447835932be9c383c3c24f0d887ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:38:01.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6191" for this suite.

• [SLOW TEST:7.083 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":247,"skipped":4004,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:38:01.351: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-b40012ed-1732-48c3-b126-0b42f2d01ebd in namespace container-probe-7485
Jan 13 08:38:03.391: INFO: Started pod test-webserver-b40012ed-1732-48c3-b126-0b42f2d01ebd in namespace container-probe-7485
STEP: checking the pod's current state and verifying that restartCount is present
Jan 13 08:38:03.393: INFO: Initial restart count of pod test-webserver-b40012ed-1732-48c3-b126-0b42f2d01ebd is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:42:03.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7485" for this suite.

• [SLOW TEST:242.566 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":248,"skipped":4004,"failed":0}
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:42:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 08:42:03.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5" in namespace "downward-api-6352" to be "Succeeded or Failed"
Jan 13 08:42:03.955: INFO: Pod "downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652481ms
Jan 13 08:42:05.958: INFO: Pod "downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005633645s
STEP: Saw pod success
Jan 13 08:42:05.958: INFO: Pod "downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5" satisfied condition "Succeeded or Failed"
Jan 13 08:42:05.960: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5 container client-container: <nil>
STEP: delete the pod
Jan 13 08:42:05.979: INFO: Waiting for pod downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5 to disappear
Jan 13 08:42:05.981: INFO: Pod downwardapi-volume-cbcbaafa-53e4-446e-8747-6124576862e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:42:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6352" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":249,"skipped":4004,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:42:05.986: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-33dc00aa-1c8b-44cb-b794-2c1ca77a1d7e in namespace container-probe-9408
Jan 13 08:42:08.034: INFO: Started pod liveness-33dc00aa-1c8b-44cb-b794-2c1ca77a1d7e in namespace container-probe-9408
STEP: checking the pod's current state and verifying that restartCount is present
Jan 13 08:42:08.035: INFO: Initial restart count of pod liveness-33dc00aa-1c8b-44cb-b794-2c1ca77a1d7e is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:46:08.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9408" for this suite.

• [SLOW TEST:242.671 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":250,"skipped":4022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:46:08.658: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-8385/configmap-test-d4328774-9178-4caf-a822-2a17e9a9cbcd
STEP: Creating a pod to test consume configMaps
Jan 13 08:46:08.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9" in namespace "configmap-8385" to be "Succeeded or Failed"
Jan 13 08:46:08.701: INFO: Pod "pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746759ms
Jan 13 08:46:10.706: INFO: Pod "pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008284088s
STEP: Saw pod success
Jan 13 08:46:10.707: INFO: Pod "pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9" satisfied condition "Succeeded or Failed"
Jan 13 08:46:10.710: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9 container env-test: <nil>
STEP: delete the pod
Jan 13 08:46:10.731: INFO: Waiting for pod pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9 to disappear
Jan 13 08:46:10.735: INFO: Pod pod-configmaps-4079eb2c-3997-4d8d-8560-21b65731c0d9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:46:10.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8385" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":251,"skipped":4086,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:46:10.740: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:47:10.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4222" for this suite.

• [SLOW TEST:60.045 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":252,"skipped":4099,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:47:10.785: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:47:11.336: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:47:14.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 13 08:47:16.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=webhook-348 attach --namespace=webhook-348 to-be-attached-pod -i -c=container1'
Jan 13 08:47:16.560: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:47:16.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-348" for this suite.
STEP: Destroying namespace "webhook-348-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":253,"skipped":4102,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:47:16.626: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-32718e52-b095-4721-bf75-a229f2c3af8a
STEP: Creating a pod to test consume configMaps
Jan 13 08:47:16.673: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588" in namespace "configmap-7828" to be "Succeeded or Failed"
Jan 13 08:47:16.675: INFO: Pod "pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538972ms
Jan 13 08:47:18.678: INFO: Pod "pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004932867s
STEP: Saw pod success
Jan 13 08:47:18.678: INFO: Pod "pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588" satisfied condition "Succeeded or Failed"
Jan 13 08:47:18.680: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:47:18.700: INFO: Waiting for pod pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588 to disappear
Jan 13 08:47:18.703: INFO: Pod pod-configmaps-ba2bfacc-c0be-4525-a96a-00326bc39588 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:47:18.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7828" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":254,"skipped":4107,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:47:18.708: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jan 13 08:47:18.751: INFO: namespace kubectl-9081
Jan 13 08:47:18.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9081 create -f -'
Jan 13 08:47:19.043: INFO: stderr: ""
Jan 13 08:47:19.043: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 13 08:47:20.046: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:47:20.046: INFO: Found 0 / 1
Jan 13 08:47:21.050: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:47:21.050: INFO: Found 1 / 1
Jan 13 08:47:21.050: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 13 08:47:21.066: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 13 08:47:21.066: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 13 08:47:21.066: INFO: wait on agnhost-primary startup in kubectl-9081 
Jan 13 08:47:21.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9081 logs agnhost-primary-mmthc agnhost-primary'
Jan 13 08:47:21.131: INFO: stderr: ""
Jan 13 08:47:21.131: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 13 08:47:21.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9081 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 13 08:47:21.197: INFO: stderr: ""
Jan 13 08:47:21.197: INFO: stdout: "service/rm2 exposed\n"
Jan 13 08:47:21.200: INFO: Service rm2 in namespace kubectl-9081 found.
STEP: exposing service
Jan 13 08:47:23.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9081 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 13 08:47:23.266: INFO: stderr: ""
Jan 13 08:47:23.266: INFO: stdout: "service/rm3 exposed\n"
Jan 13 08:47:23.273: INFO: Service rm3 in namespace kubectl-9081 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:47:25.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9081" for this suite.

• [SLOW TEST:6.577 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":255,"skipped":4107,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:47:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2576.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2576.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2576.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 13 08:47:27.348: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.351: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.353: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.355: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.360: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.362: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.364: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.365: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:27.369: INFO: Lookups using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local]

Jan 13 08:47:32.372: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.375: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.377: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.385: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.387: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.389: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.390: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:32.394: INFO: Lookups using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local]

Jan 13 08:47:37.373: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.375: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.377: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.387: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.389: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.393: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.394: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:37.397: INFO: Lookups using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local]

Jan 13 08:47:42.373: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.375: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.377: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.384: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.385: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.387: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.389: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:42.394: INFO: Lookups using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local]

Jan 13 08:47:47.374: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.377: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.379: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.381: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.387: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.389: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.392: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.394: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:47.398: INFO: Lookups using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local]

Jan 13 08:47:52.373: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.377: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.379: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.381: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.386: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.388: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.389: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.391: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local from pod dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd: the server could not find the requested resource (get pods dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd)
Jan 13 08:47:52.394: INFO: Lookups using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2576.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2576.svc.cluster.local jessie_udp@dns-test-service-2.dns-2576.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2576.svc.cluster.local]

Jan 13 08:47:57.408: INFO: DNS probes using dns-2576/dns-test-25122a64-c9e0-4caf-984c-3c90da2cdbbd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:47:57.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2576" for this suite.

• [SLOW TEST:32.168 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":256,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:47:57.453: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jan 13 08:47:57.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 create -f -'
Jan 13 08:47:57.690: INFO: stderr: ""
Jan 13 08:47:57.690: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 13 08:47:57.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:47:57.748: INFO: stderr: ""
Jan 13 08:47:57.749: INFO: stdout: "update-demo-nautilus-56ttx update-demo-nautilus-fw5s7 "
Jan 13 08:47:57.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-56ttx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:47:57.799: INFO: stderr: ""
Jan 13 08:47:57.799: INFO: stdout: ""
Jan 13 08:47:57.799: INFO: update-demo-nautilus-56ttx is created but not running
Jan 13 08:48:02.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:48:02.855: INFO: stderr: ""
Jan 13 08:48:02.855: INFO: stdout: "update-demo-nautilus-56ttx update-demo-nautilus-fw5s7 "
Jan 13 08:48:02.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-56ttx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:02.910: INFO: stderr: ""
Jan 13 08:48:02.910: INFO: stdout: "true"
Jan 13 08:48:02.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-56ttx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:48:02.962: INFO: stderr: ""
Jan 13 08:48:02.962: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:48:02.962: INFO: validating pod update-demo-nautilus-56ttx
Jan 13 08:48:02.973: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:48:02.973: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:48:02.973: INFO: update-demo-nautilus-56ttx is verified up and running
Jan 13 08:48:02.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:03.023: INFO: stderr: ""
Jan 13 08:48:03.023: INFO: stdout: "true"
Jan 13 08:48:03.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:48:03.077: INFO: stderr: ""
Jan 13 08:48:03.077: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:48:03.077: INFO: validating pod update-demo-nautilus-fw5s7
Jan 13 08:48:03.081: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:48:03.081: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:48:03.081: INFO: update-demo-nautilus-fw5s7 is verified up and running
STEP: scaling down the replication controller
Jan 13 08:48:03.083: INFO: scanned /root for discovery docs: <nil>
Jan 13 08:48:03.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 13 08:48:04.147: INFO: stderr: ""
Jan 13 08:48:04.147: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 13 08:48:04.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:48:04.205: INFO: stderr: ""
Jan 13 08:48:04.205: INFO: stdout: "update-demo-nautilus-56ttx update-demo-nautilus-fw5s7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 13 08:48:09.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:48:09.264: INFO: stderr: ""
Jan 13 08:48:09.264: INFO: stdout: "update-demo-nautilus-fw5s7 "
Jan 13 08:48:09.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:09.315: INFO: stderr: ""
Jan 13 08:48:09.315: INFO: stdout: "true"
Jan 13 08:48:09.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:48:09.367: INFO: stderr: ""
Jan 13 08:48:09.367: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:48:09.367: INFO: validating pod update-demo-nautilus-fw5s7
Jan 13 08:48:09.370: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:48:09.370: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:48:09.370: INFO: update-demo-nautilus-fw5s7 is verified up and running
STEP: scaling up the replication controller
Jan 13 08:48:09.372: INFO: scanned /root for discovery docs: <nil>
Jan 13 08:48:09.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 13 08:48:10.447: INFO: stderr: ""
Jan 13 08:48:10.447: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 13 08:48:10.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:48:10.510: INFO: stderr: ""
Jan 13 08:48:10.510: INFO: stdout: "update-demo-nautilus-fw5s7 update-demo-nautilus-wvhdc "
Jan 13 08:48:10.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:10.579: INFO: stderr: ""
Jan 13 08:48:10.579: INFO: stdout: "true"
Jan 13 08:48:10.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:48:10.639: INFO: stderr: ""
Jan 13 08:48:10.639: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:48:10.639: INFO: validating pod update-demo-nautilus-fw5s7
Jan 13 08:48:10.642: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:48:10.642: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:48:10.642: INFO: update-demo-nautilus-fw5s7 is verified up and running
Jan 13 08:48:10.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-wvhdc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:10.695: INFO: stderr: ""
Jan 13 08:48:10.695: INFO: stdout: ""
Jan 13 08:48:10.695: INFO: update-demo-nautilus-wvhdc is created but not running
Jan 13 08:48:15.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 13 08:48:15.750: INFO: stderr: ""
Jan 13 08:48:15.750: INFO: stdout: "update-demo-nautilus-fw5s7 update-demo-nautilus-wvhdc "
Jan 13 08:48:15.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:15.801: INFO: stderr: ""
Jan 13 08:48:15.802: INFO: stdout: "true"
Jan 13 08:48:15.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-fw5s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:48:15.852: INFO: stderr: ""
Jan 13 08:48:15.852: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:48:15.852: INFO: validating pod update-demo-nautilus-fw5s7
Jan 13 08:48:15.856: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:48:15.856: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:48:15.856: INFO: update-demo-nautilus-fw5s7 is verified up and running
Jan 13 08:48:15.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-wvhdc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 13 08:48:15.905: INFO: stderr: ""
Jan 13 08:48:15.905: INFO: stdout: "true"
Jan 13 08:48:15.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods update-demo-nautilus-wvhdc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 13 08:48:15.965: INFO: stderr: ""
Jan 13 08:48:15.965: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jan 13 08:48:15.965: INFO: validating pod update-demo-nautilus-wvhdc
Jan 13 08:48:15.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 13 08:48:15.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 13 08:48:15.968: INFO: update-demo-nautilus-wvhdc is verified up and running
STEP: using delete to clean up resources
Jan 13 08:48:15.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 delete --grace-period=0 --force -f -'
Jan 13 08:48:16.022: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 13 08:48:16.022: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 13 08:48:16.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get rc,svc -l name=update-demo --no-headers'
Jan 13 08:48:16.086: INFO: stderr: "No resources found in kubectl-7963 namespace.\n"
Jan 13 08:48:16.086: INFO: stdout: ""
Jan 13 08:48:16.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-7963 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 13 08:48:16.147: INFO: stderr: ""
Jan 13 08:48:16.147: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:48:16.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7963" for this suite.

• [SLOW TEST:18.702 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":257,"skipped":4126,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:48:16.155: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jan 13 08:48:16.229: INFO: Found Service test-service-ljzlr in namespace services-7794 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 13 08:48:16.229: INFO: Service test-service-ljzlr created
STEP: Getting /status
Jan 13 08:48:16.233: INFO: Service test-service-ljzlr has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 13 08:48:16.238: INFO: observed Service test-service-ljzlr in namespace services-7794 with annotations: map[] & LoadBalancer: {[]}
Jan 13 08:48:16.238: INFO: Found Service test-service-ljzlr in namespace services-7794 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 13 08:48:16.238: INFO: Service test-service-ljzlr has service status patched
STEP: updating the ServiceStatus
Jan 13 08:48:16.243: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 13 08:48:16.244: INFO: Observed Service test-service-ljzlr in namespace services-7794 with annotations: map[] & Conditions: {[]}
Jan 13 08:48:16.244: INFO: Observed event: &Service{ObjectMeta:{test-service-ljzlr  services-7794  1d7d5e37-f669-4661-9ef2-11da8792a071 414515 0 2022-01-13 08:48:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-01-13 08:48:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.157.116,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.157.116],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 13 08:48:16.244: INFO: Found Service test-service-ljzlr in namespace services-7794 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 13 08:48:16.244: INFO: Service test-service-ljzlr has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 13 08:48:16.251: INFO: observed Service test-service-ljzlr in namespace services-7794 with labels: map[test-service-static:true]
Jan 13 08:48:16.251: INFO: observed Service test-service-ljzlr in namespace services-7794 with labels: map[test-service-static:true]
Jan 13 08:48:16.251: INFO: observed Service test-service-ljzlr in namespace services-7794 with labels: map[test-service-static:true]
Jan 13 08:48:16.251: INFO: Found Service test-service-ljzlr in namespace services-7794 with labels: map[test-service:patched test-service-static:true]
Jan 13 08:48:16.251: INFO: Service test-service-ljzlr patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 13 08:48:16.260: INFO: Observed event: ADDED
Jan 13 08:48:16.260: INFO: Observed event: MODIFIED
Jan 13 08:48:16.260: INFO: Observed event: MODIFIED
Jan 13 08:48:16.260: INFO: Observed event: MODIFIED
Jan 13 08:48:16.260: INFO: Found Service test-service-ljzlr in namespace services-7794 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 13 08:48:16.260: INFO: Service test-service-ljzlr deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:48:16.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7794" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":258,"skipped":4143,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:48:16.272: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-704d47ac-46c6-4ea4-a040-6cf3faa411d7
STEP: Creating a pod to test consume configMaps
Jan 13 08:48:16.331: INFO: Waiting up to 5m0s for pod "pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f" in namespace "configmap-5411" to be "Succeeded or Failed"
Jan 13 08:48:16.336: INFO: Pod "pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.624052ms
Jan 13 08:48:18.341: INFO: Pod "pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01005095s
STEP: Saw pod success
Jan 13 08:48:18.341: INFO: Pod "pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f" satisfied condition "Succeeded or Failed"
Jan 13 08:48:18.343: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f container configmap-volume-test: <nil>
STEP: delete the pod
Jan 13 08:48:18.364: INFO: Waiting for pod pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f to disappear
Jan 13 08:48:18.366: INFO: Pod pod-configmaps-02cf1cd2-1112-4958-aafb-dda3c0dbc44f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:48:18.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5411" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":259,"skipped":4157,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:48:18.375: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 13 08:48:18.680: INFO: Pod name wrapped-volume-race-2e9e862d-8b8f-46e5-b44e-9e903681d0b6: Found 2 pods out of 5
Jan 13 08:48:23.686: INFO: Pod name wrapped-volume-race-2e9e862d-8b8f-46e5-b44e-9e903681d0b6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2e9e862d-8b8f-46e5-b44e-9e903681d0b6 in namespace emptydir-wrapper-2047, will wait for the garbage collector to delete the pods
Jan 13 08:48:33.758: INFO: Deleting ReplicationController wrapped-volume-race-2e9e862d-8b8f-46e5-b44e-9e903681d0b6 took: 3.064712ms
Jan 13 08:48:33.858: INFO: Terminating ReplicationController wrapped-volume-race-2e9e862d-8b8f-46e5-b44e-9e903681d0b6 pods took: 100.085447ms
STEP: Creating RC which spawns configmap-volume pods
Jan 13 08:48:48.777: INFO: Pod name wrapped-volume-race-9564b680-8e39-48b9-8289-388db52ee0cd: Found 0 pods out of 5
Jan 13 08:48:53.782: INFO: Pod name wrapped-volume-race-9564b680-8e39-48b9-8289-388db52ee0cd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9564b680-8e39-48b9-8289-388db52ee0cd in namespace emptydir-wrapper-2047, will wait for the garbage collector to delete the pods
Jan 13 08:49:03.852: INFO: Deleting ReplicationController wrapped-volume-race-9564b680-8e39-48b9-8289-388db52ee0cd took: 3.335043ms
Jan 13 08:49:03.953: INFO: Terminating ReplicationController wrapped-volume-race-9564b680-8e39-48b9-8289-388db52ee0cd pods took: 100.934171ms
STEP: Creating RC which spawns configmap-volume pods
Jan 13 08:49:18.968: INFO: Pod name wrapped-volume-race-911b9b90-50ef-48f3-a810-efec0a132e1f: Found 0 pods out of 5
Jan 13 08:49:23.974: INFO: Pod name wrapped-volume-race-911b9b90-50ef-48f3-a810-efec0a132e1f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-911b9b90-50ef-48f3-a810-efec0a132e1f in namespace emptydir-wrapper-2047, will wait for the garbage collector to delete the pods
Jan 13 08:49:34.046: INFO: Deleting ReplicationController wrapped-volume-race-911b9b90-50ef-48f3-a810-efec0a132e1f took: 4.135119ms
Jan 13 08:49:34.147: INFO: Terminating ReplicationController wrapped-volume-race-911b9b90-50ef-48f3-a810-efec0a132e1f pods took: 100.631665ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:49:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2047" for this suite.

• [SLOW TEST:90.529 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":260,"skipped":4167,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:49:48.904: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 13 08:49:48.934: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:49:53.747: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:11.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5850" for this suite.

• [SLOW TEST:23.030 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":261,"skipped":4173,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:11.934: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-ff965469-fcef-4185-bf64-a057361fd51c
STEP: Creating a pod to test consume configMaps
Jan 13 08:50:11.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9" in namespace "configmap-5192" to be "Succeeded or Failed"
Jan 13 08:50:11.978: INFO: Pod "pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567693ms
Jan 13 08:50:13.981: INFO: Pod "pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006006063s
Jan 13 08:50:15.984: INFO: Pod "pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009816229s
STEP: Saw pod success
Jan 13 08:50:15.984: INFO: Pod "pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9" satisfied condition "Succeeded or Failed"
Jan 13 08:50:15.986: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:50:16.009: INFO: Waiting for pod pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9 to disappear
Jan 13 08:50:16.017: INFO: Pod pod-configmaps-9235869d-45cf-438a-b106-a9078c6775d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:16.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5192" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":262,"skipped":4174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:16.022: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:18.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8120" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":263,"skipped":4209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:18.086: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 13 08:50:18.128: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:18.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2142" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":264,"skipped":4257,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:18.157: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jan 13 08:50:18.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9129 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jan 13 08:50:18.244: INFO: stderr: ""
Jan 13 08:50:18.244: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 13 08:50:18.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9129 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Jan 13 08:50:18.527: INFO: stderr: ""
Jan 13 08:50:18.527: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jan 13 08:50:18.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=kubectl-9129 delete pods e2e-test-httpd-pod'
Jan 13 08:50:28.888: INFO: stderr: ""
Jan 13 08:50:28.888: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:28.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9129" for this suite.

• [SLOW TEST:10.735 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":265,"skipped":4263,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:28.893: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-585f0519-e9b5-4ca6-bcc0-a725728a7b33
STEP: Creating configMap with name cm-test-opt-upd-1d3733cd-263c-4097-a5a1-4da9be6103d2
STEP: Creating the pod
Jan 13 08:50:28.953: INFO: The status of Pod pod-configmaps-55ce76e7-9ee6-42fa-ba77-a02a0419669e is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:50:30.957: INFO: The status of Pod pod-configmaps-55ce76e7-9ee6-42fa-ba77-a02a0419669e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-585f0519-e9b5-4ca6-bcc0-a725728a7b33
STEP: Updating configmap cm-test-opt-upd-1d3733cd-263c-4097-a5a1-4da9be6103d2
STEP: Creating configMap with name cm-test-opt-create-50bb530a-9436-4388-bd74-50d608e3ac12
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:33.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7235" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":266,"skipped":4275,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jan 13 08:50:33.573: INFO: created pod pod-service-account-defaultsa
Jan 13 08:50:33.573: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 13 08:50:33.576: INFO: created pod pod-service-account-mountsa
Jan 13 08:50:33.576: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 13 08:50:33.582: INFO: created pod pod-service-account-nomountsa
Jan 13 08:50:33.582: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 13 08:50:33.592: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 13 08:50:33.592: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 13 08:50:33.597: INFO: created pod pod-service-account-mountsa-mountspec
Jan 13 08:50:33.597: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 13 08:50:33.607: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 13 08:50:33.607: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 13 08:50:33.619: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 13 08:50:33.619: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 13 08:50:33.631: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 13 08:50:33.631: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 13 08:50:33.637: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 13 08:50:33.637: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:33.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2262" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":267,"skipped":4288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:33.646: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-14c92e4c-b1c8-433a-a604-368613094cc7
STEP: Creating a pod to test consume secrets
Jan 13 08:50:33.688: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879" in namespace "projected-4906" to be "Succeeded or Failed"
Jan 13 08:50:33.691: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879": Phase="Pending", Reason="", readiness=false. Elapsed: 3.012336ms
Jan 13 08:50:35.694: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006578048s
Jan 13 08:50:37.699: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011560447s
Jan 13 08:50:39.702: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014281268s
Jan 13 08:50:41.705: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01733187s
Jan 13 08:50:43.708: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.020140826s
STEP: Saw pod success
Jan 13 08:50:43.708: INFO: Pod "pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879" satisfied condition "Succeeded or Failed"
Jan 13 08:50:43.710: INFO: Trying to get logs from node worker-s001 pod pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:50:43.721: INFO: Waiting for pod pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879 to disappear
Jan 13 08:50:43.725: INFO: Pod pod-projected-secrets-32838d7e-6a66-4311-8dde-cd0417176879 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:43.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4906" for this suite.

• [SLOW TEST:10.083 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":268,"skipped":4312,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:43.729: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 13 08:50:43.760: INFO: Waiting up to 5m0s for pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37" in namespace "emptydir-9148" to be "Succeeded or Failed"
Jan 13 08:50:43.763: INFO: Pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336511ms
Jan 13 08:50:45.766: INFO: Pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005753672s
Jan 13 08:50:47.769: INFO: Pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008860266s
Jan 13 08:50:49.773: INFO: Pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012918553s
Jan 13 08:50:51.778: INFO: Pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018181784s
STEP: Saw pod success
Jan 13 08:50:51.779: INFO: Pod "pod-ada71bdb-b033-4b49-b5be-d00e98923a37" satisfied condition "Succeeded or Failed"
Jan 13 08:50:51.781: INFO: Trying to get logs from node worker-s001 pod pod-ada71bdb-b033-4b49-b5be-d00e98923a37 container test-container: <nil>
STEP: delete the pod
Jan 13 08:50:51.795: INFO: Waiting for pod pod-ada71bdb-b033-4b49-b5be-d00e98923a37 to disappear
Jan 13 08:50:51.798: INFO: Pod pod-ada71bdb-b033-4b49-b5be-d00e98923a37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:51.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9148" for this suite.

• [SLOW TEST:8.078 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":269,"skipped":4318,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:51.807: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-d4cf5965-4e5c-4813-93df-9452a402732c
STEP: Creating a pod to test consume configMaps
Jan 13 08:50:51.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd" in namespace "configmap-1370" to be "Succeeded or Failed"
Jan 13 08:50:51.858: INFO: Pod "pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.304674ms
Jan 13 08:50:53.867: INFO: Pod "pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016592658s
STEP: Saw pod success
Jan 13 08:50:53.867: INFO: Pod "pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd" satisfied condition "Succeeded or Failed"
Jan 13 08:50:53.868: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd container agnhost-container: <nil>
STEP: delete the pod
Jan 13 08:50:53.884: INFO: Waiting for pod pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd to disappear
Jan 13 08:50:53.886: INFO: Pod pod-configmaps-7434820e-678a-48c3-b6e1-aa5c6547abcd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:50:53.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1370" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":270,"skipped":4321,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:50:53.890: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jan 13 08:50:53.932: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:50:55.936: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 13 08:50:55.948: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:50:57.953: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 13 08:50:57.965: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:50:57.973: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 08:50:59.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:50:59.980: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 08:51:01.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:51:01.977: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 08:51:03.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:51:03.979: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 08:51:05.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:51:05.978: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 08:51:07.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:51:07.978: INFO: Pod pod-with-poststart-http-hook still exists
Jan 13 08:51:09.974: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 13 08:51:09.979: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:51:09.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7598" for this suite.

• [SLOW TEST:16.100 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":271,"skipped":4340,"failed":0}
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:51:09.990: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:51:10.020: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 13 08:51:10.035: INFO: The status of Pod pod-logs-websocket-90be75d8-029d-470e-bc54-ca94dea077a1 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:51:12.038: INFO: The status of Pod pod-logs-websocket-90be75d8-029d-470e-bc54-ca94dea077a1 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:51:12.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9121" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":272,"skipped":4340,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:51:12.057: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-1459
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1459
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1459
Jan 13 08:51:12.095: INFO: Found 0 stateful pods, waiting for 1
Jan 13 08:51:22.098: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 13 08:51:22.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:51:22.232: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:51:22.232: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:51:22.232: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:51:22.234: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 13 08:51:32.238: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:51:32.238: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:51:32.254: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999809s
Jan 13 08:51:33.257: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991063623s
Jan 13 08:51:34.261: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988480923s
Jan 13 08:51:35.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984224034s
Jan 13 08:51:36.269: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981358122s
Jan 13 08:51:37.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975296821s
Jan 13 08:51:38.277: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971955922s
Jan 13 08:51:39.280: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968501409s
Jan 13 08:51:40.285: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965019368s
Jan 13 08:51:41.289: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.471954ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1459
Jan 13 08:51:42.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:51:42.435: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 08:51:42.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 08:51:42.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 08:51:42.437: INFO: Found 1 stateful pods, waiting for 3
Jan 13 08:51:52.440: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:51:52.440: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 13 08:51:52.440: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 13 08:51:52.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:51:52.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:51:52.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:51:52.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:51:52.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:51:52.710: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:51:52.710: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:51:52.710: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:51:52.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 13 08:51:52.836: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 13 08:51:52.836: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 13 08:51:52.836: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 13 08:51:52.836: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:51:52.838: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 13 08:52:02.844: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:52:02.845: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:52:02.845: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 13 08:52:02.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999812s
Jan 13 08:52:03.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996561196s
Jan 13 08:52:04.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992050864s
Jan 13 08:52:05.873: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986538498s
Jan 13 08:52:06.878: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981021319s
Jan 13 08:52:07.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976499138s
Jan 13 08:52:08.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.971120476s
Jan 13 08:52:09.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968039842s
Jan 13 08:52:10.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963244465s
Jan 13 08:52:11.902: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.454579ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1459
Jan 13 08:52:12.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:13.031: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 08:52:13.031: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 08:52:13.031: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 08:52:13.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:13.158: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 13 08:52:13.158: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 13 08:52:13.158: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 13 08:52:13.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:13.299: INFO: rc: 126
Jan 13 08:52:13.299: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:
OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: process_linux.go:130: executing setns process caused: exit status 1: unknown

stderr:
command terminated with exit code 126

error:
exit status 126
Jan 13 08:52:23.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:23.356: INFO: rc: 1
Jan 13 08:52:23.356: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:52:33.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:33.417: INFO: rc: 1
Jan 13 08:52:33.417: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:52:43.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:43.497: INFO: rc: 1
Jan 13 08:52:43.497: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:52:53.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:52:53.556: INFO: rc: 1
Jan 13 08:52:53.556: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:53:03.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:53:03.609: INFO: rc: 1
Jan 13 08:53:03.609: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:53:13.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:53:13.663: INFO: rc: 1
Jan 13 08:53:13.663: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:53:23.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:53:23.715: INFO: rc: 1
Jan 13 08:53:23.715: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:53:33.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:53:33.770: INFO: rc: 1
Jan 13 08:53:33.770: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:53:43.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:53:43.825: INFO: rc: 1
Jan 13 08:53:43.825: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:53:53.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:53:53.880: INFO: rc: 1
Jan 13 08:53:53.880: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:54:03.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:54:03.935: INFO: rc: 1
Jan 13 08:54:03.935: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:54:13.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:54:13.991: INFO: rc: 1
Jan 13 08:54:13.991: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:54:23.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:54:24.047: INFO: rc: 1
Jan 13 08:54:24.047: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:54:34.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:54:34.102: INFO: rc: 1
Jan 13 08:54:34.102: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:54:44.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:54:44.158: INFO: rc: 1
Jan 13 08:54:44.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:54:54.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:54:54.213: INFO: rc: 1
Jan 13 08:54:54.213: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:55:04.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:55:04.266: INFO: rc: 1
Jan 13 08:55:04.266: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:55:14.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:55:14.322: INFO: rc: 1
Jan 13 08:55:14.322: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:55:24.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:55:24.377: INFO: rc: 1
Jan 13 08:55:24.377: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:55:34.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:55:34.432: INFO: rc: 1
Jan 13 08:55:34.432: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:55:44.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:55:44.489: INFO: rc: 1
Jan 13 08:55:44.489: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:55:54.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:55:54.544: INFO: rc: 1
Jan 13 08:55:54.544: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:56:04.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:56:04.604: INFO: rc: 1
Jan 13 08:56:04.604: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:56:14.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:56:14.664: INFO: rc: 1
Jan 13 08:56:14.664: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:56:24.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:56:24.720: INFO: rc: 1
Jan 13 08:56:24.720: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:56:34.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:56:34.774: INFO: rc: 1
Jan 13 08:56:34.774: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:56:44.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:56:44.830: INFO: rc: 1
Jan 13 08:56:44.830: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:56:54.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:56:54.897: INFO: rc: 1
Jan 13 08:56:54.897: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:57:04.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:57:04.955: INFO: rc: 1
Jan 13 08:57:04.955: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jan 13 08:57:14.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=statefulset-1459 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 13 08:57:15.009: INFO: rc: 1
Jan 13 08:57:15.009: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jan 13 08:57:15.009: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jan 13 08:57:15.016: INFO: Deleting all statefulset in ns statefulset-1459
Jan 13 08:57:15.020: INFO: Scaling statefulset ss to 0
Jan 13 08:57:15.025: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 08:57:15.027: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:57:15.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1459" for this suite.

• [SLOW TEST:362.989 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":273,"skipped":4347,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:57:15.046: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 13 08:57:15.087: INFO: The status of Pod annotationupdate60d03bb0-b2cd-4200-a61b-d3bf111aacf4 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:57:17.090: INFO: The status of Pod annotationupdate60d03bb0-b2cd-4200-a61b-d3bf111aacf4 is Running (Ready = true)
Jan 13 08:57:17.613: INFO: Successfully updated pod "annotationupdate60d03bb0-b2cd-4200-a61b-d3bf111aacf4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:57:19.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4678" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":274,"skipped":4362,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:57:19.634: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-73103ae1-3e45-4e8d-9adc-14786d14e4bd
STEP: Creating a pod to test consume secrets
Jan 13 08:57:19.689: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3" in namespace "projected-7608" to be "Succeeded or Failed"
Jan 13 08:57:19.698: INFO: Pod "pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.398479ms
Jan 13 08:57:21.702: INFO: Pod "pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012562641s
Jan 13 08:57:23.706: INFO: Pod "pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016968427s
STEP: Saw pod success
Jan 13 08:57:23.706: INFO: Pod "pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3" satisfied condition "Succeeded or Failed"
Jan 13 08:57:23.709: INFO: Trying to get logs from node worker-s001 pod pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 13 08:57:23.726: INFO: Waiting for pod pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3 to disappear
Jan 13 08:57:23.728: INFO: Pod pod-projected-secrets-3dbe293c-010e-44b2-93c5-8b324b5e93d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:57:23.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7608" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":275,"skipped":4366,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:57:23.736: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5573
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 13 08:57:23.762: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 13 08:57:23.783: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 08:57:25.787: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:27.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:29.790: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:31.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:33.787: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:35.787: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:37.787: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:39.795: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:41.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:43.790: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 13 08:57:45.791: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 13 08:57:45.796: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jan 13 08:57:47.816: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 13 08:57:47.816: INFO: Breadth first check of 10.10.28.10 on host 192.168.100.59...
Jan 13 08:57:47.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.28.13:9080/dial?request=hostname&protocol=udp&host=10.10.28.10&port=8081&tries=1'] Namespace:pod-network-test-5573 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:57:47.818: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:57:47.900: INFO: Waiting for responses: map[]
Jan 13 08:57:47.900: INFO: reached 10.10.28.10 after 0/1 tries
Jan 13 08:57:47.900: INFO: Breadth first check of 10.10.131.158 on host 192.168.100.60...
Jan 13 08:57:47.902: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.28.13:9080/dial?request=hostname&protocol=udp&host=10.10.131.158&port=8081&tries=1'] Namespace:pod-network-test-5573 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 08:57:47.902: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 08:57:47.980: INFO: Waiting for responses: map[]
Jan 13 08:57:47.980: INFO: reached 10.10.131.158 after 0/1 tries
Jan 13 08:57:47.980: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:57:47.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5573" for this suite.

• [SLOW TEST:24.250 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":276,"skipped":4371,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:57:47.986: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jan 13 08:57:48.021: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:58:11.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8529" for this suite.

• [SLOW TEST:23.100 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":277,"skipped":4373,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:58:11.086: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:58:11.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9101" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":278,"skipped":4375,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:58:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:58:11.176: INFO: Creating simple deployment test-new-deployment
Jan 13 08:58:11.188: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jan 13 08:58:13.230: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1822  d1af273b-8877-48c7-822e-5ff429f6dbae 418723 3 2022-01-13 08:58:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-01-13 08:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-01-13 08:58:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037fb3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-13 08:58:13 +0000 UTC,LastTransitionTime:2022-01-13 08:58:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-01-13 08:58:13 +0000 UTC,LastTransitionTime:2022-01-13 08:58:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 13 08:58:13.235: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-1822  eeabc40d-bcd6-44ef-86af-621aa81ac8d8 418727 2 2022-01-13 08:58:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d1af273b-8877-48c7-822e-5ff429f6dbae 0xc0037fb867 0xc0037fb868}] []  [{kube-controller-manager Update apps/v1 2022-01-13 08:58:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1af273b-8877-48c7-822e-5ff429f6dbae\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037fb8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 13 08:58:13.239: INFO: Pod "test-new-deployment-847dcfb7fb-25nwg" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-25nwg test-new-deployment-847dcfb7fb- deployment-1822  6d5a346d-74a4-4167-8102-701545dd2dd7 418728 0 2022-01-13 08:58:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb eeabc40d-bcd6-44ef-86af-621aa81ac8d8 0xc0076ceb37 0xc0076ceb38}] []  [{kube-controller-manager Update v1 2022-01-13 08:58:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eeabc40d-bcd6-44ef-86af-621aa81ac8d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2klt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2klt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:58:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 13 08:58:13.239: INFO: Pod "test-new-deployment-847dcfb7fb-w6wwz" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-w6wwz test-new-deployment-847dcfb7fb- deployment-1822  44cde2ad-5c57-46c2-b5d0-f6ceff613914 418718 0 2022-01-13 08:58:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:10.10.28.7/32 cni.projectcalico.org/podIPs:10.10.28.7/32] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb eeabc40d-bcd6-44ef-86af-621aa81ac8d8 0xc0076cee30 0xc0076cee31}] []  [{kube-controller-manager Update v1 2022-01-13 08:58:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eeabc40d-bcd6-44ef-86af-621aa81ac8d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2022-01-13 08:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2022-01-13 08:58:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5692z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5692z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:58:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-13 08:58:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.59,PodIP:10.10.28.7,StartTime:2022-01-13 08:58:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-13 08:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://f2ab1154bc2fe2007ca0e5eb1c0df431d6f7e3f65c230919c122ec518cebb152,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:58:13.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1822" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":279,"skipped":4387,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:58:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 13 08:58:33.409: INFO: EndpointSlice for Service endpointslice-3827/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:58:43.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3827" for this suite.

• [SLOW TEST:30.180 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":280,"skipped":4388,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:58:43.432: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 13 08:58:43.760: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 08:58:46.775: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 08:58:46.779: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 08:58:49.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3924" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.548 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":281,"skipped":4393,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 08:58:49.980: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0113 08:58:50.038247      24 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:02.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2096" for this suite.

• [SLOW TEST:72.094 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":282,"skipped":4396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:02.074: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 13 09:00:02.172: INFO: Waiting up to 5m0s for pod "pod-2a46c46e-2e4d-47b1-9f98-15872d049971" in namespace "emptydir-2698" to be "Succeeded or Failed"
Jan 13 09:00:02.174: INFO: Pod "pod-2a46c46e-2e4d-47b1-9f98-15872d049971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3175ms
Jan 13 09:00:04.178: INFO: Pod "pod-2a46c46e-2e4d-47b1-9f98-15872d049971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005735664s
Jan 13 09:00:06.184: INFO: Pod "pod-2a46c46e-2e4d-47b1-9f98-15872d049971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012451266s
STEP: Saw pod success
Jan 13 09:00:06.184: INFO: Pod "pod-2a46c46e-2e4d-47b1-9f98-15872d049971" satisfied condition "Succeeded or Failed"
Jan 13 09:00:06.186: INFO: Trying to get logs from node worker-s001 pod pod-2a46c46e-2e4d-47b1-9f98-15872d049971 container test-container: <nil>
STEP: delete the pod
Jan 13 09:00:06.215: INFO: Waiting for pod pod-2a46c46e-2e4d-47b1-9f98-15872d049971 to disappear
Jan 13 09:00:06.221: INFO: Pod pod-2a46c46e-2e4d-47b1-9f98-15872d049971 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:06.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2698" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":283,"skipped":4468,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:06.227: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 13 09:00:06.260: INFO: Waiting up to 5m0s for pod "pod-d4ff0316-f098-4bcf-a775-2f331ba73683" in namespace "emptydir-6006" to be "Succeeded or Failed"
Jan 13 09:00:06.267: INFO: Pod "pod-d4ff0316-f098-4bcf-a775-2f331ba73683": Phase="Pending", Reason="", readiness=false. Elapsed: 6.635142ms
Jan 13 09:00:08.271: INFO: Pod "pod-d4ff0316-f098-4bcf-a775-2f331ba73683": Phase="Running", Reason="", readiness=true. Elapsed: 2.010804921s
Jan 13 09:00:10.276: INFO: Pod "pod-d4ff0316-f098-4bcf-a775-2f331ba73683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015562178s
STEP: Saw pod success
Jan 13 09:00:10.276: INFO: Pod "pod-d4ff0316-f098-4bcf-a775-2f331ba73683" satisfied condition "Succeeded or Failed"
Jan 13 09:00:10.278: INFO: Trying to get logs from node worker-s001 pod pod-d4ff0316-f098-4bcf-a775-2f331ba73683 container test-container: <nil>
STEP: delete the pod
Jan 13 09:00:10.297: INFO: Waiting for pod pod-d4ff0316-f098-4bcf-a775-2f331ba73683 to disappear
Jan 13 09:00:10.302: INFO: Pod pod-d4ff0316-f098-4bcf-a775-2f331ba73683 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:10.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6006" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":284,"skipped":4474,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:10.317: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2052
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2052
I0113 09:00:10.387661      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2052, replica count: 2
Jan 13 09:00:13.439: INFO: Creating new exec pod
I0113 09:00:13.439403      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 13 09:00:16.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2052 exec execpodtp2z8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 13 09:00:16.689: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 13 09:00:16.689: INFO: stdout: "externalname-service-5w4zl"
Jan 13 09:00:16.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2052 exec execpodtp2z8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.48.71 80'
Jan 13 09:00:16.816: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.48.71 80\nConnection to 10.96.48.71 80 port [tcp/http] succeeded!\n"
Jan 13 09:00:16.816: INFO: stdout: ""
Jan 13 09:00:17.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=services-2052 exec execpodtp2z8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.48.71 80'
Jan 13 09:00:17.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.48.71 80\nConnection to 10.96.48.71 80 port [tcp/http] succeeded!\n"
Jan 13 09:00:17.960: INFO: stdout: "externalname-service-5w4zl"
Jan 13 09:00:17.960: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:17.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2052" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:7.667 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":285,"skipped":4488,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:17.984: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 13 09:00:18.048: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c" in namespace "projected-8984" to be "Succeeded or Failed"
Jan 13 09:00:18.050: INFO: Pod "downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869178ms
Jan 13 09:00:20.054: INFO: Pod "downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005359522s
STEP: Saw pod success
Jan 13 09:00:20.054: INFO: Pod "downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c" satisfied condition "Succeeded or Failed"
Jan 13 09:00:20.056: INFO: Trying to get logs from node worker-s001 pod downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c container client-container: <nil>
STEP: delete the pod
Jan 13 09:00:20.079: INFO: Waiting for pod downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c to disappear
Jan 13 09:00:20.093: INFO: Pod downwardapi-volume-2f23773b-1f9a-4193-ac33-d4af8f1cc59c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:20.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8984" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":286,"skipped":4493,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:20.100: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-nmkk
STEP: Creating a pod to test atomic-volume-subpath
Jan 13 09:00:20.149: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nmkk" in namespace "subpath-2885" to be "Succeeded or Failed"
Jan 13 09:00:20.152: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27513ms
Jan 13 09:00:22.157: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007760011s
Jan 13 09:00:24.160: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 4.011428711s
Jan 13 09:00:26.166: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 6.017026738s
Jan 13 09:00:28.170: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 8.020909569s
Jan 13 09:00:30.173: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 10.024008602s
Jan 13 09:00:32.177: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 12.027793746s
Jan 13 09:00:34.181: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 14.031570522s
Jan 13 09:00:36.185: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 16.035711556s
Jan 13 09:00:38.189: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 18.039975964s
Jan 13 09:00:40.195: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 20.045648469s
Jan 13 09:00:42.201: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Running", Reason="", readiness=true. Elapsed: 22.051669753s
Jan 13 09:00:44.210: INFO: Pod "pod-subpath-test-downwardapi-nmkk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.060667301s
STEP: Saw pod success
Jan 13 09:00:44.210: INFO: Pod "pod-subpath-test-downwardapi-nmkk" satisfied condition "Succeeded or Failed"
Jan 13 09:00:44.212: INFO: Trying to get logs from node worker-s001 pod pod-subpath-test-downwardapi-nmkk container test-container-subpath-downwardapi-nmkk: <nil>
STEP: delete the pod
Jan 13 09:00:44.231: INFO: Waiting for pod pod-subpath-test-downwardapi-nmkk to disappear
Jan 13 09:00:44.233: INFO: Pod pod-subpath-test-downwardapi-nmkk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nmkk
Jan 13 09:00:44.233: INFO: Deleting pod "pod-subpath-test-downwardapi-nmkk" in namespace "subpath-2885"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:44.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2885" for this suite.

• [SLOW TEST:24.139 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":287,"skipped":4500,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:44.239: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 13 09:00:44.277: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:00:46.281: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:47.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3028" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":288,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:47.298: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 13 09:00:47.356: INFO: The status of Pod annotationupdate1f984f7b-efc2-483c-8990-a05933efc32a is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:00:49.359: INFO: The status of Pod annotationupdate1f984f7b-efc2-483c-8990-a05933efc32a is Running (Ready = true)
Jan 13 09:00:49.874: INFO: Successfully updated pod "annotationupdate1f984f7b-efc2-483c-8990-a05933efc32a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:51.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9436" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":289,"skipped":4529,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:51.893: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 13 09:00:51.923: INFO: Waiting up to 5m0s for pod "pod-6d1b9363-26bd-446d-8737-dd240b77c042" in namespace "emptydir-252" to be "Succeeded or Failed"
Jan 13 09:00:51.947: INFO: Pod "pod-6d1b9363-26bd-446d-8737-dd240b77c042": Phase="Pending", Reason="", readiness=false. Elapsed: 24.522055ms
Jan 13 09:00:53.950: INFO: Pod "pod-6d1b9363-26bd-446d-8737-dd240b77c042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026977183s
STEP: Saw pod success
Jan 13 09:00:53.950: INFO: Pod "pod-6d1b9363-26bd-446d-8737-dd240b77c042" satisfied condition "Succeeded or Failed"
Jan 13 09:00:53.951: INFO: Trying to get logs from node worker-s001 pod pod-6d1b9363-26bd-446d-8737-dd240b77c042 container test-container: <nil>
STEP: delete the pod
Jan 13 09:00:53.964: INFO: Waiting for pod pod-6d1b9363-26bd-446d-8737-dd240b77c042 to disappear
Jan 13 09:00:53.966: INFO: Pod pod-6d1b9363-26bd-446d-8737-dd240b77c042 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:53.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-252" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":290,"skipped":4535,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:53.975: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 09:00:54.777: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 13 09:00:56.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661254, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661254, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661254, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661254, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 09:00:59.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:00:59.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3958" for this suite.
STEP: Destroying namespace "webhook-3958-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.893 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":291,"skipped":4540,"failed":0}
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:00:59.869: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 13 09:00:59.908: INFO: PodSpec: initContainers in spec.initContainers
Jan 13 09:01:47.611: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e1c48f51-cd9f-41a3-bc74-bb1927dd8594", GenerateName:"", Namespace:"init-container-369", SelfLink:"", UID:"5b7b75e9-b136-4b34-af61-ff4bf06900d4", ResourceVersion:"420355", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63777661259, loc:(*time.Location)(0x9e12f00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"908876947"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.10.28.31/32", "cni.projectcalico.org/podIPs":"10.10.28.31/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0063e5f38), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0063e5f50)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0063e5f68), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0063e5f80)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0063e5f98), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0063e5fb0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-d4nnq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000fb7e20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d4nnq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d4nnq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-d4nnq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc008c77f78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-s001", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002309ea0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002312020)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023120e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0023120e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0023120ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006da4970), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661259, loc:(*time.Location)(0x9e12f00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661259, loc:(*time.Location)(0x9e12f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661259, loc:(*time.Location)(0x9e12f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777661259, loc:(*time.Location)(0x9e12f00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.100.59", PodIP:"10.10.28.31", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.28.31"}}, StartTime:(*v1.Time)(0xc0063e5fe0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002309f80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002974000)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://c064c46afc385a7df3abe1dccba118de38fa43ff6000a69f6dbc7520e62510e7", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000fb7f00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000fb7ec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00231216f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:01:47.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-369" for this suite.

• [SLOW TEST:47.753 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":292,"skipped":4540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:01:47.622: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 09:01:47.666: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:01:54.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-228" for this suite.

• [SLOW TEST:6.619 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":293,"skipped":4568,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:01:54.241: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-3bafdeaf-6d4e-41a4-91e4-6094b00492a4
STEP: Creating a pod to test consume configMaps
Jan 13 09:01:54.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972" in namespace "configmap-123" to be "Succeeded or Failed"
Jan 13 09:01:54.339: INFO: Pod "pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972": Phase="Pending", Reason="", readiness=false. Elapsed: 29.616682ms
Jan 13 09:01:56.344: INFO: Pod "pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034804342s
Jan 13 09:01:58.348: INFO: Pod "pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03922883s
STEP: Saw pod success
Jan 13 09:01:58.348: INFO: Pod "pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972" satisfied condition "Succeeded or Failed"
Jan 13 09:01:58.351: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 09:01:58.366: INFO: Waiting for pod pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972 to disappear
Jan 13 09:01:58.369: INFO: Pod pod-configmaps-22784c2f-5e09-49f1-8899-73ffd01f2972 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:01:58.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-123" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":294,"skipped":4580,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:01:58.378: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f
Jan 13 09:01:58.462: INFO: Pod name my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f: Found 0 pods out of 1
Jan 13 09:02:03.466: INFO: Pod name my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f: Found 1 pods out of 1
Jan 13 09:02:03.466: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f" are running
Jan 13 09:02:03.468: INFO: Pod "my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f-l6ggc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 09:01:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 09:02:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 09:02:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-13 09:01:58 +0000 UTC Reason: Message:}])
Jan 13 09:02:03.468: INFO: Trying to dial the pod
Jan 13 09:02:08.476: INFO: Controller my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f: Got expected result from replica 1 [my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f-l6ggc]: "my-hostname-basic-e6ff37e2-5541-47b2-904f-d2ff9c24af1f-l6ggc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:02:08.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8689" for this suite.

• [SLOW TEST:10.107 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":295,"skipped":4597,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:02:08.485: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-cd4c5652-b6b7-4acc-844d-f223b4917b9a
STEP: Creating a pod to test consume secrets
Jan 13 09:02:08.539: INFO: Waiting up to 5m0s for pod "pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6" in namespace "secrets-6115" to be "Succeeded or Failed"
Jan 13 09:02:08.546: INFO: Pod "pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.410672ms
Jan 13 09:02:10.550: INFO: Pod "pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011237191s
STEP: Saw pod success
Jan 13 09:02:10.550: INFO: Pod "pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6" satisfied condition "Succeeded or Failed"
Jan 13 09:02:10.552: INFO: Trying to get logs from node worker-s001 pod pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6 container secret-volume-test: <nil>
STEP: delete the pod
Jan 13 09:02:10.574: INFO: Waiting for pod pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6 to disappear
Jan 13 09:02:10.577: INFO: Pod pod-secrets-c8a3b8dc-3467-47bd-9082-8243024829c6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:02:10.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6115" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":296,"skipped":4606,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:02:10.586: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 09:02:10.655: INFO: created pod
Jan 13 09:02:10.655: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4123" to be "Succeeded or Failed"
Jan 13 09:02:10.661: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.907247ms
Jan 13 09:02:12.666: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011718825s
STEP: Saw pod success
Jan 13 09:02:12.666: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 13 09:02:42.666: INFO: polling logs
Jan 13 09:02:42.683: INFO: Pod logs: 
2022/01/13 09:02:11 OK: Got token
2022/01/13 09:02:11 validating with in-cluster discovery
2022/01/13 09:02:11 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/01/13 09:02:11 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4123:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1642065130, NotBefore:1642064530, IssuedAt:1642064530, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4123", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8ae8f258-7052-40b8-aaca-dc50d31f9e97"}}}
2022/01/13 09:02:11 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/01/13 09:02:11 OK: Validated signature on JWT
2022/01/13 09:02:11 OK: Got valid claims from token!
2022/01/13 09:02:11 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4123:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1642065130, NotBefore:1642064530, IssuedAt:1642064530, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4123", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8ae8f258-7052-40b8-aaca-dc50d31f9e97"}}}

Jan 13 09:02:42.683: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:02:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4123" for this suite.

• [SLOW TEST:32.107 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":297,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:02:42.693: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 13 09:02:42.734: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:02:58.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2996" for this suite.

• [SLOW TEST:16.192 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":298,"skipped":4634,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:02:58.886: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-vftt
STEP: Creating a pod to test atomic-volume-subpath
Jan 13 09:02:58.921: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vftt" in namespace "subpath-4713" to be "Succeeded or Failed"
Jan 13 09:02:58.924: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461185ms
Jan 13 09:03:00.928: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 2.006835924s
Jan 13 09:03:02.932: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 4.011383006s
Jan 13 09:03:04.937: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 6.015688855s
Jan 13 09:03:06.941: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 8.019485879s
Jan 13 09:03:08.945: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 10.024114792s
Jan 13 09:03:10.949: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 12.027768189s
Jan 13 09:03:12.955: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 14.034160246s
Jan 13 09:03:14.960: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 16.039289755s
Jan 13 09:03:16.964: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 18.042637219s
Jan 13 09:03:18.968: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 20.047066415s
Jan 13 09:03:20.990: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Running", Reason="", readiness=true. Elapsed: 22.068449005s
Jan 13 09:03:22.993: INFO: Pod "pod-subpath-test-configmap-vftt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.072071819s
STEP: Saw pod success
Jan 13 09:03:22.993: INFO: Pod "pod-subpath-test-configmap-vftt" satisfied condition "Succeeded or Failed"
Jan 13 09:03:22.995: INFO: Trying to get logs from node worker-s001 pod pod-subpath-test-configmap-vftt container test-container-subpath-configmap-vftt: <nil>
STEP: delete the pod
Jan 13 09:03:23.012: INFO: Waiting for pod pod-subpath-test-configmap-vftt to disappear
Jan 13 09:03:23.014: INFO: Pod pod-subpath-test-configmap-vftt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vftt
Jan 13 09:03:23.014: INFO: Deleting pod "pod-subpath-test-configmap-vftt" in namespace "subpath-4713"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:03:23.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4713" for this suite.

• [SLOW TEST:24.134 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":299,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:03:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:03:23.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3014" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":300,"skipped":4704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:03:23.062: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 09:03:23.121: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Creating first CR 
Jan 13 09:03:25.667: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-13T09:03:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-13T09:03:25Z]] name:name1 resourceVersion:421047 uid:adae73dc-9168-4cc5-90d1-a9c3c6c82b63] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 13 09:03:35.673: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-13T09:03:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-13T09:03:35Z]] name:name2 resourceVersion:421099 uid:eb5a7324-160d-45ff-ac5d-2fc174d80939] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 13 09:03:45.694: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-13T09:03:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-13T09:03:45Z]] name:name1 resourceVersion:421135 uid:adae73dc-9168-4cc5-90d1-a9c3c6c82b63] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 13 09:03:55.701: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-13T09:03:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-13T09:03:55Z]] name:name2 resourceVersion:421170 uid:eb5a7324-160d-45ff-ac5d-2fc174d80939] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 13 09:04:05.707: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-13T09:03:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-13T09:03:45Z]] name:name1 resourceVersion:421205 uid:adae73dc-9168-4cc5-90d1-a9c3c6c82b63] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 13 09:04:15.713: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-13T09:03:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-13T09:03:55Z]] name:name2 resourceVersion:421240 uid:eb5a7324-160d-45ff-ac5d-2fc174d80939] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:04:26.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8977" for this suite.

• [SLOW TEST:63.170 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":301,"skipped":4732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:04:26.233: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0113 09:04:27.327509      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 13 09:05:29.349: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:29.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2526" for this suite.

• [SLOW TEST:63.122 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":302,"skipped":4767,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:29.355: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 13 09:05:29.389: INFO: Waiting up to 5m0s for pod "pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0" in namespace "emptydir-8918" to be "Succeeded or Failed"
Jan 13 09:05:29.392: INFO: Pod "pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320248ms
Jan 13 09:05:31.396: INFO: Pod "pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007167295s
STEP: Saw pod success
Jan 13 09:05:31.396: INFO: Pod "pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0" satisfied condition "Succeeded or Failed"
Jan 13 09:05:31.398: INFO: Trying to get logs from node worker-s001 pod pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0 container test-container: <nil>
STEP: delete the pod
Jan 13 09:05:31.430: INFO: Waiting for pod pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0 to disappear
Jan 13 09:05:31.433: INFO: Pod pod-3b4671de-5aba-4416-ba92-9b8a063ee7c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:31.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8918" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4778,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-8d4ec395-1039-4d65-b471-844025698d41
STEP: Creating a pod to test consume configMaps
Jan 13 09:05:31.481: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18" in namespace "projected-7018" to be "Succeeded or Failed"
Jan 13 09:05:31.485: INFO: Pod "pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059242ms
Jan 13 09:05:33.490: INFO: Pod "pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008480724s
STEP: Saw pod success
Jan 13 09:05:33.490: INFO: Pod "pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18" satisfied condition "Succeeded or Failed"
Jan 13 09:05:33.492: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 09:05:33.506: INFO: Waiting for pod pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18 to disappear
Jan 13 09:05:33.508: INFO: Pod pod-projected-configmaps-cce7533e-705b-4800-9813-248f588eac18 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:33.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7018" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":304,"skipped":4789,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:33.512: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:37.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1054" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":305,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:38.020: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-27fbe27e-5e16-4da2-80f8-d05c323f286b
STEP: Creating a pod to test consume configMaps
Jan 13 09:05:38.069: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3" in namespace "configmap-9847" to be "Succeeded or Failed"
Jan 13 09:05:38.075: INFO: Pod "pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97779ms
Jan 13 09:05:40.080: INFO: Pod "pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010767062s
Jan 13 09:05:42.085: INFO: Pod "pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015858408s
STEP: Saw pod success
Jan 13 09:05:42.085: INFO: Pod "pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3" satisfied condition "Succeeded or Failed"
Jan 13 09:05:42.087: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 09:05:42.106: INFO: Waiting for pod pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3 to disappear
Jan 13 09:05:42.108: INFO: Pod pod-configmaps-b9a6649d-11e7-473c-a042-01c533f6ecd3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:42.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9847" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":306,"skipped":4835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:42.117: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 13 09:05:42.144: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:45.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6794" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":307,"skipped":4902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:45.564: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-4688/secret-test-4cee315c-ead6-4fda-acc9-42b427ebf435
STEP: Creating a pod to test consume secrets
Jan 13 09:05:45.613: INFO: Waiting up to 5m0s for pod "pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f" in namespace "secrets-4688" to be "Succeeded or Failed"
Jan 13 09:05:45.616: INFO: Pod "pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.956922ms
Jan 13 09:05:47.620: INFO: Pod "pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006726842s
STEP: Saw pod success
Jan 13 09:05:47.620: INFO: Pod "pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f" satisfied condition "Succeeded or Failed"
Jan 13 09:05:47.622: INFO: Trying to get logs from node worker-s001 pod pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f container env-test: <nil>
STEP: delete the pod
Jan 13 09:05:47.638: INFO: Waiting for pod pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f to disappear
Jan 13 09:05:47.640: INFO: Pod pod-configmaps-c40cd33d-059c-45e7-afc8-04c31ba4581f no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4688" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":308,"skipped":4924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:47.646: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-91c69af1-182c-4860-87bb-8b9c01653fd3
STEP: Creating a pod to test consume configMaps
Jan 13 09:05:47.692: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63" in namespace "projected-7251" to be "Succeeded or Failed"
Jan 13 09:05:47.699: INFO: Pod "pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63": Phase="Pending", Reason="", readiness=false. Elapsed: 7.222891ms
Jan 13 09:05:49.702: INFO: Pod "pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010054274s
STEP: Saw pod success
Jan 13 09:05:49.702: INFO: Pod "pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63" satisfied condition "Succeeded or Failed"
Jan 13 09:05:49.704: INFO: Trying to get logs from node worker-s001 pod pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63 container agnhost-container: <nil>
STEP: delete the pod
Jan 13 09:05:49.731: INFO: Waiting for pod pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63 to disappear
Jan 13 09:05:49.732: INFO: Pod pod-projected-configmaps-c1ff3f56-15c6-4a29-81e4-11d59b2d3c63 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:49.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7251" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":309,"skipped":5025,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:49.737: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-556fcf73-f931-4690-8b2a-a786a48d90fa
STEP: Creating configMap with name cm-test-opt-upd-12f82f69-69be-430e-b0da-32bccefecc19
STEP: Creating the pod
Jan 13 09:05:49.787: INFO: The status of Pod pod-projected-configmaps-5af499e1-05ef-47ce-b3ec-ad3ff02d1ac5 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:05:51.793: INFO: The status of Pod pod-projected-configmaps-5af499e1-05ef-47ce-b3ec-ad3ff02d1ac5 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-556fcf73-f931-4690-8b2a-a786a48d90fa
STEP: Updating configmap cm-test-opt-upd-12f82f69-69be-430e-b0da-32bccefecc19
STEP: Creating configMap with name cm-test-opt-create-5daf427b-b60e-455b-94e1-598ef9e876cb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:05:53.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8993" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":310,"skipped":5029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:05:53.853: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 13 09:05:53.891: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422053 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:05:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:05:53.891: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422053 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:05:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 13 09:06:03.899: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422121 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:06:03.899: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422121 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 13 09:06:13.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422162 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:06:13.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422162 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 13 09:06:23.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422197 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:06:23.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3872  a0020b99-797c-4816-9ceb-c6821ceb6ca7 422197 0 2022-01-13 09:05:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 13 09:06:33.921: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3872  cffd51a5-299c-43ab-b9f4-e70f9d5ddd57 422232 0 2022-01-13 09:06:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:06:33.922: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3872  cffd51a5-299c-43ab-b9f4-e70f9d5ddd57 422232 0 2022-01-13 09:06:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 13 09:06:43.928: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3872  cffd51a5-299c-43ab-b9f4-e70f9d5ddd57 422267 0 2022-01-13 09:06:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 13 09:06:43.928: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3872  cffd51a5-299c-43ab-b9f4-e70f9d5ddd57 422267 0 2022-01-13 09:06:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-13 09:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:06:53.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3872" for this suite.

• [SLOW TEST:60.082 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":311,"skipped":5062,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:06:53.936: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-7c65e6ae-424c-41f3-ae4d-e2eaab2559dd in namespace container-probe-6599
Jan 13 09:06:57.981: INFO: Started pod busybox-7c65e6ae-424c-41f3-ae4d-e2eaab2559dd in namespace container-probe-6599
STEP: checking the pod's current state and verifying that restartCount is present
Jan 13 09:06:57.983: INFO: Initial restart count of pod busybox-7c65e6ae-424c-41f3-ae4d-e2eaab2559dd is 0
Jan 13 09:07:46.099: INFO: Restart count of pod container-probe-6599/busybox-7c65e6ae-424c-41f3-ae4d-e2eaab2559dd is now 1 (48.115656693s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:07:46.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6599" for this suite.

• [SLOW TEST:52.189 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":312,"skipped":5082,"failed":0}
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:07:46.125: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 13 09:07:49.195: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:07:49.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8830" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":313,"skipped":5082,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:07:49.228: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 13 09:07:49.278: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:07:51.282: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:07:53.281: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 13 09:07:54.294: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:07:55.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4918" for this suite.

• [SLOW TEST:6.084 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":314,"skipped":5090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:07:55.313: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-ec45cf2f-1561-4081-aef6-ae8ff83f4776
STEP: Creating secret with name secret-projected-all-test-volume-fb503007-8f31-428c-9468-782c8fe4912b
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 13 09:07:55.377: INFO: Waiting up to 5m0s for pod "projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39" in namespace "projected-1753" to be "Succeeded or Failed"
Jan 13 09:07:55.382: INFO: Pod "projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.635079ms
Jan 13 09:07:57.387: INFO: Pod "projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009581622s
Jan 13 09:07:59.394: INFO: Pod "projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01694433s
STEP: Saw pod success
Jan 13 09:07:59.394: INFO: Pod "projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39" satisfied condition "Succeeded or Failed"
Jan 13 09:07:59.396: INFO: Trying to get logs from node worker-s001 pod projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 13 09:07:59.420: INFO: Waiting for pod projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39 to disappear
Jan 13 09:07:59.423: INFO: Pod projected-volume-902c1d78-ac77-4440-bd6d-9222b1902b39 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:07:59.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1753" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":315,"skipped":5128,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:07:59.433: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a in namespace container-probe-4551
Jan 13 09:08:03.475: INFO: Started pod liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a in namespace container-probe-4551
STEP: checking the pod's current state and verifying that restartCount is present
Jan 13 09:08:03.477: INFO: Initial restart count of pod liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a is 0
Jan 13 09:08:21.521: INFO: Restart count of pod container-probe-4551/liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a is now 1 (18.044124275s elapsed)
Jan 13 09:08:41.572: INFO: Restart count of pod container-probe-4551/liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a is now 2 (38.095717871s elapsed)
Jan 13 09:09:01.621: INFO: Restart count of pod container-probe-4551/liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a is now 3 (58.144459548s elapsed)
Jan 13 09:09:21.672: INFO: Restart count of pod container-probe-4551/liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a is now 4 (1m18.195700751s elapsed)
Jan 13 09:10:33.618: INFO: Restart count of pod container-probe-4551/liveness-7e7759a1-6c9e-4ac8-ac12-0906b472885a is now 5 (2m30.14078748s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:10:33.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4551" for this suite.

• [SLOW TEST:154.212 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":316,"skipped":5130,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:10:33.645: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 13 09:12:34.211: INFO: Successfully updated pod "var-expansion-bb812507-1622-4600-8e2b-d4d839493a97"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 13 09:12:36.224: INFO: Deleting pod "var-expansion-bb812507-1622-4600-8e2b-d4d839493a97" in namespace "var-expansion-7526"
Jan 13 09:12:36.227: INFO: Wait up to 5m0s for pod "var-expansion-bb812507-1622-4600-8e2b-d4d839493a97" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:13:20.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7526" for this suite.

• [SLOW TEST:166.599 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":317,"skipped":5139,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:13:20.244: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 13 09:13:20.285: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:13:24.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8558" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":318,"skipped":5151,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:13:24.315: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 13 09:13:24.370: INFO: The status of Pod pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:13:26.374: INFO: The status of Pod pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 13 09:13:26.886: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6"
Jan 13 09:13:26.886: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6" in namespace "pods-8365" to be "terminated due to deadline exceeded"
Jan 13 09:13:26.894: INFO: Pod "pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6": Phase="Running", Reason="", readiness=true. Elapsed: 8.329206ms
Jan 13 09:13:28.897: INFO: Pod "pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011419987s
Jan 13 09:13:30.901: INFO: Pod "pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015033129s
Jan 13 09:13:30.901: INFO: Pod "pod-update-activedeadlineseconds-8983a045-dfb8-4eb9-aa57-c28687f013c6" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:13:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8365" for this suite.

• [SLOW TEST:6.592 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":319,"skipped":5153,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:13:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 13 09:13:30.947: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:14:30.984: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jan 13 09:14:30.998: INFO: Created pod: pod0-sched-preemption-low-priority
Jan 13 09:14:31.015: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:14:39.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1170" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.189 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":320,"skipped":5153,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:14:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 09:14:39.835: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 13 09:14:41.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 13 09:14:43.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63777662079, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 09:14:46.854: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:14:56.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6511" for this suite.
STEP: Destroying namespace "webhook-6511-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.911 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":321,"skipped":5160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:14:57.007: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 13 09:14:57.072: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:14:57.074: INFO: Number of nodes with available pods: 0
Jan 13 09:14:57.074: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 09:14:58.079: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:14:58.087: INFO: Number of nodes with available pods: 0
Jan 13 09:14:58.087: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 09:14:59.105: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:14:59.107: INFO: Number of nodes with available pods: 1
Jan 13 09:14:59.107: INFO: Node worker-s001 is running more than one daemon pod
Jan 13 09:15:00.079: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:15:00.081: INFO: Number of nodes with available pods: 2
Jan 13 09:15:00.081: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 13 09:15:00.097: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:15:00.100: INFO: Number of nodes with available pods: 1
Jan 13 09:15:00.100: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 09:15:01.105: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:15:01.107: INFO: Number of nodes with available pods: 1
Jan 13 09:15:01.107: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 09:15:02.104: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:15:02.108: INFO: Number of nodes with available pods: 1
Jan 13 09:15:02.108: INFO: Node worker-s002 is running more than one daemon pod
Jan 13 09:15:03.104: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 13 09:15:03.106: INFO: Number of nodes with available pods: 2
Jan 13 09:15:03.106: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4092, will wait for the garbage collector to delete the pods
Jan 13 09:15:03.168: INFO: Deleting DaemonSet.extensions daemon-set took: 2.778615ms
Jan 13 09:15:03.268: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.417907ms
Jan 13 09:15:08.672: INFO: Number of nodes with available pods: 0
Jan 13 09:15:08.672: INFO: Number of running nodes: 0, number of available pods: 0
Jan 13 09:15:08.674: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"424769"},"items":null}

Jan 13 09:15:08.675: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"424769"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:15:08.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4092" for this suite.

• [SLOW TEST:11.682 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":322,"skipped":5198,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:15:08.690: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jan 13 09:15:08.731: INFO: The status of Pod pod-hostip-7c5b6591-5c31-419f-a907-2e2b7f7956c4 is Pending, waiting for it to be Running (with Ready = true)
Jan 13 09:15:10.735: INFO: The status of Pod pod-hostip-7c5b6591-5c31-419f-a907-2e2b7f7956c4 is Running (Ready = true)
Jan 13 09:15:10.738: INFO: Pod pod-hostip-7c5b6591-5c31-419f-a907-2e2b7f7956c4 has hostIP: 192.168.100.59
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:15:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4275" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":323,"skipped":5215,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:15:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 13 09:15:11.208: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 13 09:15:14.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:15:14.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3826" for this suite.
STEP: Destroying namespace "webhook-3826-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":324,"skipped":5215,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:15:14.345: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 13 09:15:14.390: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:16:14.430: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:16:14.432: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 13 09:16:18.481: INFO: found a healthy node: worker-s001
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 09:16:30.558: INFO: pods created so far: [1 1 1]
Jan 13 09:16:30.558: INFO: length of pods created so far: 3
Jan 13 09:16:34.569: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:16:41.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4083" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:16:41.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3165" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:87.277 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":325,"skipped":5216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:16:41.622: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 13 09:16:41.650: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 13 09:16:41.656: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 13 09:16:41.656: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 13 09:16:41.670: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 13 09:16:41.670: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 13 09:16:41.679: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 13 09:16:41.679: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 13 09:16:48.716: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:16:48.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2726" for this suite.

• [SLOW TEST:7.108 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":326,"skipped":5254,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:16:48.731: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 13 09:16:50.779: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-821 PodName:pod-sharedvolume-d0df42a2-92fb-4c84-b871-34f7ac631b94 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 13 09:16:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
Jan 13 09:16:50.854: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:16:50.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-821" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":327,"skipped":5274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:16:50.867: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2665
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2665
STEP: Creating statefulset with conflicting port in namespace statefulset-2665
STEP: Waiting until pod test-pod will start running in namespace statefulset-2665
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2665
Jan 13 09:16:54.942: INFO: Observed stateful pod in namespace: statefulset-2665, name: ss-0, uid: a53819c9-0ce8-4d33-8f34-b3539673a254, status phase: Pending. Waiting for statefulset controller to delete.
Jan 13 09:16:55.288: INFO: Observed stateful pod in namespace: statefulset-2665, name: ss-0, uid: a53819c9-0ce8-4d33-8f34-b3539673a254, status phase: Failed. Waiting for statefulset controller to delete.
Jan 13 09:16:55.297: INFO: Observed stateful pod in namespace: statefulset-2665, name: ss-0, uid: a53819c9-0ce8-4d33-8f34-b3539673a254, status phase: Failed. Waiting for statefulset controller to delete.
Jan 13 09:16:55.307: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2665
STEP: Removing pod with conflicting port in namespace statefulset-2665
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2665 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jan 13 09:16:59.345: INFO: Deleting all statefulset in ns statefulset-2665
Jan 13 09:16:59.351: INFO: Scaling statefulset ss to 0
Jan 13 09:17:09.367: INFO: Waiting for statefulset status.replicas updated to 0
Jan 13 09:17:09.369: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:17:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2665" for this suite.

• [SLOW TEST:18.522 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":328,"skipped":5298,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:17:09.390: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-aee07ece-7d58-4521-89dd-702518ca8b25 in namespace container-probe-5372
Jan 13 09:17:13.470: INFO: Started pod busybox-aee07ece-7d58-4521-89dd-702518ca8b25 in namespace container-probe-5372
STEP: checking the pod's current state and verifying that restartCount is present
Jan 13 09:17:13.472: INFO: Initial restart count of pod busybox-aee07ece-7d58-4521-89dd-702518ca8b25 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:21:14.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5372" for this suite.

• [SLOW TEST:244.833 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":329,"skipped":5302,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:21:14.223: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 13 09:21:16.283: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:21:16.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7650" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":330,"skipped":5308,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:21:16.306: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-1469
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1469
STEP: Deleting pre-stop pod
Jan 13 09:21:25.388: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:21:25.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1469" for this suite.

• [SLOW TEST:9.118 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":331,"skipped":5318,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:21:25.424: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-00666305-7cba-497e-b013-63cc9b485c0b
STEP: Creating a pod to test consume secrets
Jan 13 09:21:25.460: INFO: Waiting up to 5m0s for pod "pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b" in namespace "secrets-8225" to be "Succeeded or Failed"
Jan 13 09:21:25.465: INFO: Pod "pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.485043ms
Jan 13 09:21:27.470: INFO: Pod "pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00922737s
STEP: Saw pod success
Jan 13 09:21:27.470: INFO: Pod "pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b" satisfied condition "Succeeded or Failed"
Jan 13 09:21:27.472: INFO: Trying to get logs from node worker-s001 pod pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b container secret-env-test: <nil>
STEP: delete the pod
Jan 13 09:21:27.492: INFO: Waiting for pod pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b to disappear
Jan 13 09:21:27.495: INFO: Pod pod-secrets-08745031-7b06-4745-8887-3d1896f3f31b no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:21:27.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8225" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":332,"skipped":5325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:21:27.500: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:21:34.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9393" for this suite.

• [SLOW TEST:7.075 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":333,"skipped":5358,"failed":0}
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:21:34.575: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jan 13 09:21:34.614: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 13 09:22:34.645: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 09:22:34.647: INFO: Starting informer...
STEP: Starting pod...
Jan 13 09:22:34.859: INFO: Pod is running on worker-s001. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 13 09:22:34.874: INFO: Pod wasn't evicted. Proceeding
Jan 13 09:22:34.874: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 13 09:23:49.894: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:23:49.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9868" for this suite.

• [SLOW TEST:135.327 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":334,"skipped":5366,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:23:49.902: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-33459a7d-c10a-4d76-aa66-3b29d99d99a5
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:23:49.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1069" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":335,"skipped":5374,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:23:49.942: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 13 09:23:49.968: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 13 09:23:54.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-3010 --namespace=crd-publish-openapi-3010 create -f -'
Jan 13 09:23:55.211: INFO: stderr: ""
Jan 13 09:23:55.211: INFO: stdout: "e2e-test-crd-publish-openapi-5168-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 13 09:23:55.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-3010 --namespace=crd-publish-openapi-3010 delete e2e-test-crd-publish-openapi-5168-crds test-cr'
Jan 13 09:23:55.276: INFO: stderr: ""
Jan 13 09:23:55.276: INFO: stdout: "e2e-test-crd-publish-openapi-5168-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 13 09:23:55.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-3010 --namespace=crd-publish-openapi-3010 apply -f -'
Jan 13 09:23:55.511: INFO: stderr: ""
Jan 13 09:23:55.511: INFO: stdout: "e2e-test-crd-publish-openapi-5168-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 13 09:23:55.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-3010 --namespace=crd-publish-openapi-3010 delete e2e-test-crd-publish-openapi-5168-crds test-cr'
Jan 13 09:23:55.566: INFO: stderr: ""
Jan 13 09:23:55.566: INFO: stdout: "e2e-test-crd-publish-openapi-5168-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 13 09:23:55.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-160527841 --namespace=crd-publish-openapi-3010 explain e2e-test-crd-publish-openapi-5168-crds'
Jan 13 09:23:55.774: INFO: stderr: ""
Jan 13 09:23:55.774: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5168-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:24:00.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3010" for this suite.

• [SLOW TEST:10.673 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":336,"skipped":5381,"failed":0}
SSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:24:00.615: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 13 09:24:00.666: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 13 09:24:00.677: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:24:00.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1960" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":337,"skipped":5384,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:24:00.700: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 13 09:24:00.789: INFO: Waiting up to 5m0s for pod "pod-a96174b5-3a8b-4705-8f18-22434e0d04ff" in namespace "emptydir-4221" to be "Succeeded or Failed"
Jan 13 09:24:00.796: INFO: Pod "pod-a96174b5-3a8b-4705-8f18-22434e0d04ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.010762ms
Jan 13 09:24:02.811: INFO: Pod "pod-a96174b5-3a8b-4705-8f18-22434e0d04ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0226388s
STEP: Saw pod success
Jan 13 09:24:02.811: INFO: Pod "pod-a96174b5-3a8b-4705-8f18-22434e0d04ff" satisfied condition "Succeeded or Failed"
Jan 13 09:24:02.816: INFO: Trying to get logs from node worker-s001 pod pod-a96174b5-3a8b-4705-8f18-22434e0d04ff container test-container: <nil>
STEP: delete the pod
Jan 13 09:24:02.836: INFO: Waiting for pod pod-a96174b5-3a8b-4705-8f18-22434e0d04ff to disappear
Jan 13 09:24:02.841: INFO: Pod pod-a96174b5-3a8b-4705-8f18-22434e0d04ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:24:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4221" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":338,"skipped":5384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 13 09:24:02.846: INFO: >>> kubeConfig: /tmp/kubeconfig-160527841
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0113 09:24:03.904895      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 13 09:25:05.919: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 13 09:25:05.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9764" for this suite.

• [SLOW TEST:63.089 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":339,"skipped":5423,"failed":0}
SSSSSSSSJan 13 09:25:05.935: INFO: Running AfterSuite actions on all nodes
Jan 13 09:25:05.935: INFO: Running AfterSuite actions on node 1
Jan 13 09:25:05.935: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5431,"failed":0}

Ran 339 of 5770 Specs in 6689.817 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5431 Skipped
PASS

Ginkgo ran 1 suite in 1h51m30.963226571s
Test Suite Passed
