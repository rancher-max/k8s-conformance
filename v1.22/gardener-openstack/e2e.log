Conformance test: not doing test setup.
I1019 15:53:31.791827    4339 e2e.go:129] Starting e2e run "53c206ff-763e-4b70-8a0f-781602aa468c" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1634658811 - Will randomize all specs
Will run 346 of 6432 specs

Oct 19 15:53:33.496: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 15:53:33.497: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Oct 19 15:53:33.513: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Oct 19 15:53:33.548: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 19 15:53:33.548: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Oct 19 15:53:33.548: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct 19 15:53:33.558: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Oct 19 15:53:33.558: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Oct 19 15:53:33.558: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Oct 19 15:53:33.558: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Oct 19 15:53:33.558: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Oct 19 15:53:33.558: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Oct 19 15:53:33.558: INFO: e2e test version: v1.22.2
Oct 19 15:53:33.560: INFO: kube-apiserver version: v1.22.2
Oct 19 15:53:33.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 15:53:33.564: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:53:33.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
W1019 15:53:33.590035    4339 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Oct 19 15:53:33.590: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Oct 19 15:53:33.603: INFO: PSP annotation exists on dry run pod: "extensions.gardener.cloud.provider-openstack.csi-driver-node"; assuming PodSecurityPolicy is enabled
W1019 15:53:33.605311    4339 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W1019 15:53:33.608065    4339 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Oct 19 15:53:33.616: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb
Oct 19 15:53:33.733: INFO: Pod name my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb: Found 0 pods out of 1
Oct 19 15:53:38.738: INFO: Pod name my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb: Found 1 pods out of 1
Oct 19 15:53:38.738: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb" are running
Oct 19 15:53:38.740: INFO: Pod "my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb-smmmk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 15:53:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 15:53:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 15:53:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 15:53:33 +0000 UTC Reason: Message:}])
Oct 19 15:53:38.740: INFO: Trying to dial the pod
Oct 19 15:53:43.803: INFO: Controller my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb: Got expected result from replica 1 [my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb-smmmk]: "my-hostname-basic-3f782f61-327e-4cab-a2d7-9159a14b67bb-smmmk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:53:43.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1412" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":1,"skipped":21,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:53:43.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 19 15:53:45.969: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:53:45.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1406" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":76,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:53:45.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9803
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9803
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9803
Oct 19 15:53:46.132: INFO: Found 0 stateful pods, waiting for 1
Oct 19 15:53:56.140: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Oct 19 15:53:56.143: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 15:53:56.339: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 15:53:56.339: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 15:53:56.339: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 15:53:56.343: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 19 15:54:06.346: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 15:54:06.347: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 15:54:06.356: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999982s
Oct 19 15:54:07.363: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99775112s
Oct 19 15:54:08.367: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990786238s
Oct 19 15:54:09.370: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987569048s
Oct 19 15:54:10.373: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984346936s
Oct 19 15:54:11.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981293184s
Oct 19 15:54:12.388: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969965209s
Oct 19 15:54:13.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966419733s
Oct 19 15:54:14.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963360483s
Oct 19 15:54:15.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.695047ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9803
Oct 19 15:54:16.401: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 15:54:16.591: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 15:54:16.591: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 15:54:16.591: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 15:54:16.594: INFO: Found 1 stateful pods, waiting for 3
Oct 19 15:54:26.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 15:54:26.601: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 15:54:26.601: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Oct 19 15:54:26.608: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 15:54:26.879: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 15:54:26.879: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 15:54:26.879: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 15:54:26.879: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 15:54:27.079: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 15:54:27.079: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 15:54:27.079: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 15:54:27.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 15:54:27.288: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 15:54:27.288: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 15:54:27.288: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 15:54:27.288: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 15:54:27.290: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Oct 19 15:54:37.296: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 15:54:37.296: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 15:54:37.296: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 15:54:37.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999834s
Oct 19 15:54:38.307: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997374977s
Oct 19 15:54:39.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993509317s
Oct 19 15:54:40.315: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988928161s
Oct 19 15:54:41.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985350501s
Oct 19 15:54:42.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981753589s
Oct 19 15:54:43.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976953282s
Oct 19 15:54:44.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971066548s
Oct 19 15:54:45.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967415074s
Oct 19 15:54:46.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.61231ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9803
Oct 19 15:54:47.346: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 15:54:47.663: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 15:54:47.663: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 15:54:47.663: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 15:54:47.663: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 15:54:47.873: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 15:54:47.873: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 15:54:47.873: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 15:54:47.873: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9803 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 15:54:48.138: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 15:54:48.139: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 15:54:48.139: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 15:54:48.139: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 15:54:58.150: INFO: Deleting all statefulset in ns statefulset-9803
Oct 19 15:54:58.153: INFO: Scaling statefulset ss to 0
Oct 19 15:54:58.163: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 15:54:58.165: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:54:58.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9803" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":3,"skipped":80,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:54:58.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Oct 19 15:54:58.319: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Oct 19 15:54:58.319: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 create -f -'
Oct 19 15:54:58.470: INFO: stderr: ""
Oct 19 15:54:58.470: INFO: stdout: "service/agnhost-replica created\n"
Oct 19 15:54:58.470: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Oct 19 15:54:58.470: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 create -f -'
Oct 19 15:54:58.589: INFO: stderr: ""
Oct 19 15:54:58.589: INFO: stdout: "service/agnhost-primary created\n"
Oct 19 15:54:58.589: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct 19 15:54:58.589: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 create -f -'
Oct 19 15:54:58.705: INFO: stderr: ""
Oct 19 15:54:58.705: INFO: stdout: "service/frontend created\n"
Oct 19 15:54:58.705: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Oct 19 15:54:58.705: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 create -f -'
Oct 19 15:54:58.826: INFO: stderr: ""
Oct 19 15:54:58.826: INFO: stdout: "deployment.apps/frontend created\n"
Oct 19 15:54:58.826: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 19 15:54:58.826: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 create -f -'
Oct 19 15:54:58.947: INFO: stderr: ""
Oct 19 15:54:58.947: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Oct 19 15:54:58.947: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 19 15:54:58.947: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 create -f -'
Oct 19 15:54:59.059: INFO: stderr: ""
Oct 19 15:54:59.059: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Oct 19 15:54:59.059: INFO: Waiting for all frontend pods to be Running.
Oct 19 15:55:04.109: INFO: Waiting for frontend to serve content.
Oct 19 15:55:04.168: INFO: Trying to add a new entry to the guestbook.
Oct 19 15:55:04.225: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Oct 19 15:55:04.274: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 delete --grace-period=0 --force -f -'
Oct 19 15:55:04.327: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:55:04.327: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Oct 19 15:55:04.327: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 delete --grace-period=0 --force -f -'
Oct 19 15:55:04.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:55:04.379: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 19 15:55:04.379: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 delete --grace-period=0 --force -f -'
Oct 19 15:55:04.429: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:55:04.429: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 19 15:55:04.429: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 delete --grace-period=0 --force -f -'
Oct 19 15:55:04.477: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:55:04.477: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 19 15:55:04.477: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 delete --grace-period=0 --force -f -'
Oct 19 15:55:04.524: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:55:04.524: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 19 15:55:04.524: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6419 delete --grace-period=0 --force -f -'
Oct 19 15:55:04.571: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:55:04.571: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:04.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6419" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":4,"skipped":85,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:04.579: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 19 15:55:04.724: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:55:06.727: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 19 15:55:06.739: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:55:08.743: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Oct 19 15:55:08.750: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 19 15:55:08.752: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 19 15:55:10.753: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 19 15:55:10.756: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 19 15:55:12.752: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 19 15:55:12.756: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:12.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1646" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:12.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1322
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Oct 19 15:55:12.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:29.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1322" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":6,"skipped":162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:29.610: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Oct 19 15:55:29.750: INFO: created test-podtemplate-1
Oct 19 15:55:29.753: INFO: created test-podtemplate-2
Oct 19 15:55:29.755: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Oct 19 15:55:29.757: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Oct 19 15:55:29.766: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:29.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-781" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":7,"skipped":188,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:29.773: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Oct 19 15:55:29.912: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.912: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.914: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.914: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.919: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.920: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.943: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:29.943: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 19 15:55:30.814: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Oct 19 15:55:30.814: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Oct 19 15:55:31.248: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Oct 19 15:55:31.254: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 0
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.256: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.273: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.273: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.308: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.308: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:31.330: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:31.330: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:32.819: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:32.819: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:32.833: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
STEP: listing Deployments
Oct 19 15:55:32.836: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Oct 19 15:55:32.842: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Oct 19 15:55:32.850: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:32.850: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:32.881: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:32.885: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:32.897: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:33.834: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:33.840: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:33.842: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:33.846: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:33.850: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 19 15:55:35.249: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 1
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:35.271: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:35.272: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 2
Oct 19 15:55:35.272: INFO: observed Deployment test-deployment in namespace deployment-7363 with ReadyReplicas 3
STEP: deleting the Deployment
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.278: INFO: observed event type MODIFIED
Oct 19 15:55:35.279: INFO: observed event type MODIFIED
Oct 19 15:55:35.279: INFO: observed event type MODIFIED
Oct 19 15:55:35.279: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 15:55:35.282: INFO: Log out all the ReplicaSets if there is no deployment created
Oct 19 15:55:35.284: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-7363  18470393-2189-47e6-9698-e725a865784c 5300 4 2021-10-19 15:55:31 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment c431f6ab-e450-4787-9281-85cf2dd08c97 0xc005b28607 0xc005b28608}] []  [{kube-controller-manager Update apps/v1 2021-10-19 15:55:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c431f6ab-e450-4787-9281-85cf2dd08c97\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:55:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005b286a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Oct 19 15:55:35.288: INFO: pod: "test-deployment-56c98d85f9-7mtlp":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-7mtlp test-deployment-56c98d85f9- deployment-7363  c1349bac-e1d1-4bf6-b157-48deb6c9a5d8 5296 0 2021-10-19 15:55:32 +0000 UTC 2021-10-19 15:55:34 +0000 UTC 0xc005aa82d0 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/podIP:100.96.1.19/32 cni.projectcalico.org/podIPs:100.96.1.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 18470393-2189-47e6-9698-e725a865784c 0xc005aa8397 0xc005aa8398}] []  [{kube-controller-manager Update v1 2021-10-19 15:55:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18470393-2189-47e6-9698-e725a865784c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 15:55:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 15:55:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xt54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xt54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.19,StartTime:2021-10-19 15:55:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 15:55:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:containerd://f0aa1f270a85260ab53405413966e1719714464c78059c6f8f5bfc01dc7972b1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Oct 19 15:55:35.288: INFO: pod: "test-deployment-56c98d85f9-s759x":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-s759x test-deployment-56c98d85f9- deployment-7363  72dcbef0-7b58-4d59-87f7-472e07a04e0e 5298 0 2021-10-19 15:55:31 +0000 UTC 2021-10-19 15:55:36 +0000 UTC 0xc005aa85d0 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/podIP:100.96.0.18/32 cni.projectcalico.org/podIPs:100.96.0.18/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 18470393-2189-47e6-9698-e725a865784c 0xc005aa8637 0xc005aa8638}] []  [{calico Update v1 2021-10-19 15:55:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2021-10-19 15:55:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18470393-2189-47e6-9698-e725a865784c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 15:55:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p662w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p662w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.18,StartTime:2021-10-19 15:55:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 15:55:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:containerd://186d71be0332a8af5935e81976a51b8b3f75ed0f35b2ad137962d1dc70240d18,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Oct 19 15:55:35.288: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-7363  3e8bc6ef-a2ab-45cc-b29d-6fb2c352ca75 5219 3 2021-10-19 15:55:29 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment c431f6ab-e450-4787-9281-85cf2dd08c97 0xc005b28717 0xc005b28718}] []  [{kube-controller-manager Update apps/v1 2021-10-19 15:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c431f6ab-e450-4787-9281-85cf2dd08c97\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:55:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005b287b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Oct 19 15:55:35.293: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-7363  3740f505-b95d-4b55-992e-587651c5cdea 5294 2 2021-10-19 15:55:32 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment c431f6ab-e450-4787-9281-85cf2dd08c97 0xc005b28827 0xc005b28828}] []  [{kube-controller-manager Update apps/v1 2021-10-19 15:55:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c431f6ab-e450-4787-9281-85cf2dd08c97\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:55:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005b288e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Oct 19 15:55:35.295: INFO: pod: "test-deployment-d4dfddfbf-8drdl":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-8drdl test-deployment-d4dfddfbf- deployment-7363  723a40c2-509e-454a-b4e3-5de14473e805 5293 0 2021-10-19 15:55:33 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/podIP:100.96.1.20/32 cni.projectcalico.org/podIPs:100.96.1.20/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 3740f505-b95d-4b55-992e-587651c5cdea 0xc005ad9e97 0xc005ad9e98}] []  [{kube-controller-manager Update v1 2021-10-19 15:55:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3740f505-b95d-4b55-992e-587651c5cdea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 15:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 15:55:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6lzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6lzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.20,StartTime:2021-10-19 15:55:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 15:55:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://443ece28bcb5c862ff860b0958037cf8008e711a55f2dcbe6c7b42de86ec2094,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Oct 19 15:55:35.295: INFO: pod: "test-deployment-d4dfddfbf-8grd6":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-8grd6 test-deployment-d4dfddfbf- deployment-7363  98afce61-5feb-4324-99c6-d325d9efcb17 5254 0 2021-10-19 15:55:32 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/podIP:100.96.0.19/32 cni.projectcalico.org/podIPs:100.96.0.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 3740f505-b95d-4b55-992e-587651c5cdea 0xc005b600e7 0xc005b600e8}] []  [{kube-controller-manager Update v1 2021-10-19 15:55:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3740f505-b95d-4b55-992e-587651c5cdea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 15:55:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 15:55:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qhph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qhph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:55:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.19,StartTime:2021-10-19 15:55:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 15:55:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://63c517fa8bffc112a943ca0e02f5532b1451e25f58aec5b6a847e47b1bd1e303,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:35.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7363" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":8,"skipped":189,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:35.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4069
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 15:55:35.432: INFO: Creating pod...
Oct 19 15:55:35.442: INFO: Pod Quantity: 1 Status: Pending
Oct 19 15:55:36.446: INFO: Pod Quantity: 1 Status: Pending
Oct 19 15:55:37.445: INFO: Pod Status: Running
Oct 19 15:55:37.445: INFO: Creating service...
Oct 19 15:55:37.451: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/DELETE
Oct 19 15:55:37.546: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Oct 19 15:55:37.546: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/GET
Oct 19 15:55:37.552: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Oct 19 15:55:37.552: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/HEAD
Oct 19 15:55:37.559: INFO: http.Client request:HEAD | StatusCode:200
Oct 19 15:55:37.559: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/OPTIONS
Oct 19 15:55:37.602: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Oct 19 15:55:37.602: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/PATCH
Oct 19 15:55:37.606: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Oct 19 15:55:37.606: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/POST
Oct 19 15:55:37.610: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Oct 19 15:55:37.610: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/pods/agnhost/proxy/some/path/with/PUT
Oct 19 15:55:37.617: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Oct 19 15:55:37.618: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/DELETE
Oct 19 15:55:37.622: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Oct 19 15:55:37.622: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/GET
Oct 19 15:55:37.627: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Oct 19 15:55:37.627: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/HEAD
Oct 19 15:55:37.632: INFO: http.Client request:HEAD | StatusCode:200
Oct 19 15:55:37.632: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/OPTIONS
Oct 19 15:55:37.636: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Oct 19 15:55:37.636: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/PATCH
Oct 19 15:55:37.641: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Oct 19 15:55:37.641: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/POST
Oct 19 15:55:37.645: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Oct 19 15:55:37.645: INFO: Starting http.Client for https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-4069/services/test-service/proxy/some/path/with/PUT
Oct 19 15:55:37.650: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:37.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4069" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":9,"skipped":195,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:37.657: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-5419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 15:55:38.071: INFO: Checking APIGroup: apiregistration.k8s.io
Oct 19 15:55:38.073: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Oct 19 15:55:38.073: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Oct 19 15:55:38.073: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Oct 19 15:55:38.073: INFO: Checking APIGroup: apps
Oct 19 15:55:38.074: INFO: PreferredVersion.GroupVersion: apps/v1
Oct 19 15:55:38.074: INFO: Versions found [{apps/v1 v1}]
Oct 19 15:55:38.074: INFO: apps/v1 matches apps/v1
Oct 19 15:55:38.074: INFO: Checking APIGroup: events.k8s.io
Oct 19 15:55:38.076: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Oct 19 15:55:38.076: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.076: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Oct 19 15:55:38.076: INFO: Checking APIGroup: authentication.k8s.io
Oct 19 15:55:38.077: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Oct 19 15:55:38.077: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Oct 19 15:55:38.077: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Oct 19 15:55:38.077: INFO: Checking APIGroup: authorization.k8s.io
Oct 19 15:55:38.078: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Oct 19 15:55:38.078: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Oct 19 15:55:38.078: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Oct 19 15:55:38.078: INFO: Checking APIGroup: autoscaling
Oct 19 15:55:38.080: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Oct 19 15:55:38.080: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Oct 19 15:55:38.080: INFO: autoscaling/v1 matches autoscaling/v1
Oct 19 15:55:38.080: INFO: Checking APIGroup: batch
Oct 19 15:55:38.081: INFO: PreferredVersion.GroupVersion: batch/v1
Oct 19 15:55:38.081: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Oct 19 15:55:38.081: INFO: batch/v1 matches batch/v1
Oct 19 15:55:38.081: INFO: Checking APIGroup: certificates.k8s.io
Oct 19 15:55:38.082: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Oct 19 15:55:38.082: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Oct 19 15:55:38.082: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Oct 19 15:55:38.082: INFO: Checking APIGroup: networking.k8s.io
Oct 19 15:55:38.084: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Oct 19 15:55:38.084: INFO: Versions found [{networking.k8s.io/v1 v1}]
Oct 19 15:55:38.084: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Oct 19 15:55:38.084: INFO: Checking APIGroup: policy
Oct 19 15:55:38.085: INFO: PreferredVersion.GroupVersion: policy/v1
Oct 19 15:55:38.085: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Oct 19 15:55:38.085: INFO: policy/v1 matches policy/v1
Oct 19 15:55:38.085: INFO: Checking APIGroup: rbac.authorization.k8s.io
Oct 19 15:55:38.087: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Oct 19 15:55:38.087: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Oct 19 15:55:38.087: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Oct 19 15:55:38.087: INFO: Checking APIGroup: storage.k8s.io
Oct 19 15:55:38.088: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Oct 19 15:55:38.088: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.088: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Oct 19 15:55:38.088: INFO: Checking APIGroup: admissionregistration.k8s.io
Oct 19 15:55:38.089: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Oct 19 15:55:38.089: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Oct 19 15:55:38.089: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Oct 19 15:55:38.089: INFO: Checking APIGroup: apiextensions.k8s.io
Oct 19 15:55:38.091: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Oct 19 15:55:38.091: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Oct 19 15:55:38.091: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Oct 19 15:55:38.091: INFO: Checking APIGroup: scheduling.k8s.io
Oct 19 15:55:38.092: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Oct 19 15:55:38.092: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Oct 19 15:55:38.092: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Oct 19 15:55:38.092: INFO: Checking APIGroup: coordination.k8s.io
Oct 19 15:55:38.093: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Oct 19 15:55:38.093: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Oct 19 15:55:38.093: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Oct 19 15:55:38.093: INFO: Checking APIGroup: node.k8s.io
Oct 19 15:55:38.095: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Oct 19 15:55:38.095: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.095: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Oct 19 15:55:38.095: INFO: Checking APIGroup: discovery.k8s.io
Oct 19 15:55:38.096: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Oct 19 15:55:38.096: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.096: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Oct 19 15:55:38.096: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Oct 19 15:55:38.097: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Oct 19 15:55:38.097: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.097: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Oct 19 15:55:38.097: INFO: Checking APIGroup: autoscaling.k8s.io
Oct 19 15:55:38.099: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Oct 19 15:55:38.099: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Oct 19 15:55:38.099: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Oct 19 15:55:38.099: INFO: Checking APIGroup: crd.projectcalico.org
Oct 19 15:55:38.100: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Oct 19 15:55:38.100: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Oct 19 15:55:38.100: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Oct 19 15:55:38.100: INFO: Checking APIGroup: cert.gardener.cloud
Oct 19 15:55:38.101: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Oct 19 15:55:38.101: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Oct 19 15:55:38.101: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Oct 19 15:55:38.101: INFO: Checking APIGroup: dns.gardener.cloud
Oct 19 15:55:38.103: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Oct 19 15:55:38.103: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Oct 19 15:55:38.103: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Oct 19 15:55:38.103: INFO: Checking APIGroup: snapshot.storage.k8s.io
Oct 19 15:55:38.104: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Oct 19 15:55:38.104: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.104: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Oct 19 15:55:38.104: INFO: Checking APIGroup: metrics.k8s.io
Oct 19 15:55:38.105: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Oct 19 15:55:38.105: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Oct 19 15:55:38.105: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:38.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5419" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":10,"skipped":199,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:38.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 19 15:55:38.281: INFO: Number of nodes with available pods: 0
Oct 19 15:55:38.281: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:55:39.288: INFO: Number of nodes with available pods: 1
Oct 19 15:55:39.288: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:55:40.290: INFO: Number of nodes with available pods: 2
Oct 19 15:55:40.290: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Getting /status
Oct 19 15:55:40.297: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Oct 19 15:55:40.303: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Oct 19 15:55:40.305: INFO: Observed &DaemonSet event: ADDED
Oct 19 15:55:40.305: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.306: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.306: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.306: INFO: Found daemon set daemon-set in namespace daemonsets-8932 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Oct 19 15:55:40.306: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Oct 19 15:55:40.312: INFO: Observed &DaemonSet event: ADDED
Oct 19 15:55:40.312: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.312: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.312: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.312: INFO: Observed daemon set daemon-set in namespace daemonsets-8932 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Oct 19 15:55:40.312: INFO: Observed &DaemonSet event: MODIFIED
Oct 19 15:55:40.312: INFO: Found daemon set daemon-set in namespace daemonsets-8932 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Oct 19 15:55:40.312: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8932, will wait for the garbage collector to delete the pods
Oct 19 15:55:40.372: INFO: Deleting DaemonSet.extensions daemon-set took: 4.479159ms
Oct 19 15:55:40.472: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.563608ms
Oct 19 15:55:42.875: INFO: Number of nodes with available pods: 0
Oct 19 15:55:42.875: INFO: Number of running nodes: 0, number of available pods: 0
Oct 19 15:55:42.877: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5432"},"items":null}

Oct 19 15:55:42.879: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5432"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:42.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8932" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":11,"skipped":207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:42.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8794.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8794.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 15:55:51.232: INFO: DNS probes using dns-8794/dns-test-382cc2dd-cb9c-4702-b1ea-1bcd28d3f813 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:51.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8794" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":12,"skipped":233,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:51.244: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-7f87c1f6-93af-4184-ba4c-c127466f60a7
STEP: Creating a pod to test consume configMaps
Oct 19 15:55:51.390: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0" in namespace "projected-1293" to be "Succeeded or Failed"
Oct 19 15:55:51.393: INFO: Pod "pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367651ms
Oct 19 15:55:53.397: INFO: Pod "pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007024933s
STEP: Saw pod success
Oct 19 15:55:53.397: INFO: Pod "pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0" satisfied condition "Succeeded or Failed"
Oct 19 15:55:53.399: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 19 15:55:53.414: INFO: Waiting for pod pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0 to disappear
Oct 19 15:55:53.416: INFO: Pod pod-projected-configmaps-2ed76d70-6deb-411a-9ae6-6ffec6c156f0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1293" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:53.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 15:55:53.562: INFO: The status of Pod busybox-scheduling-b88ba968-b099-4ece-971d-39af4fee650c is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:55:55.565: INFO: The status of Pod busybox-scheduling-b88ba968-b099-4ece-971d-39af4fee650c is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:55.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5110" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":267,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:55.584: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 19 15:55:55.722: INFO: Waiting up to 5m0s for pod "pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7" in namespace "emptydir-2940" to be "Succeeded or Failed"
Oct 19 15:55:55.724: INFO: Pod "pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159812ms
Oct 19 15:55:57.727: INFO: Pod "pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005287081s
STEP: Saw pod success
Oct 19 15:55:57.727: INFO: Pod "pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7" satisfied condition "Succeeded or Failed"
Oct 19 15:55:57.729: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7 container test-container: <nil>
STEP: delete the pod
Oct 19 15:55:57.740: INFO: Waiting for pod pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7 to disappear
Oct 19 15:55:57.742: INFO: Pod pod-e09ead9c-53e9-4cc2-b037-6d205dfb61f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:55:57.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2940" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":280,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:55:57.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 19 15:55:57.898: INFO: Number of nodes with available pods: 0
Oct 19 15:55:57.898: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:55:58.905: INFO: Number of nodes with available pods: 0
Oct 19 15:55:58.905: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:55:59.905: INFO: Number of nodes with available pods: 2
Oct 19 15:55:59.905: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Oct 19 15:55:59.921: INFO: Number of nodes with available pods: 1
Oct 19 15:55:59.921: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:56:00.928: INFO: Number of nodes with available pods: 2
Oct 19 15:56:00.928: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5404, will wait for the garbage collector to delete the pods
Oct 19 15:56:00.989: INFO: Deleting DaemonSet.extensions daemon-set took: 3.112601ms
Oct 19 15:56:01.089: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.437043ms
Oct 19 15:56:04.092: INFO: Number of nodes with available pods: 0
Oct 19 15:56:04.092: INFO: Number of running nodes: 0, number of available pods: 0
Oct 19 15:56:04.095: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5673"},"items":null}

Oct 19 15:56:04.097: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5673"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:56:04.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5404" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":16,"skipped":286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:56:04.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 19 15:56:04.329: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6843 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Oct 19 15:56:04.401: INFO: stderr: ""
Oct 19 15:56:04.401: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Oct 19 15:56:04.401: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6843 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Oct 19 15:56:04.536: INFO: stderr: ""
Oct 19 15:56:04.536: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 19 15:56:04.554: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6843 delete pods e2e-test-httpd-pod'
Oct 19 15:56:06.953: INFO: stderr: ""
Oct 19 15:56:06.953: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:56:06.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6843" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":17,"skipped":315,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:56:06.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:56:07.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3110" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":318,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:56:07.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 15:56:07.275: INFO: Pod name rollover-pod: Found 0 pods out of 1
Oct 19 15:56:12.566: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 19 15:56:12.566: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Oct 19 15:56:14.569: INFO: Creating deployment "test-rollover-deployment"
Oct 19 15:56:14.576: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Oct 19 15:56:16.581: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Oct 19 15:56:16.587: INFO: Ensure that both replica sets have 1 created replica
Oct 19 15:56:16.596: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Oct 19 15:56:16.602: INFO: Updating deployment test-rollover-deployment
Oct 19 15:56:16.602: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Oct 19 15:56:18.609: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Oct 19 15:56:18.613: INFO: Make sure deployment "test-rollover-deployment" is complete
Oct 19 15:56:18.618: INFO: all replica sets need to contain the pod-template-hash label
Oct 19 15:56:18.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255777, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 15:56:20.625: INFO: all replica sets need to contain the pod-template-hash label
Oct 19 15:56:20.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255777, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 15:56:22.624: INFO: all replica sets need to contain the pod-template-hash label
Oct 19 15:56:22.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255777, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 15:56:24.626: INFO: all replica sets need to contain the pod-template-hash label
Oct 19 15:56:24.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255777, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 15:56:26.627: INFO: all replica sets need to contain the pod-template-hash label
Oct 19 15:56:26.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255777, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770255774, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 15:56:28.624: INFO: 
Oct 19 15:56:28.624: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 15:56:28.631: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9142  9947bf35-6e66-4e21-99e7-460159ab01d1 5903 2 2021-10-19 15:56:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-10-19 15:56:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ab4e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-10-19 15:56:14 +0000 UTC,LastTransitionTime:2021-10-19 15:56:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-10-19 15:56:28 +0000 UTC,LastTransitionTime:2021-10-19 15:56:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 19 15:56:28.634: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-9142  d7d7da6b-65e9-4df1-80fd-e62937112741 5896 2 2021-10-19 15:56:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9947bf35-6e66-4e21-99e7-460159ab01d1 0xc002d2eeb0 0xc002d2eeb1}] []  [{kube-controller-manager Update apps/v1 2021-10-19 15:56:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9947bf35-6e66-4e21-99e7-460159ab01d1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d2ef48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 19 15:56:28.634: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Oct 19 15:56:28.634: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9142  d2e46c69-00d3-448e-8ff6-cecb9208e43e 5902 2 2021-10-19 15:56:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9947bf35-6e66-4e21-99e7-460159ab01d1 0xc002d2ec87 0xc002d2ec88}] []  [{e2e.test Update apps/v1 2021-10-19 15:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9947bf35-6e66-4e21-99e7-460159ab01d1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d2ed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 19 15:56:28.634: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9142  4d4f9e8b-5f3a-4b06-9a76-68953d517341 5811 2 2021-10-19 15:56:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9947bf35-6e66-4e21-99e7-460159ab01d1 0xc002d2eda7 0xc002d2eda8}] []  [{kube-controller-manager Update apps/v1 2021-10-19 15:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9947bf35-6e66-4e21-99e7-460159ab01d1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 15:56:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d2ee58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 19 15:56:28.636: INFO: Pod "test-rollover-deployment-98c5f4599-b4l4x" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-b4l4x test-rollover-deployment-98c5f4599- deployment-9142  0db4a673-2b3f-4578-aef1-7d09fd45e84d 5827 0 2021-10-19 15:56:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:100.96.0.31/32 cni.projectcalico.org/podIPs:100.96.0.31/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 d7d7da6b-65e9-4df1-80fd-e62937112741 0xc002d2f470 0xc002d2f471}] []  [{kube-controller-manager Update v1 2021-10-19 15:56:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d7da6b-65e9-4df1-80fd-e62937112741\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 15:56:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 15:56:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p99bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p99bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:56:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:56:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 15:56:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.31,StartTime:2021-10-19 15:56:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 15:56:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://f10d65616257cf408eee010ea43ced6926d3a26e5179d560cfab855e1fea8d41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:56:28.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9142" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":19,"skipped":323,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:56:28.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Oct 19 15:56:28.785: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 19 15:56:33.789: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Oct 19 15:56:33.791: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Oct 19 15:56:33.797: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Oct 19 15:56:33.799: INFO: Observed &ReplicaSet event: ADDED
Oct 19 15:56:33.799: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.799: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.799: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.799: INFO: Found replicaset test-rs in namespace replicaset-3376 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Oct 19 15:56:33.799: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Oct 19 15:56:33.799: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Oct 19 15:56:33.806: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Oct 19 15:56:33.808: INFO: Observed &ReplicaSet event: ADDED
Oct 19 15:56:33.808: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.808: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.808: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.808: INFO: Observed replicaset test-rs in namespace replicaset-3376 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Oct 19 15:56:33.808: INFO: Observed &ReplicaSet event: MODIFIED
Oct 19 15:56:33.808: INFO: Found replicaset test-rs in namespace replicaset-3376 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Oct 19 15:56:33.808: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:56:33.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3376" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":20,"skipped":323,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:56:33.817: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 19 15:56:33.952: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 19 15:57:33.987: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:57:33.990: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 15:57:34.132: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Oct 19 15:57:34.134: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:57:34.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4288" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:57:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1686" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":21,"skipped":343,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:57:34.191: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 19 15:57:34.329: INFO: Waiting up to 5m0s for pod "pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7" in namespace "emptydir-3950" to be "Succeeded or Failed"
Oct 19 15:57:34.332: INFO: Pod "pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.23338ms
Oct 19 15:57:36.336: INFO: Pod "pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006875738s
STEP: Saw pod success
Oct 19 15:57:36.336: INFO: Pod "pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7" satisfied condition "Succeeded or Failed"
Oct 19 15:57:36.338: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7 container test-container: <nil>
STEP: delete the pod
Oct 19 15:57:36.390: INFO: Waiting for pod pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7 to disappear
Oct 19 15:57:36.393: INFO: Pod pod-35c3a28e-ba12-4f87-a66c-8f5783f900b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:57:36.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3950" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":350,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:57:36.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 19 15:57:36.542: INFO: The status of Pod labelsupdatec082796c-41f7-40a7-9e9a-ad4c1a2c057c is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:57:38.545: INFO: The status of Pod labelsupdatec082796c-41f7-40a7-9e9a-ad4c1a2c057c is Running (Ready = true)
Oct 19 15:57:39.063: INFO: Successfully updated pod "labelsupdatec082796c-41f7-40a7-9e9a-ad4c1a2c057c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:57:43.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3332" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:57:43.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-2236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Oct 19 15:57:43.245: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:57:45.248: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.3.120 on the node which pod1 resides and expect scheduled
Oct 19 15:57:45.257: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:57:47.261: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.3.120 but use UDP protocol on the node which pod2 resides
Oct 19 15:57:47.274: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:57:49.277: INFO: The status of Pod pod3 is Running (Ready = false)
Oct 19 15:57:51.279: INFO: The status of Pod pod3 is Running (Ready = true)
Oct 19 15:57:51.287: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:57:53.291: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Oct 19 15:57:53.294: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.120 http://127.0.0.1:54323/hostname] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 15:57:53.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 15:58:13.382: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54323
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Oct 19 15:58:13.382: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.120 http://127.0.0.1:54323/hostname] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 15:58:13.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 15:58:16.473: INFO: Can not connect from e2e-host-exec to pod(pod1) to serverIP: 127.0.0.1, port: 54323
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Oct 19 15:58:16.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.3.120 http://127.0.0.1:54323/hostname] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 15:58:16.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.120, port: 54323
Oct 19 15:58:22.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.3.120:54323/hostname] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 15:58:22.385: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.3.120, port: 54323 UDP
Oct 19 15:58:23.921: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.3.120 54323] Namespace:hostport-2236 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 15:58:23.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:29.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2236" for this suite.
•{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":24,"skipped":384,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:29.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-354
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Oct 19 15:58:29.253: INFO: namespace kubectl-354
Oct 19 15:58:29.253: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-354 create -f -'
Oct 19 15:58:29.379: INFO: stderr: ""
Oct 19 15:58:29.379: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 19 15:58:30.383: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 15:58:30.383: INFO: Found 1 / 1
Oct 19 15:58:30.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 19 15:58:30.386: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 15:58:30.386: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 19 15:58:30.386: INFO: wait on agnhost-primary startup in kubectl-354 
Oct 19 15:58:30.386: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-354 logs agnhost-primary-nl584 agnhost-primary'
Oct 19 15:58:30.465: INFO: stderr: ""
Oct 19 15:58:30.465: INFO: stdout: "Paused\n"
STEP: exposing RC
Oct 19 15:58:30.465: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-354 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Oct 19 15:58:30.529: INFO: stderr: ""
Oct 19 15:58:30.529: INFO: stdout: "service/rm2 exposed\n"
Oct 19 15:58:30.533: INFO: Service rm2 in namespace kubectl-354 found.
STEP: exposing service
Oct 19 15:58:32.542: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-354 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Oct 19 15:58:32.602: INFO: stderr: ""
Oct 19 15:58:32.602: INFO: stdout: "service/rm3 exposed\n"
Oct 19 15:58:32.606: INFO: Service rm3 in namespace kubectl-354 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:34.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-354" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":25,"skipped":397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:34.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-18f30ed7-2273-4813-8e14-3d0ced0bca14
STEP: Creating a pod to test consume configMaps
Oct 19 15:58:34.769: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141" in namespace "projected-6876" to be "Succeeded or Failed"
Oct 19 15:58:34.773: INFO: Pod "pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141": Phase="Pending", Reason="", readiness=false. Elapsed: 3.390488ms
Oct 19 15:58:36.777: INFO: Pod "pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007977936s
STEP: Saw pod success
Oct 19 15:58:36.778: INFO: Pod "pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141" satisfied condition "Succeeded or Failed"
Oct 19 15:58:36.782: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 15:58:36.794: INFO: Waiting for pod pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141 to disappear
Oct 19 15:58:36.797: INFO: Pod pod-projected-configmaps-e130e44f-dd56-40be-9aef-26db65f4e141 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:36.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6876" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":441,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:36.805: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 19 15:58:37.004: INFO: Number of nodes with available pods: 0
Oct 19 15:58:37.004: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:58:38.014: INFO: Number of nodes with available pods: 0
Oct 19 15:58:38.014: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:58:39.013: INFO: Number of nodes with available pods: 2
Oct 19 15:58:39.013: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Oct 19 15:58:39.030: INFO: Number of nodes with available pods: 1
Oct 19 15:58:39.030: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:58:40.039: INFO: Number of nodes with available pods: 1
Oct 19 15:58:40.039: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:58:41.039: INFO: Number of nodes with available pods: 1
Oct 19 15:58:41.039: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:58:42.039: INFO: Number of nodes with available pods: 1
Oct 19 15:58:42.039: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 15:58:43.041: INFO: Number of nodes with available pods: 2
Oct 19 15:58:43.041: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1743, will wait for the garbage collector to delete the pods
Oct 19 15:58:43.103: INFO: Deleting DaemonSet.extensions daemon-set took: 5.223419ms
Oct 19 15:58:43.204: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.821617ms
Oct 19 15:58:45.307: INFO: Number of nodes with available pods: 0
Oct 19 15:58:45.307: INFO: Number of running nodes: 0, number of available pods: 0
Oct 19 15:58:45.310: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6849"},"items":null}

Oct 19 15:58:45.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6849"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:45.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1743" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":27,"skipped":449,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:45.334: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:45.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9051" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":28,"skipped":459,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:45.483: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-5661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Oct 19 15:58:45.630: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:47.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5661" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":29,"skipped":472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:47.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Oct 19 15:58:47.809: INFO: Waiting up to 5m0s for pod "var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4" in namespace "var-expansion-6200" to be "Succeeded or Failed"
Oct 19 15:58:47.812: INFO: Pod "var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.150426ms
Oct 19 15:58:49.818: INFO: Pod "var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00915531s
STEP: Saw pod success
Oct 19 15:58:49.818: INFO: Pod "var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4" satisfied condition "Succeeded or Failed"
Oct 19 15:58:49.821: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4 container dapi-container: <nil>
STEP: delete the pod
Oct 19 15:58:49.833: INFO: Waiting for pod var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4 to disappear
Oct 19 15:58:49.836: INFO: Pod var-expansion-defb94b1-fb16-4cf3-ae75-b9443829e0f4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:49.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6200" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":531,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:49.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-8857
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8857 to expose endpoints map[]
Oct 19 15:58:49.997: INFO: successfully validated that service multi-endpoint-test in namespace services-8857 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8857
Oct 19 15:58:50.008: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:58:52.013: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8857 to expose endpoints map[pod1:[100]]
Oct 19 15:58:52.031: INFO: successfully validated that service multi-endpoint-test in namespace services-8857 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8857
Oct 19 15:58:52.041: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:58:54.046: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8857 to expose endpoints map[pod1:[100] pod2:[101]]
Oct 19 15:58:54.092: INFO: successfully validated that service multi-endpoint-test in namespace services-8857 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Oct 19 15:58:54.092: INFO: Creating new exec pod
Oct 19 15:58:57.107: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8857 exec execpodjhnqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Oct 19 15:58:57.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Oct 19 15:58:57.360: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 15:58:57.361: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8857 exec execpodjhnqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.129.254 80'
Oct 19 15:58:57.654: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.67.129.254 80\nConnection to 100.67.129.254 80 port [tcp/http] succeeded!\n"
Oct 19 15:58:57.654: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 15:58:57.654: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8857 exec execpodjhnqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Oct 19 15:58:57.881: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Oct 19 15:58:57.881: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 15:58:57.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8857 exec execpodjhnqr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.129.254 81'
Oct 19 15:58:58.048: INFO: stderr: "+ nc -v -t -w 2 100.67.129.254 81\n+ echo hostName\nConnection to 100.67.129.254 81 port [tcp/*] succeeded!\n"
Oct 19 15:58:58.048: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8857
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8857 to expose endpoints map[pod2:[101]]
Oct 19 15:58:58.067: INFO: successfully validated that service multi-endpoint-test in namespace services-8857 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8857
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8857 to expose endpoints map[]
Oct 19 15:58:58.080: INFO: successfully validated that service multi-endpoint-test in namespace services-8857 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:58:58.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8857" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":31,"skipped":542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:58:58.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-bc8nq in namespace proxy-7182
I1019 15:58:58.240608    4339 runners.go:190] Created replication controller with name: proxy-service-bc8nq, namespace: proxy-7182, replica count: 1
I1019 15:58:59.292231    4339 runners.go:190] proxy-service-bc8nq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1019 15:59:00.293252    4339 runners.go:190] proxy-service-bc8nq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 15:59:00.296: INFO: setup took 2.067352576s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Oct 19 15:59:00.312: INFO: (0) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 15.490326ms)
Oct 19 15:59:00.316: INFO: (0) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 18.949371ms)
Oct 19 15:59:00.320: INFO: (0) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 22.918127ms)
Oct 19 15:59:00.320: INFO: (0) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 23.301858ms)
Oct 19 15:59:00.320: INFO: (0) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 23.123836ms)
Oct 19 15:59:00.320: INFO: (0) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 23.10671ms)
Oct 19 15:59:00.377: INFO: (0) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 80.356973ms)
Oct 19 15:59:00.377: INFO: (0) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 80.564628ms)
Oct 19 15:59:00.378: INFO: (0) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 80.494856ms)
Oct 19 15:59:00.378: INFO: (0) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 80.540584ms)
Oct 19 15:59:00.378: INFO: (0) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 80.581051ms)
Oct 19 15:59:00.379: INFO: (0) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 82.077797ms)
Oct 19 15:59:00.379: INFO: (0) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 82.157197ms)
Oct 19 15:59:00.380: INFO: (0) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 82.871681ms)
Oct 19 15:59:00.380: INFO: (0) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 82.947374ms)
Oct 19 15:59:00.380: INFO: (0) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 82.78663ms)
Oct 19 15:59:00.386: INFO: (1) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 6.161522ms)
Oct 19 15:59:00.386: INFO: (1) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 6.154924ms)
Oct 19 15:59:00.386: INFO: (1) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 6.203034ms)
Oct 19 15:59:00.386: INFO: (1) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 6.329226ms)
Oct 19 15:59:00.386: INFO: (1) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 6.286102ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 9.601257ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.687843ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 9.653516ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 9.652419ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.822862ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 9.94455ms)
Oct 19 15:59:00.390: INFO: (1) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 10.138325ms)
Oct 19 15:59:00.391: INFO: (1) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 11.106102ms)
Oct 19 15:59:00.391: INFO: (1) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 11.06928ms)
Oct 19 15:59:00.392: INFO: (1) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 12.05565ms)
Oct 19 15:59:00.392: INFO: (1) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 12.318345ms)
Oct 19 15:59:00.398: INFO: (2) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 5.340249ms)
Oct 19 15:59:00.398: INFO: (2) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 5.528991ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.989187ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 9.018435ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 9.060124ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 9.012025ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.986529ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 9.010167ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.052636ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 9.061286ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 9.089066ms)
Oct 19 15:59:00.401: INFO: (2) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.10274ms)
Oct 19 15:59:00.404: INFO: (2) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 11.561476ms)
Oct 19 15:59:00.404: INFO: (2) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 11.562236ms)
Oct 19 15:59:00.404: INFO: (2) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 11.841583ms)
Oct 19 15:59:00.404: INFO: (2) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 11.829025ms)
Oct 19 15:59:00.410: INFO: (3) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 5.840693ms)
Oct 19 15:59:00.410: INFO: (3) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 6.021568ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.965412ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 8.768393ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.622582ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.720631ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.69689ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.611238ms)
Oct 19 15:59:00.413: INFO: (3) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 8.933762ms)
Oct 19 15:59:00.415: INFO: (3) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 10.800249ms)
Oct 19 15:59:00.416: INFO: (3) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 11.63731ms)
Oct 19 15:59:00.478: INFO: (3) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 73.184885ms)
Oct 19 15:59:00.478: INFO: (3) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 73.622578ms)
Oct 19 15:59:00.478: INFO: (3) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 73.222812ms)
Oct 19 15:59:00.478: INFO: (3) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 73.544454ms)
Oct 19 15:59:00.480: INFO: (3) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 75.717271ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.045503ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 8.416782ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 8.390971ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 8.265411ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.678897ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 8.667344ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 8.57149ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.662388ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.770898ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 9.163221ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.948275ms)
Oct 19 15:59:00.489: INFO: (4) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.080439ms)
Oct 19 15:59:00.491: INFO: (4) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 10.860143ms)
Oct 19 15:59:00.492: INFO: (4) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 11.456387ms)
Oct 19 15:59:00.492: INFO: (4) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 11.381725ms)
Oct 19 15:59:00.492: INFO: (4) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 11.145773ms)
Oct 19 15:59:00.497: INFO: (5) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 5.551859ms)
Oct 19 15:59:00.497: INFO: (5) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 5.32399ms)
Oct 19 15:59:00.498: INFO: (5) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 5.424873ms)
Oct 19 15:59:00.498: INFO: (5) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 5.695451ms)
Oct 19 15:59:00.498: INFO: (5) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 6.258156ms)
Oct 19 15:59:00.498: INFO: (5) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 6.071407ms)
Oct 19 15:59:00.498: INFO: (5) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 6.188677ms)
Oct 19 15:59:00.500: INFO: (5) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 7.444075ms)
Oct 19 15:59:00.500: INFO: (5) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 7.339964ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 9.117709ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 9.234256ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 9.272478ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 9.181122ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 9.345712ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.250507ms)
Oct 19 15:59:00.501: INFO: (5) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.242555ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 8.062653ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 8.524829ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 8.406253ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 8.384352ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 8.363988ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.192649ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.111299ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.243451ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 8.296041ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.376934ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.555524ms)
Oct 19 15:59:00.510: INFO: (6) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.353964ms)
Oct 19 15:59:00.578: INFO: (6) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 76.343419ms)
Oct 19 15:59:00.581: INFO: (6) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 78.549441ms)
Oct 19 15:59:00.581: INFO: (6) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 78.985625ms)
Oct 19 15:59:00.581: INFO: (6) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 78.597204ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 8.903755ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.88513ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.916149ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 8.912778ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.990732ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 8.96085ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.902647ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.045016ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 9.012736ms)
Oct 19 15:59:00.590: INFO: (7) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 8.948487ms)
Oct 19 15:59:00.592: INFO: (7) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 11.29543ms)
Oct 19 15:59:00.592: INFO: (7) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 11.190912ms)
Oct 19 15:59:00.592: INFO: (7) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 11.684045ms)
Oct 19 15:59:00.592: INFO: (7) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 11.736191ms)
Oct 19 15:59:00.593: INFO: (7) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 11.93166ms)
Oct 19 15:59:00.593: INFO: (7) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 11.967688ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 7.772548ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 7.844388ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 7.755089ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 8.286565ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 8.102362ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 7.960072ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 7.933276ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.052037ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 8.266412ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.359843ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.263839ms)
Oct 19 15:59:00.601: INFO: (8) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.335035ms)
Oct 19 15:59:00.604: INFO: (8) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 11.08425ms)
Oct 19 15:59:00.604: INFO: (8) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 10.959275ms)
Oct 19 15:59:00.604: INFO: (8) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 10.815428ms)
Oct 19 15:59:00.604: INFO: (8) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 10.861009ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 6.42026ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 6.461886ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 6.490304ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 6.784528ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 6.781469ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 6.789419ms)
Oct 19 15:59:00.611: INFO: (9) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 6.88627ms)
Oct 19 15:59:00.613: INFO: (9) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.930843ms)
Oct 19 15:59:00.613: INFO: (9) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.849032ms)
Oct 19 15:59:00.613: INFO: (9) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.871091ms)
Oct 19 15:59:00.613: INFO: (9) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.8833ms)
Oct 19 15:59:00.613: INFO: (9) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.938845ms)
Oct 19 15:59:00.614: INFO: (9) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.574331ms)
Oct 19 15:59:00.614: INFO: (9) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 9.684089ms)
Oct 19 15:59:00.614: INFO: (9) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 9.615755ms)
Oct 19 15:59:00.616: INFO: (9) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 11.803537ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 64.442116ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 64.029212ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 64.511188ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 64.256873ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 64.406532ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 64.132681ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 64.110313ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 64.217495ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 64.738598ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 64.324676ms)
Oct 19 15:59:00.681: INFO: (10) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 64.48877ms)
Oct 19 15:59:00.682: INFO: (10) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 66.069088ms)
Oct 19 15:59:00.684: INFO: (10) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 67.595393ms)
Oct 19 15:59:00.684: INFO: (10) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 67.450233ms)
Oct 19 15:59:00.684: INFO: (10) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 67.667852ms)
Oct 19 15:59:00.685: INFO: (10) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 68.947253ms)
Oct 19 15:59:00.691: INFO: (11) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 5.839736ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 7.227833ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 6.906292ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 6.890659ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 7.060783ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 7.137566ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 6.976675ms)
Oct 19 15:59:00.693: INFO: (11) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 7.052269ms)
Oct 19 15:59:00.695: INFO: (11) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 9.81069ms)
Oct 19 15:59:00.696: INFO: (11) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 10.116171ms)
Oct 19 15:59:00.696: INFO: (11) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 10.242433ms)
Oct 19 15:59:00.696: INFO: (11) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 10.293804ms)
Oct 19 15:59:00.696: INFO: (11) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 10.362521ms)
Oct 19 15:59:00.696: INFO: (11) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 10.071998ms)
Oct 19 15:59:00.696: INFO: (11) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 10.014955ms)
Oct 19 15:59:00.739: INFO: (11) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 53.25772ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.113223ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 8.17542ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 8.165148ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 8.095639ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.103536ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.147662ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 8.192434ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.095651ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.148053ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.114175ms)
Oct 19 15:59:00.747: INFO: (12) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.256847ms)
Oct 19 15:59:00.749: INFO: (12) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 10.128553ms)
Oct 19 15:59:00.750: INFO: (12) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 10.899178ms)
Oct 19 15:59:00.750: INFO: (12) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 11.034886ms)
Oct 19 15:59:00.750: INFO: (12) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 11.478128ms)
Oct 19 15:59:00.751: INFO: (12) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 11.724722ms)
Oct 19 15:59:00.756: INFO: (13) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 5.531581ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 8.172976ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 8.12735ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.160541ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 8.107676ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 8.237459ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.391013ms)
Oct 19 15:59:00.759: INFO: (13) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 8.359105ms)
Oct 19 15:59:00.780: INFO: (13) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 29.214838ms)
Oct 19 15:59:00.780: INFO: (13) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 29.38589ms)
Oct 19 15:59:00.780: INFO: (13) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 29.307636ms)
Oct 19 15:59:00.780: INFO: (13) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 29.242828ms)
Oct 19 15:59:00.780: INFO: (13) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 29.262471ms)
Oct 19 15:59:00.783: INFO: (13) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 32.053579ms)
Oct 19 15:59:00.783: INFO: (13) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 32.035667ms)
Oct 19 15:59:00.783: INFO: (13) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 32.058965ms)
Oct 19 15:59:00.788: INFO: (14) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 5.538338ms)
Oct 19 15:59:00.788: INFO: (14) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 5.463426ms)
Oct 19 15:59:00.788: INFO: (14) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 5.535977ms)
Oct 19 15:59:00.788: INFO: (14) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 5.61751ms)
Oct 19 15:59:00.789: INFO: (14) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 6.300271ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 11.433549ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 11.518961ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 11.414764ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 11.419853ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 11.537629ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 11.462817ms)
Oct 19 15:59:00.794: INFO: (14) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 11.396269ms)
Oct 19 15:59:00.795: INFO: (14) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 12.419678ms)
Oct 19 15:59:00.796: INFO: (14) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 13.574906ms)
Oct 19 15:59:00.796: INFO: (14) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 13.489096ms)
Oct 19 15:59:00.796: INFO: (14) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 13.54577ms)
Oct 19 15:59:00.802: INFO: (15) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 5.41155ms)
Oct 19 15:59:00.802: INFO: (15) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 5.469529ms)
Oct 19 15:59:00.803: INFO: (15) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 6.303501ms)
Oct 19 15:59:00.803: INFO: (15) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 6.267073ms)
Oct 19 15:59:00.803: INFO: (15) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 6.228918ms)
Oct 19 15:59:00.803: INFO: (15) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 6.266329ms)
Oct 19 15:59:00.806: INFO: (15) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.019656ms)
Oct 19 15:59:00.806: INFO: (15) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.079214ms)
Oct 19 15:59:00.806: INFO: (15) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.04283ms)
Oct 19 15:59:00.806: INFO: (15) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 9.026405ms)
Oct 19 15:59:00.806: INFO: (15) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.993855ms)
Oct 19 15:59:00.806: INFO: (15) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 9.615896ms)
Oct 19 15:59:00.808: INFO: (15) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 11.778778ms)
Oct 19 15:59:00.808: INFO: (15) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 11.787346ms)
Oct 19 15:59:00.808: INFO: (15) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 11.819787ms)
Oct 19 15:59:00.808: INFO: (15) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 11.807269ms)
Oct 19 15:59:00.814: INFO: (16) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 6.093132ms)
Oct 19 15:59:00.814: INFO: (16) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 5.955368ms)
Oct 19 15:59:00.815: INFO: (16) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 6.062825ms)
Oct 19 15:59:00.815: INFO: (16) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 6.03363ms)
Oct 19 15:59:00.815: INFO: (16) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 6.264831ms)
Oct 19 15:59:00.815: INFO: (16) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 6.248931ms)
Oct 19 15:59:00.815: INFO: (16) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 6.238844ms)
Oct 19 15:59:00.815: INFO: (16) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 6.357282ms)
Oct 19 15:59:00.818: INFO: (16) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 9.304555ms)
Oct 19 15:59:00.818: INFO: (16) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 9.346758ms)
Oct 19 15:59:00.880: INFO: (16) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 71.005224ms)
Oct 19 15:59:00.880: INFO: (16) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 71.012351ms)
Oct 19 15:59:00.881: INFO: (16) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 72.804722ms)
Oct 19 15:59:00.881: INFO: (16) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 72.845549ms)
Oct 19 15:59:00.881: INFO: (16) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 72.907438ms)
Oct 19 15:59:00.882: INFO: (16) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 73.787306ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 9.35604ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 9.45149ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 9.460452ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 9.513462ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 9.427688ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 9.516176ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 9.453718ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 9.478682ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 9.421602ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 9.464569ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 9.540564ms)
Oct 19 15:59:00.892: INFO: (17) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 9.461024ms)
Oct 19 15:59:00.895: INFO: (17) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 12.752586ms)
Oct 19 15:59:00.895: INFO: (17) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 12.723848ms)
Oct 19 15:59:00.895: INFO: (17) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 12.833539ms)
Oct 19 15:59:00.896: INFO: (17) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 13.742591ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 8.421796ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 8.405347ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 8.427769ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 8.498561ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 8.394172ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 8.418524ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 8.462398ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.475959ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 8.595379ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 8.466304ms)
Oct 19 15:59:00.905: INFO: (18) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 8.539772ms)
Oct 19 15:59:00.907: INFO: (18) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 10.96337ms)
Oct 19 15:59:00.910: INFO: (18) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 13.285384ms)
Oct 19 15:59:00.910: INFO: (18) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 13.376649ms)
Oct 19 15:59:00.910: INFO: (18) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 13.36428ms)
Oct 19 15:59:00.910: INFO: (18) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 13.300334ms)
Oct 19 15:59:00.916: INFO: (19) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname1/proxy/: tls baz (200; 5.556356ms)
Oct 19 15:59:00.916: INFO: (19) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 5.746178ms)
Oct 19 15:59:00.916: INFO: (19) /api/v1/namespaces/proxy-7182/services/https:proxy-service-bc8nq:tlsportname2/proxy/: tls qux (200; 6.517871ms)
Oct 19 15:59:00.918: INFO: (19) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:460/proxy/: tls baz (200; 7.61656ms)
Oct 19 15:59:00.918: INFO: (19) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:462/proxy/: tls qux (200; 7.930517ms)
Oct 19 15:59:00.918: INFO: (19) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">test<... (200; 8.011667ms)
Oct 19 15:59:00.918: INFO: (19) /api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/https:proxy-service-bc8nq-tdjmm:443/proxy/tlsrewritem... (200; 7.88186ms)
Oct 19 15:59:00.919: INFO: (19) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm/proxy/rewriteme">test</a> (200; 9.201046ms)
Oct 19 15:59:00.919: INFO: (19) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname2/proxy/: bar (200; 9.146549ms)
Oct 19 15:59:00.919: INFO: (19) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 9.074144ms)
Oct 19 15:59:00.919: INFO: (19) /api/v1/namespaces/proxy-7182/services/http:proxy-service-bc8nq:portname1/proxy/: foo (200; 8.979269ms)
Oct 19 15:59:00.919: INFO: (19) /api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/: <a href="/api/v1/namespaces/proxy-7182/pods/http:proxy-service-bc8nq-tdjmm:1080/proxy/rewriteme">... (200; 9.032505ms)
Oct 19 15:59:00.922: INFO: (19) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:162/proxy/: bar (200; 11.732913ms)
Oct 19 15:59:00.922: INFO: (19) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname2/proxy/: bar (200; 11.9192ms)
Oct 19 15:59:00.922: INFO: (19) /api/v1/namespaces/proxy-7182/services/proxy-service-bc8nq:portname1/proxy/: foo (200; 11.849233ms)
Oct 19 15:59:00.977: INFO: (19) /api/v1/namespaces/proxy-7182/pods/proxy-service-bc8nq-tdjmm:160/proxy/: foo (200; 67.567264ms)
STEP: deleting ReplicationController proxy-service-bc8nq in namespace proxy-7182, will wait for the garbage collector to delete the pods
Oct 19 15:59:01.037: INFO: Deleting ReplicationController proxy-service-bc8nq took: 5.002535ms
Oct 19 15:59:01.137: INFO: Terminating ReplicationController proxy-service-bc8nq pods took: 100.285563ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:02.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7182" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":32,"skipped":574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:02.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:18.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9623" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":33,"skipped":616,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:18.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9417
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Oct 19 15:59:20.748: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9417 PodName:pod-sharedvolume-8d9f0169-7037-4a03-be50-bbbd88c74012 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 15:59:20.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 15:59:20.894: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:20.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9417" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":34,"skipped":631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:20.903: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 15:59:21.052: INFO: The status of Pod busybox-readonly-fsde5dbd39-fa3c-43e9-a294-308bcb966fce is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:59:23.056: INFO: The status of Pod busybox-readonly-fsde5dbd39-fa3c-43e9-a294-308bcb966fce is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:23.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2112" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:23.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2740.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2740.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2740.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2740.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2740.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2740.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 15:59:25.364: INFO: DNS probes using dns-2740/dns-test-ccf90120-55ec-45dd-b4dd-7f8ac7d1f464 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:25.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2740" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":36,"skipped":707,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:25.378: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 19 15:59:25.524: INFO: The status of Pod pod-update-ab1d6b07-0167-4f09-b29c-c4b97dd629c1 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:59:27.529: INFO: The status of Pod pod-update-ab1d6b07-0167-4f09-b29c-c4b97dd629c1 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 15:59:29.530: INFO: The status of Pod pod-update-ab1d6b07-0167-4f09-b29c-c4b97dd629c1 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 19 15:59:30.049: INFO: Successfully updated pod "pod-update-ab1d6b07-0167-4f09-b29c-c4b97dd629c1"
STEP: verifying the updated pod is in kubernetes
Oct 19 15:59:30.055: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:30.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8932" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":710,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:30.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-0ceb57f5-0e90-433f-9f96-0a130b45d37f
STEP: Creating secret with name secret-projected-all-test-volume-7a617929-2fe5-495e-952f-a9e2d15428ee
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct 19 15:59:30.215: INFO: Waiting up to 5m0s for pod "projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc" in namespace "projected-6939" to be "Succeeded or Failed"
Oct 19 15:59:30.218: INFO: Pod "projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960201ms
Oct 19 15:59:32.223: INFO: Pod "projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007469223s
STEP: Saw pod success
Oct 19 15:59:32.223: INFO: Pod "projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc" satisfied condition "Succeeded or Failed"
Oct 19 15:59:32.226: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc container projected-all-volume-test: <nil>
STEP: delete the pod
Oct 19 15:59:32.248: INFO: Waiting for pod projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc to disappear
Oct 19 15:59:32.251: INFO: Pod projected-volume-bc68b14a-7591-4c84-8443-e4164ef137fc no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:32.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6939" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":38,"skipped":736,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:32.260: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Oct 19 15:59:32.396: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 create -f -'
Oct 19 15:59:32.535: INFO: stderr: ""
Oct 19 15:59:32.535: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 19 15:59:32.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 15:59:32.585: INFO: stderr: ""
Oct 19 15:59:32.585: INFO: stdout: "update-demo-nautilus-6mc7t update-demo-nautilus-dsrn2 "
Oct 19 15:59:32.585: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-6mc7t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 15:59:32.640: INFO: stderr: ""
Oct 19 15:59:32.640: INFO: stdout: ""
Oct 19 15:59:32.640: INFO: update-demo-nautilus-6mc7t is created but not running
Oct 19 15:59:37.641: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 15:59:37.693: INFO: stderr: ""
Oct 19 15:59:37.693: INFO: stdout: "update-demo-nautilus-6mc7t update-demo-nautilus-dsrn2 "
Oct 19 15:59:37.693: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-6mc7t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 15:59:37.740: INFO: stderr: ""
Oct 19 15:59:37.740: INFO: stdout: "true"
Oct 19 15:59:37.740: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-6mc7t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 15:59:37.789: INFO: stderr: ""
Oct 19 15:59:37.789: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 15:59:37.790: INFO: validating pod update-demo-nautilus-6mc7t
Oct 19 15:59:37.847: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 15:59:37.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 15:59:37.847: INFO: update-demo-nautilus-6mc7t is verified up and running
Oct 19 15:59:37.847: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-dsrn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 15:59:37.894: INFO: stderr: ""
Oct 19 15:59:37.894: INFO: stdout: "true"
Oct 19 15:59:37.894: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-dsrn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 15:59:37.937: INFO: stderr: ""
Oct 19 15:59:37.937: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 15:59:37.937: INFO: validating pod update-demo-nautilus-dsrn2
Oct 19 15:59:37.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 15:59:37.996: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 15:59:37.996: INFO: update-demo-nautilus-dsrn2 is verified up and running
STEP: scaling down the replication controller
Oct 19 15:59:37.997: INFO: scanned /root for discovery docs: <nil>
Oct 19 15:59:37.997: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Oct 19 15:59:39.072: INFO: stderr: ""
Oct 19 15:59:39.072: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 19 15:59:39.072: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 15:59:39.122: INFO: stderr: ""
Oct 19 15:59:39.122: INFO: stdout: "update-demo-nautilus-6mc7t update-demo-nautilus-dsrn2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 19 15:59:44.124: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 15:59:44.171: INFO: stderr: ""
Oct 19 15:59:44.171: INFO: stdout: "update-demo-nautilus-dsrn2 "
Oct 19 15:59:44.171: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-dsrn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 15:59:44.227: INFO: stderr: ""
Oct 19 15:59:44.227: INFO: stdout: "true"
Oct 19 15:59:44.227: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-dsrn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 15:59:44.271: INFO: stderr: ""
Oct 19 15:59:44.271: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 15:59:44.271: INFO: validating pod update-demo-nautilus-dsrn2
Oct 19 15:59:44.277: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 15:59:44.277: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 15:59:44.277: INFO: update-demo-nautilus-dsrn2 is verified up and running
STEP: scaling up the replication controller
Oct 19 15:59:44.278: INFO: scanned /root for discovery docs: <nil>
Oct 19 15:59:44.278: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Oct 19 15:59:45.349: INFO: stderr: ""
Oct 19 15:59:45.349: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 19 15:59:45.349: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 15:59:45.402: INFO: stderr: ""
Oct 19 15:59:45.402: INFO: stdout: "update-demo-nautilus-dsrn2 update-demo-nautilus-rs24r "
Oct 19 15:59:45.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-dsrn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 15:59:45.449: INFO: stderr: ""
Oct 19 15:59:45.449: INFO: stdout: "true"
Oct 19 15:59:45.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-dsrn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 15:59:45.498: INFO: stderr: ""
Oct 19 15:59:45.498: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 15:59:45.498: INFO: validating pod update-demo-nautilus-dsrn2
Oct 19 15:59:45.547: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 15:59:45.547: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 15:59:45.547: INFO: update-demo-nautilus-dsrn2 is verified up and running
Oct 19 15:59:45.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-rs24r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 15:59:45.593: INFO: stderr: ""
Oct 19 15:59:45.593: INFO: stdout: "true"
Oct 19 15:59:45.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods update-demo-nautilus-rs24r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 15:59:45.639: INFO: stderr: ""
Oct 19 15:59:45.639: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 15:59:45.639: INFO: validating pod update-demo-nautilus-rs24r
Oct 19 15:59:45.694: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 15:59:45.694: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 15:59:45.694: INFO: update-demo-nautilus-rs24r is verified up and running
STEP: using delete to clean up resources
Oct 19 15:59:45.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 delete --grace-period=0 --force -f -'
Oct 19 15:59:45.741: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 15:59:45.741: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 19 15:59:45.741: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get rc,svc -l name=update-demo --no-headers'
Oct 19 15:59:45.791: INFO: stderr: "No resources found in kubectl-1002 namespace.\n"
Oct 19 15:59:45.791: INFO: stdout: ""
Oct 19 15:59:45.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1002 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 19 15:59:45.840: INFO: stderr: ""
Oct 19 15:59:45.840: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:45.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1002" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":39,"skipped":741,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:45.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Oct 19 15:59:46.002: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-898  8c9d7191-c0f6-4caa-847c-aa14e8c9114a 7492 0 2021-10-19 15:59:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-19 15:59:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 15:59:46.002: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-898  8c9d7191-c0f6-4caa-847c-aa14e8c9114a 7493 0 2021-10-19 15:59:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-19 15:59:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 15:59:46.002: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-898  8c9d7191-c0f6-4caa-847c-aa14e8c9114a 7494 0 2021-10-19 15:59:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-19 15:59:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Oct 19 15:59:56.028: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-898  8c9d7191-c0f6-4caa-847c-aa14e8c9114a 7552 0 2021-10-19 15:59:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-19 15:59:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 15:59:56.028: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-898  8c9d7191-c0f6-4caa-847c-aa14e8c9114a 7553 0 2021-10-19 15:59:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-19 15:59:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 15:59:56.028: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-898  8c9d7191-c0f6-4caa-847c-aa14e8c9114a 7554 0 2021-10-19 15:59:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-19 15:59:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:56.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-898" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":40,"skipped":758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:56.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Oct 19 15:59:56.187: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2439 proxy --unix-socket=/tmp/kubectl-proxy-unix978741801/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 15:59:56.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2439" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":41,"skipped":802,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 15:59:56.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 15:59:57.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:00:00.173: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:00.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4670" for this suite.
STEP: Destroying namespace "webhook-4670-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":42,"skipped":813,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:00.333: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:00:00.473: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3738d763-8268-43ba-b31f-dee9060aa347" in namespace "security-context-test-712" to be "Succeeded or Failed"
Oct 19 16:00:00.475: INFO: Pod "busybox-privileged-false-3738d763-8268-43ba-b31f-dee9060aa347": Phase="Pending", Reason="", readiness=false. Elapsed: 2.865455ms
Oct 19 16:00:02.480: INFO: Pod "busybox-privileged-false-3738d763-8268-43ba-b31f-dee9060aa347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007822486s
Oct 19 16:00:02.480: INFO: Pod "busybox-privileged-false-3738d763-8268-43ba-b31f-dee9060aa347" satisfied condition "Succeeded or Failed"
Oct 19 16:00:02.487: INFO: Got logs for pod "busybox-privileged-false-3738d763-8268-43ba-b31f-dee9060aa347": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-712" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:02.497: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:00:02.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215" in namespace "projected-4781" to be "Succeeded or Failed"
Oct 19 16:00:02.649: INFO: Pod "downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135165ms
Oct 19 16:00:04.652: INFO: Pod "downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006820914s
STEP: Saw pod success
Oct 19 16:00:04.652: INFO: Pod "downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215" satisfied condition "Succeeded or Failed"
Oct 19 16:00:04.656: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215 container client-container: <nil>
STEP: delete the pod
Oct 19 16:00:04.670: INFO: Waiting for pod downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215 to disappear
Oct 19 16:00:04.673: INFO: Pod downwardapi-volume-574512f9-81e2-4fed-849b-da726a781215 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:04.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4781" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:04.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Oct 19 16:00:04.826: INFO: Waiting up to 5m0s for pod "client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69" in namespace "containers-2779" to be "Succeeded or Failed"
Oct 19 16:00:04.830: INFO: Pod "client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.997073ms
Oct 19 16:00:06.836: INFO: Pod "client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009954491s
STEP: Saw pod success
Oct 19 16:00:06.836: INFO: Pod "client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69" satisfied condition "Succeeded or Failed"
Oct 19 16:00:06.840: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:00:06.854: INFO: Waiting for pod client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69 to disappear
Oct 19 16:00:06.857: INFO: Pod client-containers-b61b248d-b9c4-42d5-ad3b-a6904f6b3b69 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:06.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2779" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":874,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:06.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Oct 19 16:00:07.014: INFO: The status of Pod pod-hostip-e636583f-2bb4-43a2-8e11-b0896ea4d6cc is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:00:09.019: INFO: The status of Pod pod-hostip-e636583f-2bb4-43a2-8e11-b0896ea4d6cc is Running (Ready = true)
Oct 19 16:00:09.025: INFO: Pod pod-hostip-e636583f-2bb4-43a2-8e11-b0896ea4d6cc has hostIP: 10.250.1.123
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:09.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2648" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":879,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:09.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-df193c67-58a1-45e9-aa57-be8f2dea3141
STEP: Creating a pod to test consume configMaps
Oct 19 16:00:09.183: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e" in namespace "projected-4304" to be "Succeeded or Failed"
Oct 19 16:00:09.186: INFO: Pod "pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319489ms
Oct 19 16:00:11.191: INFO: Pod "pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008786094s
STEP: Saw pod success
Oct 19 16:00:11.191: INFO: Pod "pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e" satisfied condition "Succeeded or Failed"
Oct 19 16:00:11.195: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:00:11.208: INFO: Waiting for pod pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e to disappear
Oct 19 16:00:11.212: INFO: Pod pod-projected-configmaps-0d208d21-7674-4870-b092-c25c10a3ca1e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:11.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4304" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":893,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:11.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2679.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2679.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2679.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2679.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2679.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2679.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 16:00:13.559: INFO: DNS probes using dns-2679/dns-test-c3c71043-ab71-4158-be0a-b163c3caad9d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:13.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2679" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":48,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:13.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-2621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-2621
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2621
STEP: Deleting pre-stop pod
Oct 19 16:00:22.843: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:22.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2621" for this suite.
•{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":49,"skipped":921,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:22.863: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-4b9c159b-1b0d-4a55-8e50-fddf861ce697
STEP: Creating a pod to test consume configMaps
Oct 19 16:00:23.016: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8" in namespace "projected-6783" to be "Succeeded or Failed"
Oct 19 16:00:23.019: INFO: Pod "pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.875496ms
Oct 19 16:00:25.032: INFO: Pod "pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015684656s
STEP: Saw pod success
Oct 19 16:00:25.032: INFO: Pod "pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8" satisfied condition "Succeeded or Failed"
Oct 19 16:00:25.038: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:00:25.094: INFO: Waiting for pod pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8 to disappear
Oct 19 16:00:25.097: INFO: Pod pod-projected-configmaps-22e3bb3a-6c19-459e-a568-9808a4def3d8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:25.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6783" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:25.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:00:25.679: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:00:28.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:28.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6656" for this suite.
STEP: Destroying namespace "webhook-6656-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":51,"skipped":961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:28.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6284
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 19 16:00:29.057: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 19 16:00:29.091: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:00:31.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:00:33.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:00:35.104: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:00:37.096: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:00:39.096: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 19 16:00:39.102: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 19 16:00:41.107: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 19 16:00:43.130: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 19 16:00:43.130: INFO: Breadth first check of 100.96.0.60 on host 10.250.1.123...
Oct 19 16:00:43.133: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.61:9080/dial?request=hostname&protocol=http&host=100.96.0.60&port=8083&tries=1'] Namespace:pod-network-test-6284 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:00:43.134: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:00:43.299: INFO: Waiting for responses: map[]
Oct 19 16:00:43.299: INFO: reached 100.96.0.60 after 0/1 tries
Oct 19 16:00:43.299: INFO: Breadth first check of 100.96.1.30 on host 10.250.3.120...
Oct 19 16:00:43.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.61:9080/dial?request=hostname&protocol=http&host=100.96.1.30&port=8083&tries=1'] Namespace:pod-network-test-6284 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:00:43.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:00:43.564: INFO: Waiting for responses: map[]
Oct 19 16:00:43.564: INFO: reached 100.96.1.30 after 0/1 tries
Oct 19 16:00:43.564: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6284" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":999,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:43.572: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Oct 19 16:00:43.712: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Oct 19 16:00:43.770: INFO: stderr: ""
Oct 19 16:00:43.770: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Oct 19 16:00:43.770: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Oct 19 16:00:43.770: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7334" to be "running and ready, or succeeded"
Oct 19 16:00:43.773: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285601ms
Oct 19 16:00:45.777: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007057638s
Oct 19 16:00:45.777: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Oct 19 16:00:45.777: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Oct 19 16:00:45.777: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 logs logs-generator logs-generator'
Oct 19 16:00:45.835: INFO: stderr: ""
Oct 19 16:00:45.835: INFO: stdout: "I1019 16:00:44.379998       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/lz7p 393\nI1019 16:00:44.580066       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/gm5 260\nI1019 16:00:44.780593       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/g6fz 209\nI1019 16:00:44.980889       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jj24 536\nI1019 16:00:45.180120       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/sr9n 230\nI1019 16:00:45.380409       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/vgw9 371\nI1019 16:00:45.580695       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/7qc 422\nI1019 16:00:45.780856       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/djrv 568\n"
STEP: limiting log lines
Oct 19 16:00:45.835: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 logs logs-generator logs-generator --tail=1'
Oct 19 16:00:45.935: INFO: stderr: ""
Oct 19 16:00:45.935: INFO: stdout: "I1019 16:00:45.780856       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/djrv 568\n"
Oct 19 16:00:45.935: INFO: got output "I1019 16:00:45.780856       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/djrv 568\n"
STEP: limiting log bytes
Oct 19 16:00:45.935: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 logs logs-generator logs-generator --limit-bytes=1'
Oct 19 16:00:45.993: INFO: stderr: ""
Oct 19 16:00:45.993: INFO: stdout: "I"
Oct 19 16:00:45.993: INFO: got output "I"
STEP: exposing timestamps
Oct 19 16:00:45.993: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 logs logs-generator logs-generator --tail=1 --timestamps'
Oct 19 16:00:46.046: INFO: stderr: ""
Oct 19 16:00:46.046: INFO: stdout: "2021-10-19T16:00:45.980151081Z I1019 16:00:45.980089       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4hl 471\n"
Oct 19 16:00:46.046: INFO: got output "2021-10-19T16:00:45.980151081Z I1019 16:00:45.980089       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4hl 471\n"
STEP: restricting to a time range
Oct 19 16:00:48.548: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 logs logs-generator logs-generator --since=1s'
Oct 19 16:00:48.607: INFO: stderr: ""
Oct 19 16:00:48.607: INFO: stdout: "I1019 16:00:47.780344       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/22b 590\nI1019 16:00:47.980636       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/2x5k 462\nI1019 16:00:48.180902       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5qf 502\nI1019 16:00:48.380128       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/6k9 342\nI1019 16:00:48.580540       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/bwcf 481\n"
Oct 19 16:00:48.607: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 logs logs-generator logs-generator --since=24h'
Oct 19 16:00:48.663: INFO: stderr: ""
Oct 19 16:00:48.663: INFO: stdout: "I1019 16:00:44.379998       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/lz7p 393\nI1019 16:00:44.580066       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/gm5 260\nI1019 16:00:44.780593       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/g6fz 209\nI1019 16:00:44.980889       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jj24 536\nI1019 16:00:45.180120       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/sr9n 230\nI1019 16:00:45.380409       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/vgw9 371\nI1019 16:00:45.580695       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/7qc 422\nI1019 16:00:45.780856       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/djrv 568\nI1019 16:00:45.980089       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4hl 471\nI1019 16:00:46.180198       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/4bps 407\nI1019 16:00:46.380501       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/bnl 237\nI1019 16:00:46.580791       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/v6f 397\nI1019 16:00:46.780031       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/4sqv 490\nI1019 16:00:46.980322       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/6bw 575\nI1019 16:00:47.180610       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/5qqj 309\nI1019 16:00:47.380898       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/kmp 210\nI1019 16:00:47.580056       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/kb6 378\nI1019 16:00:47.780344       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/22b 590\nI1019 16:00:47.980636       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/2x5k 462\nI1019 16:00:48.180902       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5qf 502\nI1019 16:00:48.380128       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/6k9 342\nI1019 16:00:48.580540       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/bwcf 481\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Oct 19 16:00:48.663: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7334 delete pod logs-generator'
Oct 19 16:00:49.597: INFO: stderr: ""
Oct 19 16:00:49.597: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:49.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7334" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":53,"skipped":1012,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:49.606: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:00:50.047: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:00:53.068: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:00:53.072: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:56.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5983" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":54,"skipped":1014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:56.552: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Oct 19 16:00:56.701: INFO: Waiting up to 5m0s for pod "security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9" in namespace "security-context-888" to be "Succeeded or Failed"
Oct 19 16:00:56.705: INFO: Pod "security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929855ms
Oct 19 16:00:58.709: INFO: Pod "security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007516697s
STEP: Saw pod success
Oct 19 16:00:58.709: INFO: Pod "security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9" satisfied condition "Succeeded or Failed"
Oct 19 16:00:58.712: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9 container test-container: <nil>
STEP: delete the pod
Oct 19 16:00:58.727: INFO: Waiting for pod security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9 to disappear
Oct 19 16:00:58.730: INFO: Pod security-context-2e044e7e-ff35-49ab-b0ce-97cc4d5531c9 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:00:58.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-888" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":55,"skipped":1071,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:00:58.738: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Oct 19 16:00:58.876: INFO: Creating simple deployment test-deployment-lvl58
Oct 19 16:00:58.892: INFO: deployment "test-deployment-lvl58" doesn't have the required revision set
STEP: Getting /status
Oct 19 16:01:00.910: INFO: Deployment test-deployment-lvl58 has Conditions: [{Available True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lvl58-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Oct 19 16:01:00.918: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770256059, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770256059, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770256059, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770256058, loc:(*time.Location)(0xa09bc80)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lvl58-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Oct 19 16:01:00.920: INFO: Observed &Deployment event: ADDED
Oct 19 16:01:00.920: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lvl58-794dd694d8"}
Oct 19 16:01:00.920: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.920: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lvl58-794dd694d8"}
Oct 19 16:01:00.920: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Oct 19 16:01:00.921: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.921: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Oct 19 16:01:00.921: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lvl58-794dd694d8" is progressing.}
Oct 19 16:01:00.921: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.921: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Oct 19 16:01:00.921: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lvl58-794dd694d8" has successfully progressed.}
Oct 19 16:01:00.921: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.921: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Oct 19 16:01:00.921: INFO: Observed Deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lvl58-794dd694d8" has successfully progressed.}
Oct 19 16:01:00.921: INFO: Found Deployment test-deployment-lvl58 in namespace deployment-4122 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Oct 19 16:01:00.921: INFO: Deployment test-deployment-lvl58 has an updated status
STEP: patching the Statefulset Status
Oct 19 16:01:00.921: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Oct 19 16:01:00.925: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Oct 19 16:01:00.929: INFO: Observed &Deployment event: ADDED
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lvl58-794dd694d8"}
Oct 19 16:01:00.929: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lvl58-794dd694d8"}
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Oct 19 16:01:00.929: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:58 +0000 UTC 2021-10-19 16:00:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lvl58-794dd694d8" is progressing.}
Oct 19 16:01:00.929: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Oct 19 16:01:00.929: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lvl58-794dd694d8" has successfully progressed.}
Oct 19 16:01:00.930: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.930: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Oct 19 16:01:00.930: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-10-19 16:00:59 +0000 UTC 2021-10-19 16:00:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lvl58-794dd694d8" has successfully progressed.}
Oct 19 16:01:00.930: INFO: Observed deployment test-deployment-lvl58 in namespace deployment-4122 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Oct 19 16:01:00.930: INFO: Observed &Deployment event: MODIFIED
Oct 19 16:01:00.930: INFO: Found deployment test-deployment-lvl58 in namespace deployment-4122 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Oct 19 16:01:00.930: INFO: Deployment test-deployment-lvl58 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 16:01:00.933: INFO: Deployment "test-deployment-lvl58":
&Deployment{ObjectMeta:{test-deployment-lvl58  deployment-4122  78353099-f2d2-445d-8439-23b47c87de77 8369 1 2021-10-19 16:00:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-10-19 16:00:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2021-10-19 16:01:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2021-10-19 16:01:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005bcfc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lvl58-794dd694d8",LastUpdateTime:2021-10-19 16:01:00 +0000 UTC,LastTransitionTime:2021-10-19 16:01:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 19 16:01:00.936: INFO: New ReplicaSet "test-deployment-lvl58-794dd694d8" of Deployment "test-deployment-lvl58":
&ReplicaSet{ObjectMeta:{test-deployment-lvl58-794dd694d8  deployment-4122  12233fa4-9247-4506-acad-ec2b65409d13 8359 1 2021-10-19 16:00:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lvl58 78353099-f2d2-445d-8439-23b47c87de77 0xc003890037 0xc003890038}] []  [{kube-controller-manager Update apps/v1 2021-10-19 16:00:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78353099-f2d2-445d-8439-23b47c87de77\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 16:00:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038900e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 19 16:01:00.940: INFO: Pod "test-deployment-lvl58-794dd694d8-7lfx4" is available:
&Pod{ObjectMeta:{test-deployment-lvl58-794dd694d8-7lfx4 test-deployment-lvl58-794dd694d8- deployment-4122  bc6402a5-ff50-42f1-9b2d-383f883ce159 8358 0 2021-10-19 16:00:58 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/podIP:100.96.0.65/32 cni.projectcalico.org/podIPs:100.96.0.65/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-lvl58-794dd694d8 12233fa4-9247-4506-acad-ec2b65409d13 0xc003890497 0xc003890498}] []  [{kube-controller-manager Update v1 2021-10-19 16:00:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12233fa4-9247-4506-acad-ec2b65409d13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:00:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:00:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8b79h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8b79h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:00:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:00:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:00:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:00:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.65,StartTime:2021-10-19 16:00:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:00:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://da0ac8f0f0547c1592bc201e6af80963684f4d26ed04b8227f6e1c1d4ab37fcc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:00.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4122" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":56,"skipped":1079,"failed":0}
SSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:00.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Oct 19 16:01:01.087: INFO: created test-event-1
Oct 19 16:01:01.091: INFO: created test-event-2
Oct 19 16:01:01.094: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Oct 19 16:01:01.098: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Oct 19 16:01:01.106: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:01.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2680" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":57,"skipped":1087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:01.117: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-3859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Oct 19 16:01:01.255: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Oct 19 16:01:01.262: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 19 16:01:01.262: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Oct 19 16:01:01.272: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 19 16:01:01.272: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Oct 19 16:01:01.283: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Oct 19 16:01:01.283: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Oct 19 16:01:08.325: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:08.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3859" for this suite.
•{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":58,"skipped":1123,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:08.345: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-6912447b-ec64-47b2-937e-9e1a317c0224
STEP: Creating a pod to test consume secrets
Oct 19 16:01:08.492: INFO: Waiting up to 5m0s for pod "pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442" in namespace "secrets-5291" to be "Succeeded or Failed"
Oct 19 16:01:08.495: INFO: Pod "pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029014ms
Oct 19 16:01:10.499: INFO: Pod "pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006671671s
STEP: Saw pod success
Oct 19 16:01:10.499: INFO: Pod "pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442" satisfied condition "Succeeded or Failed"
Oct 19 16:01:10.502: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442 container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:01:10.514: INFO: Waiting for pod pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442 to disappear
Oct 19 16:01:10.517: INFO: Pod pod-secrets-d0d67dc4-18a7-4d7b-94f1-d97116974442 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:10.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5291" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1126,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:10.525: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Oct 19 16:01:12.678: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3758 PodName:var-expansion-3660110d-f9c4-48ec-a62e-ef4df298883a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:01:12.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: test for file in mounted path
Oct 19 16:01:12.836: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3758 PodName:var-expansion-3660110d-f9c4-48ec-a62e-ef4df298883a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:01:12.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: updating the annotation value
Oct 19 16:01:13.477: INFO: Successfully updated pod "var-expansion-3660110d-f9c4-48ec-a62e-ef4df298883a"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Oct 19 16:01:13.481: INFO: Deleting pod "var-expansion-3660110d-f9c4-48ec-a62e-ef4df298883a" in namespace "var-expansion-3758"
Oct 19 16:01:13.484: INFO: Wait up to 5m0s for pod "var-expansion-3660110d-f9c4-48ec-a62e-ef4df298883a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3758" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":60,"skipped":1142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:47.514: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-82cb7876-1eef-4347-ba0a-878f6800cb9c
STEP: Creating a pod to test consume configMaps
Oct 19 16:01:47.720: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa" in namespace "projected-1692" to be "Succeeded or Failed"
Oct 19 16:01:47.723: INFO: Pod "pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.056246ms
Oct 19 16:01:49.727: INFO: Pod "pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007115567s
STEP: Saw pod success
Oct 19 16:01:49.727: INFO: Pod "pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa" satisfied condition "Succeeded or Failed"
Oct 19 16:01:49.730: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:01:49.745: INFO: Waiting for pod pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa to disappear
Oct 19 16:01:49.748: INFO: Pod pod-projected-configmaps-23468c67-b5d8-498d-93c3-d6626f010aaa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:49.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1692" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1209,"failed":0}
SSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:49.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-5802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Oct 19 16:01:49.905: INFO: Waiting up to 5m0s for pod "security-context-45478bec-201d-4a0b-83a2-44d296013577" in namespace "security-context-5802" to be "Succeeded or Failed"
Oct 19 16:01:49.914: INFO: Pod "security-context-45478bec-201d-4a0b-83a2-44d296013577": Phase="Pending", Reason="", readiness=false. Elapsed: 9.202976ms
Oct 19 16:01:51.922: INFO: Pod "security-context-45478bec-201d-4a0b-83a2-44d296013577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016314835s
STEP: Saw pod success
Oct 19 16:01:51.922: INFO: Pod "security-context-45478bec-201d-4a0b-83a2-44d296013577" satisfied condition "Succeeded or Failed"
Oct 19 16:01:51.925: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod security-context-45478bec-201d-4a0b-83a2-44d296013577 container test-container: <nil>
STEP: delete the pod
Oct 19 16:01:51.940: INFO: Waiting for pod security-context-45478bec-201d-4a0b-83a2-44d296013577 to disappear
Oct 19 16:01:51.943: INFO: Pod security-context-45478bec-201d-4a0b-83a2-44d296013577 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:01:51.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5802" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":62,"skipped":1213,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:01:51.952: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:02:52.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7710" for this suite.
•{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:02:52.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:02:52.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e" in namespace "projected-5964" to be "Succeeded or Failed"
Oct 19 16:02:52.263: INFO: Pod "downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873372ms
Oct 19 16:02:54.267: INFO: Pod "downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007171109s
STEP: Saw pod success
Oct 19 16:02:54.267: INFO: Pod "downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e" satisfied condition "Succeeded or Failed"
Oct 19 16:02:54.270: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e container client-container: <nil>
STEP: delete the pod
Oct 19 16:02:54.284: INFO: Waiting for pod downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e to disappear
Oct 19 16:02:54.287: INFO: Pod downwardapi-volume-cd94b405-9b8a-4573-9c9a-9dcc8b36d29e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:02:54.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5964" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1263,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:02:54.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6732
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Oct 19 16:02:54.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:03:10.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6732" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":65,"skipped":1276,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:03:10.849: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:03:10.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-196" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":66,"skipped":1319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:03:11.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8161
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:03:11.138: INFO: Creating ReplicaSet my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4
Oct 19 16:03:11.146: INFO: Pod name my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4: Found 0 pods out of 1
Oct 19 16:03:16.153: INFO: Pod name my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4: Found 1 pods out of 1
Oct 19 16:03:16.153: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4" is running
Oct 19 16:03:16.155: INFO: Pod "my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4-ns4kp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 16:03:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 16:03:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 16:03:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-19 16:03:11 +0000 UTC Reason: Message:}])
Oct 19 16:03:16.155: INFO: Trying to dial the pod
Oct 19 16:03:21.216: INFO: Controller my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4: Got expected result from replica 1 [my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4-ns4kp]: "my-hostname-basic-42c6f717-bde6-4933-b022-64a7949df8f4-ns4kp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:03:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8161" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":67,"skipped":1361,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:03:21.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-742bf41b-8971-4852-81c2-2a38b9caa249 in namespace container-probe-110
Oct 19 16:03:23.383: INFO: Started pod test-webserver-742bf41b-8971-4852-81c2-2a38b9caa249 in namespace container-probe-110
STEP: checking the pod's current state and verifying that restartCount is present
Oct 19 16:03:23.386: INFO: Initial restart count of pod test-webserver-742bf41b-8971-4852-81c2-2a38b9caa249 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:07:23.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-110" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1367,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:07:23.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-96
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-96
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Oct 19 16:07:24.139: INFO: Found 0 stateful pods, waiting for 3
Oct 19 16:07:34.145: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 16:07:34.145: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 16:07:34.145: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Oct 19 16:07:34.173: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Oct 19 16:07:44.204: INFO: Updating stateful set ss2
Oct 19 16:07:44.220: INFO: Waiting for Pod statefulset-96/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Oct 19 16:07:54.249: INFO: Found 1 stateful pods, waiting for 3
Oct 19 16:08:04.253: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 16:08:04.253: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 16:08:04.253: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Oct 19 16:08:04.276: INFO: Updating stateful set ss2
Oct 19 16:08:04.284: INFO: Waiting for Pod statefulset-96/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 19 16:08:14.309: INFO: Updating stateful set ss2
Oct 19 16:08:14.338: INFO: Waiting for StatefulSet statefulset-96/ss2 to complete update
Oct 19 16:08:14.338: INFO: Waiting for Pod statefulset-96/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 16:08:24.346: INFO: Deleting all statefulset in ns statefulset-96
Oct 19 16:08:24.349: INFO: Scaling statefulset ss2 to 0
Oct 19 16:08:34.366: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:08:34.369: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:08:34.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-96" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":69,"skipped":1377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:08:34.392: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-3967
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3967 to expose endpoints map[]
Oct 19 16:08:34.545: INFO: successfully validated that service endpoint-test2 in namespace services-3967 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3967
Oct 19 16:08:34.556: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:08:36.560: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3967 to expose endpoints map[pod1:[80]]
Oct 19 16:08:36.575: INFO: successfully validated that service endpoint-test2 in namespace services-3967 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Oct 19 16:08:36.575: INFO: Creating new exec pod
Oct 19 16:08:39.591: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3967 exec execpod7gdpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Oct 19 16:08:39.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Oct 19 16:08:39.956: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:08:39.956: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3967 exec execpod7gdpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.58.96 80'
Oct 19 16:08:40.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.67.58.96 80\nConnection to 100.67.58.96 80 port [tcp/http] succeeded!\n"
Oct 19 16:08:40.284: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3967
Oct 19 16:08:40.295: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:08:42.299: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3967 to expose endpoints map[pod1:[80] pod2:[80]]
Oct 19 16:08:42.317: INFO: successfully validated that service endpoint-test2 in namespace services-3967 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Oct 19 16:08:43.317: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3967 exec execpod7gdpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Oct 19 16:08:43.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Oct 19 16:08:43.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:08:43.511: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3967 exec execpod7gdpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.58.96 80'
Oct 19 16:08:43.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.67.58.96 80\nConnection to 100.67.58.96 80 port [tcp/http] succeeded!\n"
Oct 19 16:08:43.771: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3967
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3967 to expose endpoints map[pod2:[80]]
Oct 19 16:08:43.794: INFO: successfully validated that service endpoint-test2 in namespace services-3967 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Oct 19 16:08:44.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3967 exec execpod7gdpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Oct 19 16:08:45.027: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Oct 19 16:08:45.027: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:08:45.027: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3967 exec execpod7gdpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.58.96 80'
Oct 19 16:08:45.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.67.58.96 80\nConnection to 100.67.58.96 80 port [tcp/http] succeeded!\n"
Oct 19 16:08:45.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3967
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3967 to expose endpoints map[]
Oct 19 16:08:45.310: INFO: successfully validated that service endpoint-test2 in namespace services-3967 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:08:45.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3967" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":70,"skipped":1465,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:08:45.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:08:45.475: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Oct 19 16:08:45.485: INFO: Number of nodes with available pods: 0
Oct 19 16:08:45.485: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 16:08:46.521: INFO: Number of nodes with available pods: 1
Oct 19 16:08:46.521: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq is running more than one daemon pod
Oct 19 16:08:47.495: INFO: Number of nodes with available pods: 2
Oct 19 16:08:47.495: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Oct 19 16:08:47.520: INFO: Wrong image for pod: daemon-set-xlmwp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 19 16:08:48.527: INFO: Wrong image for pod: daemon-set-xlmwp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 19 16:08:49.527: INFO: Wrong image for pod: daemon-set-xlmwp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 19 16:08:50.527: INFO: Pod daemon-set-fp74d is not available
Oct 19 16:08:50.527: INFO: Wrong image for pod: daemon-set-xlmwp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 19 16:08:51.527: INFO: Pod daemon-set-zx57g is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Oct 19 16:08:51.536: INFO: Number of nodes with available pods: 1
Oct 19 16:08:51.536: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 16:08:52.544: INFO: Number of nodes with available pods: 2
Oct 19 16:08:52.544: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4524, will wait for the garbage collector to delete the pods
Oct 19 16:08:52.617: INFO: Deleting DaemonSet.extensions daemon-set took: 4.032281ms
Oct 19 16:08:52.718: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.93613ms
Oct 19 16:08:55.522: INFO: Number of nodes with available pods: 0
Oct 19 16:08:55.522: INFO: Number of running nodes: 0, number of available pods: 0
Oct 19 16:08:55.525: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11211"},"items":null}

Oct 19 16:08:55.527: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11211"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:08:55.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4524" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":71,"skipped":1465,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:08:55.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5262
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5144a33b-5144-413a-88de-fb790aec12d9
STEP: Creating the pod
Oct 19 16:08:55.697: INFO: The status of Pod pod-projected-configmaps-748dc2d1-55d4-41ce-bc1d-e898551febf1 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:08:57.701: INFO: The status of Pod pod-projected-configmaps-748dc2d1-55d4-41ce-bc1d-e898551febf1 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-5144a33b-5144-413a-88de-fb790aec12d9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:10:06.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5262" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1468,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:10:06.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2371
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:10:06.285: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 19 16:10:09.170: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2371 --namespace=crd-publish-openapi-2371 create -f -'
Oct 19 16:10:09.458: INFO: stderr: ""
Oct 19 16:10:09.458: INFO: stdout: "e2e-test-crd-publish-openapi-2716-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 19 16:10:09.458: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2371 --namespace=crd-publish-openapi-2371 delete e2e-test-crd-publish-openapi-2716-crds test-cr'
Oct 19 16:10:09.522: INFO: stderr: ""
Oct 19 16:10:09.522: INFO: stdout: "e2e-test-crd-publish-openapi-2716-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Oct 19 16:10:09.522: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2371 --namespace=crd-publish-openapi-2371 apply -f -'
Oct 19 16:10:09.652: INFO: stderr: ""
Oct 19 16:10:09.652: INFO: stdout: "e2e-test-crd-publish-openapi-2716-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 19 16:10:09.652: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2371 --namespace=crd-publish-openapi-2371 delete e2e-test-crd-publish-openapi-2716-crds test-cr'
Oct 19 16:10:09.702: INFO: stderr: ""
Oct 19 16:10:09.702: INFO: stdout: "e2e-test-crd-publish-openapi-2716-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 19 16:10:09.702: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2371 explain e2e-test-crd-publish-openapi-2716-crds'
Oct 19 16:10:09.837: INFO: stderr: ""
Oct 19 16:10:09.837: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2716-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:10:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2371" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":73,"skipped":1482,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:10:13.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4717
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 19 16:10:13.340: INFO: Waiting up to 5m0s for pod "pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c" in namespace "emptydir-4717" to be "Succeeded or Failed"
Oct 19 16:10:13.343: INFO: Pod "pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.401877ms
Oct 19 16:10:15.347: INFO: Pod "pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007187738s
STEP: Saw pod success
Oct 19 16:10:15.347: INFO: Pod "pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c" satisfied condition "Succeeded or Failed"
Oct 19 16:10:15.350: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c container test-container: <nil>
STEP: delete the pod
Oct 19 16:10:15.369: INFO: Waiting for pod pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c to disappear
Oct 19 16:10:15.372: INFO: Pod pod-fa54049e-08a1-4afa-89ff-5aeda1e02f3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:10:15.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4717" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:10:15.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Oct 19 16:10:15.546: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:10:15.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2023" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":75,"skipped":1537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:10:15.571: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Oct 19 16:10:55.752: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1019 16:10:55.752353    4339 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Oct 19 16:10:55.752: INFO: Deleting pod "simpletest.rc-6kz8v" in namespace "gc-7087"
Oct 19 16:10:55.757: INFO: Deleting pod "simpletest.rc-7ttz6" in namespace "gc-7087"
Oct 19 16:10:55.764: INFO: Deleting pod "simpletest.rc-jr7ng" in namespace "gc-7087"
Oct 19 16:10:55.777: INFO: Deleting pod "simpletest.rc-kd5ws" in namespace "gc-7087"
Oct 19 16:10:55.783: INFO: Deleting pod "simpletest.rc-kg9xb" in namespace "gc-7087"
Oct 19 16:10:55.787: INFO: Deleting pod "simpletest.rc-l2d2b" in namespace "gc-7087"
Oct 19 16:10:55.794: INFO: Deleting pod "simpletest.rc-sz24q" in namespace "gc-7087"
Oct 19 16:10:55.799: INFO: Deleting pod "simpletest.rc-vgjx8" in namespace "gc-7087"
Oct 19 16:10:55.804: INFO: Deleting pod "simpletest.rc-vhl9r" in namespace "gc-7087"
Oct 19 16:10:55.811: INFO: Deleting pod "simpletest.rc-w7rjm" in namespace "gc-7087"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:10:55.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7087" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":76,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:10:55.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Oct 19 16:10:57.002: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1019 16:10:57.002323    4339 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:10:57.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1752" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":77,"skipped":1562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:10:57.009: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7437
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:10:57.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:11:00.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7437" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":78,"skipped":1613,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:11:00.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-078efbee-ae7f-44cf-8873-e5e192116c7e
STEP: Creating a pod to test consume secrets
Oct 19 16:11:00.401: INFO: Waiting up to 5m0s for pod "pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63" in namespace "secrets-4305" to be "Succeeded or Failed"
Oct 19 16:11:00.404: INFO: Pod "pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369304ms
Oct 19 16:11:02.409: INFO: Pod "pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008435305s
STEP: Saw pod success
Oct 19 16:11:02.409: INFO: Pod "pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63" satisfied condition "Succeeded or Failed"
Oct 19 16:11:02.412: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63 container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:11:02.427: INFO: Waiting for pod pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63 to disappear
Oct 19 16:11:02.430: INFO: Pod pod-secrets-734590e6-601e-48c8-8b4f-612d3cf90d63 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:11:02.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4305" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1618,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:11:02.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 19 16:11:02.585: INFO: The status of Pod annotationupdate19ca8a3f-655a-4a3a-85ad-78f453970172 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:11:04.589: INFO: The status of Pod annotationupdate19ca8a3f-655a-4a3a-85ad-78f453970172 is Running (Ready = true)
Oct 19 16:11:05.110: INFO: Successfully updated pod "annotationupdate19ca8a3f-655a-4a3a-85ad-78f453970172"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:11:09.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9255" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":80,"skipped":1632,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:11:09.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-7e8d1546-7059-4cb0-ab95-3d9b0c7154b0-7497
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:11:09.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1457" for this suite.
STEP: Destroying namespace "nspatchtest-7e8d1546-7059-4cb0-ab95-3d9b0c7154b0-7497" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":81,"skipped":1650,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:11:09.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-15b45f0f-cd80-47e1-a550-7e8b3143b063
STEP: Creating a pod to test consume configMaps
Oct 19 16:11:09.637: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80" in namespace "projected-368" to be "Succeeded or Failed"
Oct 19 16:11:09.641: INFO: Pod "pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.653691ms
Oct 19 16:11:11.645: INFO: Pod "pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008126451s
STEP: Saw pod success
Oct 19 16:11:11.645: INFO: Pod "pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80" satisfied condition "Succeeded or Failed"
Oct 19 16:11:11.648: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:11:11.663: INFO: Waiting for pod pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80 to disappear
Oct 19 16:11:11.666: INFO: Pod pod-projected-configmaps-0e609d02-ea7c-4ba4-87a8-f05fb5606d80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:11:11.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-368" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1650,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:11:11.675: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-1be76eb0-a256-488e-a448-610fb49a9146 in namespace container-probe-7016
Oct 19 16:11:13.829: INFO: Started pod liveness-1be76eb0-a256-488e-a448-610fb49a9146 in namespace container-probe-7016
STEP: checking the pod's current state and verifying that restartCount is present
Oct 19 16:11:13.832: INFO: Initial restart count of pod liveness-1be76eb0-a256-488e-a448-610fb49a9146 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:14.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7016" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:14.450: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 19 16:15:16.622: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:16.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7818" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:16.640: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:15:16.779: INFO: Creating deployment "webserver-deployment"
Oct 19 16:15:16.783: INFO: Waiting for observed generation 1
Oct 19 16:15:18.792: INFO: Waiting for all required pods to come up
Oct 19 16:15:18.809: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Oct 19 16:15:18.809: INFO: Waiting for deployment "webserver-deployment" to complete
Oct 19 16:15:18.817: INFO: Updating deployment "webserver-deployment" with a non-existent image
Oct 19 16:15:18.825: INFO: Updating deployment webserver-deployment
Oct 19 16:15:18.825: INFO: Waiting for observed generation 2
Oct 19 16:15:20.833: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Oct 19 16:15:20.836: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Oct 19 16:15:20.839: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 19 16:15:20.848: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Oct 19 16:15:20.848: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Oct 19 16:15:20.851: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 19 16:15:20.856: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Oct 19 16:15:20.856: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Oct 19 16:15:20.864: INFO: Updating deployment webserver-deployment
Oct 19 16:15:20.864: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Oct 19 16:15:20.872: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Oct 19 16:15:22.879: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 16:15:22.885: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8070  ea1c7b1a-7110-486c-b47c-1a3114865a71 13646 3 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002690b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-10-19 16:15:20 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-10-19 16:15:22 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Oct 19 16:15:22.889: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8070  fe28459c-6d5f-478a-86d5-03f09b170ae8 13603 3 2021-10-19 16:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ea1c7b1a-7110-486c-b47c-1a3114865a71 0xc002690f37 0xc002690f38}] []  [{kube-controller-manager Update apps/v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea1c7b1a-7110-486c-b47c-1a3114865a71\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002690fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 19 16:15:22.889: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Oct 19 16:15:22.889: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-8070  797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 13645 3 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ea1c7b1a-7110-486c-b47c-1a3114865a71 0xc002691037 0xc002691038}] []  [{kube-controller-manager Update apps/v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea1c7b1a-7110-486c-b47c-1a3114865a71\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026910c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Oct 19 16:15:22.901: INFO: Pod "webserver-deployment-795d758f88-2lw7l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2lw7l webserver-deployment-795d758f88- deployment-8070  49345f3d-8aea-4491-a0ba-eabe1c7a0352 13606 0 2021-10-19 16:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.108/32 cni.projectcalico.org/podIPs:100.96.0.108/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00247b937 0xc00247b938}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlt4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlt4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.108,StartTime:2021-10-19 16:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.901: INFO: Pod "webserver-deployment-795d758f88-49mbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-49mbp webserver-deployment-795d758f88- deployment-8070  bb54c5aa-5181-4ca1-b9fb-df66d2bd6bc7 13611 0 2021-10-19 16:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.46/32 cni.projectcalico.org/podIPs:100.96.1.46/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00247bb97 0xc00247bb98}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5v95c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5v95c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.46,StartTime:2021-10-19 16:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.901: INFO: Pod "webserver-deployment-795d758f88-4dmnw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4dmnw webserver-deployment-795d758f88- deployment-8070  2698b1f3-e6b2-49ab-88d1-d319f21c882e 13619 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.112/32 cni.projectcalico.org/podIPs:100.96.0.112/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00247bde0 0xc00247bde1}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppb9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppb9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.901: INFO: Pod "webserver-deployment-795d758f88-4t4cw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4t4cw webserver-deployment-795d758f88- deployment-8070  e64d884c-f9b6-45bc-bff1-e76e695f14d7 13634 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.54/32 cni.projectcalico.org/podIPs:100.96.1.54/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089a077 0xc00089a078}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-962h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-962h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.901: INFO: Pod "webserver-deployment-795d758f88-9qktw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9qktw webserver-deployment-795d758f88- deployment-8070  f5070fff-0f7c-4aa1-8157-181db46e5916 13521 0 2021-10-19 16:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.106/32 cni.projectcalico.org/podIPs:100.96.0.106/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089a2c7 0xc00089a2c8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2krk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2krk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.902: INFO: Pod "webserver-deployment-795d758f88-gmnfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gmnfw webserver-deployment-795d758f88- deployment-8070  528c8dbd-6da8-453b-a02a-5e982645c8a8 13625 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.51/32 cni.projectcalico.org/podIPs:100.96.1.51/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089a4e7 0xc00089a4e8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kthcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kthcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.902: INFO: Pod "webserver-deployment-795d758f88-gzkmk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gzkmk webserver-deployment-795d758f88- deployment-8070  9bede543-005e-4dad-b9b5-e614924a1af0 13613 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.48/32 cni.projectcalico.org/podIPs:100.96.1.48/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089a727 0xc00089a728}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgmjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgmjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.902: INFO: Pod "webserver-deployment-795d758f88-ntsrc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ntsrc webserver-deployment-795d758f88- deployment-8070  4b1e7fba-689e-4a77-b3ff-68360bcc70f9 13626 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.114/32 cni.projectcalico.org/podIPs:100.96.0.114/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089a957 0xc00089a958}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hrm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hrm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.902: INFO: Pod "webserver-deployment-795d758f88-nxb2n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nxb2n webserver-deployment-795d758f88- deployment-8070  51f8340b-dd6d-455b-80af-408c2828876f 13617 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.111/32 cni.projectcalico.org/podIPs:100.96.0.111/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089ab87 0xc00089ab88}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtz87,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtz87,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.902: INFO: Pod "webserver-deployment-795d758f88-rpkwl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rpkwl webserver-deployment-795d758f88- deployment-8070  92f0fae1-2c08-4624-878b-650d3f97e0fc 13637 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.58/32 cni.projectcalico.org/podIPs:100.96.1.58/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089adc7 0xc00089adc8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njxmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njxmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.903: INFO: Pod "webserver-deployment-795d758f88-vjqjm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vjqjm webserver-deployment-795d758f88- deployment-8070  9a34b82e-a6c8-4df3-9030-952046050b4e 13627 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.52/32 cni.projectcalico.org/podIPs:100.96.1.52/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089b007 0xc00089b008}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrcpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrcpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.903: INFO: Pod "webserver-deployment-795d758f88-xm9s8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xm9s8 webserver-deployment-795d758f88- deployment-8070  3f2ce604-1ff3-4903-a115-f647e56ce493 13520 0 2021-10-19 16:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.47/32 cni.projectcalico.org/podIPs:100.96.1.47/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089b277 0xc00089b278}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5kjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5kjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.903: INFO: Pod "webserver-deployment-795d758f88-xmvrq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xmvrq webserver-deployment-795d758f88- deployment-8070  f8d20d8e-7291-4c3c-8b7d-be8b1e13379d 13523 0 2021-10-19 16:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.107/32 cni.projectcalico.org/podIPs:100.96.0.107/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fe28459c-6d5f-478a-86d5-03f09b170ae8 0xc00089b497 0xc00089b498}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe28459c-6d5f-478a-86d5-03f09b170ae8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7smmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7smmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.903: INFO: Pod "webserver-deployment-847dcfb7fb-2dwqn" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2dwqn webserver-deployment-847dcfb7fb- deployment-8070  1ee8dfbf-0c3c-4a4a-a01b-ac82e43a55dd 13473 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.42/32 cni.projectcalico.org/podIPs:100.96.1.42/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc00089b6b7 0xc00089b6b8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnr9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnr9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.42,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://4a2ed67f64378333b490679c75042430a92bc8713a068d007d40ef77ddc37c03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.903: INFO: Pod "webserver-deployment-847dcfb7fb-2g2g5" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2g2g5 webserver-deployment-847dcfb7fb- deployment-8070  3fa0de4f-d29c-4c04-b017-7d7c0722b3d4 13476 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.43/32 cni.projectcalico.org/podIPs:100.96.1.43/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc00089b8d0 0xc00089b8d1}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nz45h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nz45h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.43,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://be30c01064aee952371761e14c98110ec26a6de873f91b63e24fc63c36e4e194,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.903: INFO: Pod "webserver-deployment-847dcfb7fb-6vvqm" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6vvqm webserver-deployment-847dcfb7fb- deployment-8070  631e45b9-1a0e-4427-a6f2-4ab2c9508067 13618 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.50/32 cni.projectcalico.org/podIPs:100.96.1.50/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc00089bb00 0xc00089bb01}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r78t2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r78t2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.904: INFO: Pod "webserver-deployment-847dcfb7fb-6z6xx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6z6xx webserver-deployment-847dcfb7fb- deployment-8070  9fe900ae-76fb-4f7e-8e89-1ed6749a96bc 13644 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.49/32 cni.projectcalico.org/podIPs:100.96.1.49/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc00089bee7 0xc00089bee8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m8sb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m8sb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.49,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://341b3bfc1c65ee4917e1fcc3d95e5fc09566fd2c6091e3402bdf7255f5634a73,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.904: INFO: Pod "webserver-deployment-847dcfb7fb-72dw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-72dw2 webserver-deployment-847dcfb7fb- deployment-8070  30cf56d0-0eb9-4846-b22e-89dc10a0469b 13642 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.57/32 cni.projectcalico.org/podIPs:100.96.1.57/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf21c0 0xc003bf21c1}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7lhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7lhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.904: INFO: Pod "webserver-deployment-847dcfb7fb-7vv4w" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7vv4w webserver-deployment-847dcfb7fb- deployment-8070  0db3399e-66ab-49a9-987a-0273aab8cce1 13632 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.117/32 cni.projectcalico.org/podIPs:100.96.0.117/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf23c7 0xc003bf23c8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nw7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nw7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.904: INFO: Pod "webserver-deployment-847dcfb7fb-8vh6f" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8vh6f webserver-deployment-847dcfb7fb- deployment-8070  22c72cf9-a476-4d96-8a7d-1191e6842450 13467 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.103/32 cni.projectcalico.org/podIPs:100.96.0.103/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf25c7 0xc003bf25c8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbwps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbwps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.103,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://92ac06543ae06c42a1a49200c5c9152f76162a2a7338248f1c162e9a21943342,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.904: INFO: Pod "webserver-deployment-847dcfb7fb-c254z" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-c254z webserver-deployment-847dcfb7fb- deployment-8070  8ab6b16d-e69b-4e0c-a312-87dfe7637773 13470 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.44/32 cni.projectcalico.org/podIPs:100.96.1.44/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf27f7 0xc003bf27f8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dmrnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dmrnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.44,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://47285d7430da378ab49ed949f11e26f91d6c08fa7c872683e2f08cda4c0d7da2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.904: INFO: Pod "webserver-deployment-847dcfb7fb-cj9c7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cj9c7 webserver-deployment-847dcfb7fb- deployment-8070  c2da65ef-0bc2-4679-b0f3-298a776d6570 13635 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.55/32 cni.projectcalico.org/podIPs:100.96.1.55/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf2a30 0xc003bf2a31}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-czwn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-czwn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-cvsx7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cvsx7 webserver-deployment-847dcfb7fb- deployment-8070  9e7d53e3-a0cc-40cd-bac0-8f751432b57d 13612 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.109/32 cni.projectcalico.org/podIPs:100.96.0.109/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf2c37 0xc003bf2c38}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzgc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzgc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-l6txt" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l6txt webserver-deployment-847dcfb7fb- deployment-8070  847559ee-e61c-4268-a59d-c3b4fa2bb4bd 13631 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.116/32 cni.projectcalico.org/podIPs:100.96.0.116/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf2e57 0xc003bf2e58}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8pch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8pch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-lxt7r" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lxt7r webserver-deployment-847dcfb7fb- deployment-8070  f8c4f00c-87b4-4c71-8a68-1221d99b6d14 13622 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.113/32 cni.projectcalico.org/podIPs:100.96.0.113/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3057 0xc003bf3058}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4k9zm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4k9zm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-mfzcn" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mfzcn webserver-deployment-847dcfb7fb- deployment-8070  e6bd24ce-2251-43c3-ae0c-860d2a66c8b8 13479 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.45/32 cni.projectcalico.org/podIPs:100.96.1.45/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3257 0xc003bf3258}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rncsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rncsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:100.96.1.45,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://77a496b0ad805dfd9a0c30cbdfcc01f2e817471821f72452b8ff52c2786b0853,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-n8vsx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-n8vsx webserver-deployment-847dcfb7fb- deployment-8070  cd1a4cfa-774a-438e-b3bc-64e5d49b8f0d 13614 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.110/32 cni.projectcalico.org/podIPs:100.96.0.110/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3470 0xc003bf3471}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8cns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8cns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-r6sh6" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-r6sh6 webserver-deployment-847dcfb7fb- deployment-8070  579d9e5b-e0fc-4c28-b894-2c73cc497763 13633 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.53/32 cni.projectcalico.org/podIPs:100.96.1.53/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3667 0xc003bf3668}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdhnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdhnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.905: INFO: Pod "webserver-deployment-847dcfb7fb-s4vbh" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-s4vbh webserver-deployment-847dcfb7fb- deployment-8070  b8cb56f8-19bf-47ca-a932-a6788ce6b5e6 13464 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.102/32 cni.projectcalico.org/podIPs:100.96.0.102/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3867 0xc003bf3868}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjcnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjcnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.102,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b9e944adf79e92bacc8e4cdd777fd878183ff8f2713cfeda8b3af212ef474a96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.906: INFO: Pod "webserver-deployment-847dcfb7fb-s8kh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-s8kh4 webserver-deployment-847dcfb7fb- deployment-8070  7302d6b3-9e7c-4282-8d11-d50b8d77d83d 13630 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.115/32 cni.projectcalico.org/podIPs:100.96.0.115/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3a87 0xc003bf3a88}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cd76t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cd76t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 16:15:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.906: INFO: Pod "webserver-deployment-847dcfb7fb-sbxh6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-sbxh6 webserver-deployment-847dcfb7fb- deployment-8070  29cdce43-30fe-4322-894b-eb550e060c2c 13458 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.100/32 cni.projectcalico.org/podIPs:100.96.0.100/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3c87 0xc003bf3c88}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhxcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhxcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.100,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://89aedcf86d4ec09cacff12d1635cc40881c2f5e918b48a1ab5f6e10dd4266f40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.906: INFO: Pod "webserver-deployment-847dcfb7fb-v277m" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-v277m webserver-deployment-847dcfb7fb- deployment-8070  e9c59f1d-601f-4a21-9ecc-fa54fd7f4fe7 13461 0 2021-10-19 16:15:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.105/32 cni.projectcalico.org/podIPs:100.96.0.105/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc003bf3ea7 0xc003bf3ea8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqp8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqp8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.105,StartTime:2021-10-19 16:15:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:15:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://9bde552a63756a86f63bbe43bb02e8da42f94fc130a357082d4837b56a3169af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 16:15:22.906: INFO: Pod "webserver-deployment-847dcfb7fb-vnlgx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vnlgx webserver-deployment-847dcfb7fb- deployment-8070  9c70b89d-922b-465e-ab1c-e75045960799 13636 0 2021-10-19 16:15:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.56/32 cni.projectcalico.org/podIPs:100.96.1.56/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 797c3ade-b8c8-4c9c-a977-86a6f9dce2ef 0xc0041c40c7 0xc0041c40c8}] []  [{kube-controller-manager Update v1 2021-10-19 16:15:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797c3ade-b8c8-4c9c-a977-86a6f9dce2ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 16:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-10-19 16:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9c7f4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9c7f4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:15:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.3.120,PodIP:,StartTime:2021-10-19 16:15:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:22.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8070" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":85,"skipped":1736,"failed":0}

------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:22.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-564
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Oct 19 16:15:23.068: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Oct 19 16:15:23.086: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:23.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-564" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":86,"skipped":1736,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:23.109: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5755
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5755
I1019 16:15:23.265524    4339 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5755, replica count: 2
I1019 16:15:26.317331    4339 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 16:15:26.317: INFO: Creating new exec pod
Oct 19 16:15:29.333: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5755 exec execpodtbnnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Oct 19 16:15:29.753: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 19 16:15:29.753: INFO: stdout: "externalname-service-6gjlv"
Oct 19 16:15:29.753: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5755 exec execpodtbnnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.41.174 80'
Oct 19 16:15:29.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.41.174 80\nConnection to 100.71.41.174 80 port [tcp/http] succeeded!\n"
Oct 19 16:15:29.942: INFO: stdout: ""
Oct 19 16:15:30.943: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5755 exec execpodtbnnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.41.174 80'
Oct 19 16:15:31.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.41.174 80\nConnection to 100.71.41.174 80 port [tcp/http] succeeded!\n"
Oct 19 16:15:31.146: INFO: stdout: ""
Oct 19 16:15:31.942: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5755 exec execpodtbnnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.41.174 80'
Oct 19 16:15:32.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.41.174 80\nConnection to 100.71.41.174 80 port [tcp/http] succeeded!\n"
Oct 19 16:15:32.165: INFO: stdout: "externalname-service-6gjlv"
Oct 19 16:15:32.165: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:32.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5755" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":87,"skipped":1739,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:32.183: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-7878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 19 16:15:32.340: INFO: starting watch
STEP: patching
STEP: updating
Oct 19 16:15:32.349: INFO: waiting for watch events with expected annotations
Oct 19 16:15:32.349: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:32.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7878" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":88,"skipped":1741,"failed":0}

------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:32.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-1680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:32.516: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-5668
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1680
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:32.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5668" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:32.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1680" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":89,"skipped":1741,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:32.705: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Oct 19 16:15:32.846: INFO: Pod name pod-release: Found 0 pods out of 1
Oct 19 16:15:37.873: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-611" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":90,"skipped":1747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:38.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9348
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 19 16:15:39.191: INFO: Number of nodes with available pods: 0
Oct 19 16:15:39.191: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 16:15:40.202: INFO: Number of nodes with available pods: 0
Oct 19 16:15:40.202: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 16:15:41.201: INFO: Number of nodes with available pods: 2
Oct 19 16:15:41.201: INFO: Number of running nodes: 2, number of available pods: 2
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Oct 19 16:15:41.223: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14090"},"items":null}

Oct 19 16:15:41.226: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14090"},"items":[{"metadata":{"name":"daemon-set-78gff","generateName":"daemon-set-","namespace":"daemonsets-9348","uid":"cffbd129-ce51-4403-a154-57f634358af5","resourceVersion":"14090","creationTimestamp":"2021-10-19T16:15:39Z","deletionTimestamp":"2021-10-19T16:16:11Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"100.96.1.60/32","cni.projectcalico.org/podIPs":"100.96.1.60/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6f7b1b4c-f700-41b7-a5df-bfc3367aba87","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-10-19T16:15:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-10-19T16:15:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f7b1b4c-f700-41b7-a5df-bfc3367aba87\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-10-19T16:15:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fb6tg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmhay-ddd.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fb6tg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:39Z"}],"hostIP":"10.250.3.120","podIP":"100.96.1.60","podIPs":[{"ip":"100.96.1.60"}],"startTime":"2021-10-19T16:15:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-10-19T16:15:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://12926fe5edb20854216836b8ae92bd867f553534bb566190a891795890d4dc87","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lwrbw","generateName":"daemon-set-","namespace":"daemonsets-9348","uid":"4c0f7ddb-c1a5-4ac0-b3de-db73ea5197fd","resourceVersion":"14089","creationTimestamp":"2021-10-19T16:15:39Z","deletionTimestamp":"2021-10-19T16:16:11Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"100.96.0.122/32","cni.projectcalico.org/podIPs":"100.96.0.122/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6f7b1b4c-f700-41b7-a5df-bfc3367aba87","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-10-19T16:15:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-10-19T16:15:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f7b1b4c-f700-41b7-a5df-bfc3367aba87\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-10-19T16:15:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hrtdm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"env":[{"name":"KUBERNETES_SERVICE_HOST","value":"api.tmhay-ddd.it.internal.staging.k8s.ondemand.com"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hrtdm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-10-19T16:15:39Z"}],"hostIP":"10.250.1.123","podIP":"100.96.0.122","podIPs":[{"ip":"100.96.0.122"}],"startTime":"2021-10-19T16:15:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-10-19T16:15:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://3c6c2a3e64489c71aee7847436bec06488f38b819acd08fc52799568c16ab184","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:41.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9348" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":91,"skipped":1774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:41.244: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Oct 19 16:15:41.393: INFO: Waiting up to 5m0s for pod "test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b" in namespace "svcaccounts-6479" to be "Succeeded or Failed"
Oct 19 16:15:41.396: INFO: Pod "test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981361ms
Oct 19 16:15:43.399: INFO: Pod "test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006399381s
STEP: Saw pod success
Oct 19 16:15:43.399: INFO: Pod "test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b" satisfied condition "Succeeded or Failed"
Oct 19 16:15:43.403: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:15:43.420: INFO: Waiting for pod test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b to disappear
Oct 19 16:15:43.423: INFO: Pod test-pod-1a381834-48f5-4a00-9856-343ddb3d2c8b no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:43.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6479" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":92,"skipped":1801,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:43.449: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 19 16:15:43.602: INFO: The status of Pod pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:15:45.606: INFO: The status of Pod pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 19 16:15:46.124: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2"
Oct 19 16:15:46.125: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2" in namespace "pods-3177" to be "terminated due to deadline exceeded"
Oct 19 16:15:46.128: INFO: Pod "pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2": Phase="Running", Reason="", readiness=true. Elapsed: 3.076037ms
Oct 19 16:15:48.132: INFO: Pod "pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007624034s
Oct 19 16:15:50.137: INFO: Pod "pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.012960866s
Oct 19 16:15:50.138: INFO: Pod "pod-update-activedeadlineseconds-c173f2b4-383e-49c8-9a3b-c4a2c5b034c2" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:15:50.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3177" for this suite.
•{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":93,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:15:50.149: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:16:03.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5331" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":94,"skipped":1835,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:16:03.436: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 19 16:16:03.586: INFO: Waiting up to 5m0s for pod "pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340" in namespace "emptydir-2527" to be "Succeeded or Failed"
Oct 19 16:16:03.589: INFO: Pod "pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868913ms
Oct 19 16:16:05.600: INFO: Pod "pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013727563s
STEP: Saw pod success
Oct 19 16:16:05.600: INFO: Pod "pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340" satisfied condition "Succeeded or Failed"
Oct 19 16:16:05.603: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340 container test-container: <nil>
STEP: delete the pod
Oct 19 16:16:05.616: INFO: Waiting for pod pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340 to disappear
Oct 19 16:16:05.619: INFO: Pod pod-1fc55140-17c1-4a83-9e6c-3b01f8fa3340 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:16:05.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2527" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:16:05.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-4nw6
STEP: Creating a pod to test atomic-volume-subpath
Oct 19 16:16:05.780: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4nw6" in namespace "subpath-4038" to be "Succeeded or Failed"
Oct 19 16:16:05.784: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.973268ms
Oct 19 16:16:07.790: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010566318s
Oct 19 16:16:09.795: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 4.015358235s
Oct 19 16:16:11.799: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 6.019409958s
Oct 19 16:16:13.804: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 8.02440109s
Oct 19 16:16:15.811: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 10.031461215s
Oct 19 16:16:17.815: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 12.035223941s
Oct 19 16:16:19.820: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 14.040094767s
Oct 19 16:16:21.824: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 16.044189895s
Oct 19 16:16:23.829: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 18.049145206s
Oct 19 16:16:25.834: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Running", Reason="", readiness=true. Elapsed: 20.054148213s
Oct 19 16:16:27.838: INFO: Pod "pod-subpath-test-configmap-4nw6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.058088739s
STEP: Saw pod success
Oct 19 16:16:27.838: INFO: Pod "pod-subpath-test-configmap-4nw6" satisfied condition "Succeeded or Failed"
Oct 19 16:16:27.841: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-subpath-test-configmap-4nw6 container test-container-subpath-configmap-4nw6: <nil>
STEP: delete the pod
Oct 19 16:16:27.854: INFO: Waiting for pod pod-subpath-test-configmap-4nw6 to disappear
Oct 19 16:16:27.857: INFO: Pod pod-subpath-test-configmap-4nw6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4nw6
Oct 19 16:16:27.857: INFO: Deleting pod "pod-subpath-test-configmap-4nw6" in namespace "subpath-4038"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:16:27.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4038" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":96,"skipped":1888,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:16:27.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:16:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4708" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":97,"skipped":1893,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:16:39.057: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:16:39.206: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1" in namespace "projected-307" to be "Succeeded or Failed"
Oct 19 16:16:39.209: INFO: Pod "downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197454ms
Oct 19 16:16:41.214: INFO: Pod "downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008397282s
STEP: Saw pod success
Oct 19 16:16:41.214: INFO: Pod "downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1" satisfied condition "Succeeded or Failed"
Oct 19 16:16:41.217: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1 container client-container: <nil>
STEP: delete the pod
Oct 19 16:16:41.232: INFO: Waiting for pod downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1 to disappear
Oct 19 16:16:41.234: INFO: Pod downwardapi-volume-a49c891c-6e9d-4a0a-9ef8-5699be7de4f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:16:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-307" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1901,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:16:41.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3301
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 19 16:16:41.381: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 19 16:16:41.411: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:16:43.415: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:16:45.415: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:16:47.415: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:16:49.416: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:16:51.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:16:53.415: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 19 16:16:53.422: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 19 16:16:55.443: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 19 16:16:57.427: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 19 16:16:59.428: INFO: The status of Pod netserver-1 is Running (Ready = false)
Oct 19 16:17:01.431: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 19 16:17:03.486: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 19 16:17:03.486: INFO: Going to poll 100.96.0.128 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Oct 19 16:17:03.490: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.128 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3301 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:17:03.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:17:04.650: INFO: Found all 1 expected endpoints: [netserver-0]
Oct 19 16:17:04.651: INFO: Going to poll 100.96.1.61 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Oct 19 16:17:04.655: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3301 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:17:04.655: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:17:05.837: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:17:05.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3301" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1907,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:17:05.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-509/secret-test-cc24f048-6776-4cec-af65-0a850d67462c
STEP: Creating a pod to test consume secrets
Oct 19 16:17:05.992: INFO: Waiting up to 5m0s for pod "pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5" in namespace "secrets-509" to be "Succeeded or Failed"
Oct 19 16:17:05.996: INFO: Pod "pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.921756ms
Oct 19 16:17:08.001: INFO: Pod "pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009037773s
STEP: Saw pod success
Oct 19 16:17:08.001: INFO: Pod "pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5" satisfied condition "Succeeded or Failed"
Oct 19 16:17:08.004: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5 container env-test: <nil>
STEP: delete the pod
Oct 19 16:17:08.023: INFO: Waiting for pod pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5 to disappear
Oct 19 16:17:08.027: INFO: Pod pod-configmaps-96e05223-2137-47b4-9306-5eaa55be39a5 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:17:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-509" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1913,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:17:08.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:17:08.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0" in namespace "downward-api-9588" to be "Succeeded or Failed"
Oct 19 16:17:08.217: INFO: Pod "downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971382ms
Oct 19 16:17:10.276: INFO: Pod "downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.061814201s
STEP: Saw pod success
Oct 19 16:17:10.276: INFO: Pod "downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0" satisfied condition "Succeeded or Failed"
Oct 19 16:17:10.281: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0 container client-container: <nil>
STEP: delete the pod
Oct 19 16:17:10.381: INFO: Waiting for pod downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0 to disappear
Oct 19 16:17:10.384: INFO: Pod downwardapi-volume-b3c0cf5c-e89f-4321-876f-33812f4f92f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:17:10.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9588" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1919,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:17:10.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Oct 19 16:19:11.144: INFO: Successfully updated pod "var-expansion-1c25ef9b-90e9-412c-b706-930d084a8f19"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Oct 19 16:19:13.151: INFO: Deleting pod "var-expansion-1c25ef9b-90e9-412c-b706-930d084a8f19" in namespace "var-expansion-6263"
Oct 19 16:19:13.156: INFO: Wait up to 5m0s for pod "var-expansion-1c25ef9b-90e9-412c-b706-930d084a8f19" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:19:45.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6263" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":102,"skipped":1920,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:19:45.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Oct 19 16:19:45.315: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 19 16:20:45.350: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:20:45.354: INFO: Starting informer...
STEP: Starting pod...
Oct 19 16:20:45.578: INFO: Pod is running on shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Oct 19 16:20:45.592: INFO: Pod wasn't evicted. Proceeding
Oct 19 16:20:45.592: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Oct 19 16:22:00.606: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:00.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3137" for this suite.
•{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":103,"skipped":1946,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:00.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:22:00.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 19 16:22:00.775: INFO: The status of Pod pod-logs-websocket-8d96bc79-26a6-4e3c-9a4e-9c9d41b56856 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:22:02.780: INFO: The status of Pod pod-logs-websocket-8d96bc79-26a6-4e3c-9a4e-9c9d41b56856 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:02.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4309" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:02.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:03.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3580" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":105,"skipped":1983,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:03.033: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:05.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2982" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":106,"skipped":1998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:05.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-3331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Oct 19 16:22:25.448: INFO: EndpointSlice for Service endpointslice-3331/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:35.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3331" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":107,"skipped":2059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:35.469: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-3400
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:35.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3400" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":108,"skipped":2110,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:35.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-27e7891e-25e3-4ef9-9281-b88470a78acd
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:35.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1171" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":109,"skipped":2121,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:35.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:40.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5802" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":110,"skipped":2133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:40.458: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:51.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4608" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":111,"skipped":2210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:51.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Oct 19 16:22:51.874: INFO: Waiting up to 5m0s for pod "client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d" in namespace "containers-538" to be "Succeeded or Failed"
Oct 19 16:22:51.877: INFO: Pod "client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.238864ms
Oct 19 16:22:53.882: INFO: Pod "client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008012558s
STEP: Saw pod success
Oct 19 16:22:53.882: INFO: Pod "client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d" satisfied condition "Succeeded or Failed"
Oct 19 16:22:53.885: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:22:53.899: INFO: Waiting for pod client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d to disappear
Oct 19 16:22:53.902: INFO: Pod client-containers-fd070c9d-d771-42e4-a70f-13a4bb1ca62d no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:22:53.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-538" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2245,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:22:53.912: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:22:54.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:22:57.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:23:07.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2391" for this suite.
STEP: Destroying namespace "webhook-2391-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":113,"skipped":2253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:23:08.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-1003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:28:08.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1003" for this suite.

• [SLOW TEST:300.224 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":114,"skipped":2302,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:28:08.240: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 19 16:28:08.391: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 19 16:29:08.428: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:29:08.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Oct 19 16:29:10.618: INFO: found a healthy node: shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:29:24.676: INFO: pods created so far: [1 1 1]
Oct 19 16:29:24.677: INFO: length of pods created so far: 3
Oct 19 16:29:26.689: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:29:33.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4300" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:29:33.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9316" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":115,"skipped":2323,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:29:33.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 19 16:29:33.924: INFO: Waiting up to 5m0s for pod "pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c" in namespace "emptydir-5631" to be "Succeeded or Failed"
Oct 19 16:29:33.927: INFO: Pod "pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024603ms
Oct 19 16:29:35.933: INFO: Pod "pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008279859s
STEP: Saw pod success
Oct 19 16:29:35.933: INFO: Pod "pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c" satisfied condition "Succeeded or Failed"
Oct 19 16:29:35.936: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c container test-container: <nil>
STEP: delete the pod
Oct 19 16:29:35.994: INFO: Waiting for pod pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c to disappear
Oct 19 16:29:35.997: INFO: Pod pod-ca2d8e51-0465-44a1-aaba-190e6f81d98c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:29:35.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5631" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2334,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:29:36.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7636/configmap-test-e2980268-9733-4daa-b746-39effce51aa6
STEP: Creating a pod to test consume configMaps
Oct 19 16:29:36.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957" in namespace "configmap-7636" to be "Succeeded or Failed"
Oct 19 16:29:36.158: INFO: Pod "pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095508ms
Oct 19 16:29:38.162: INFO: Pod "pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00708998s
STEP: Saw pod success
Oct 19 16:29:38.162: INFO: Pod "pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957" satisfied condition "Succeeded or Failed"
Oct 19 16:29:38.166: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957 container env-test: <nil>
STEP: delete the pod
Oct 19 16:29:38.180: INFO: Waiting for pod pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957 to disappear
Oct 19 16:29:38.183: INFO: Pod pod-configmaps-d25fb005-0118-41d0-ae22-f36cd8364957 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:29:38.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7636" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2350,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:29:38.193: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Oct 19 16:29:38.335: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 create -f -'
Oct 19 16:29:38.611: INFO: stderr: ""
Oct 19 16:29:38.611: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 19 16:29:38.611: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 16:29:38.660: INFO: stderr: ""
Oct 19 16:29:38.660: INFO: stdout: "update-demo-nautilus-hz6qh update-demo-nautilus-mb2wl "
Oct 19 16:29:38.660: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods update-demo-nautilus-hz6qh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 16:29:38.707: INFO: stderr: ""
Oct 19 16:29:38.707: INFO: stdout: ""
Oct 19 16:29:38.707: INFO: update-demo-nautilus-hz6qh is created but not running
Oct 19 16:29:43.708: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 19 16:29:43.757: INFO: stderr: ""
Oct 19 16:29:43.758: INFO: stdout: "update-demo-nautilus-hz6qh update-demo-nautilus-mb2wl "
Oct 19 16:29:43.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods update-demo-nautilus-hz6qh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 16:29:43.804: INFO: stderr: ""
Oct 19 16:29:43.804: INFO: stdout: "true"
Oct 19 16:29:43.804: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods update-demo-nautilus-hz6qh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 16:29:43.850: INFO: stderr: ""
Oct 19 16:29:43.850: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 16:29:43.850: INFO: validating pod update-demo-nautilus-hz6qh
Oct 19 16:29:43.908: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 16:29:43.908: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 16:29:43.908: INFO: update-demo-nautilus-hz6qh is verified up and running
Oct 19 16:29:43.909: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods update-demo-nautilus-mb2wl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 19 16:29:43.958: INFO: stderr: ""
Oct 19 16:29:43.958: INFO: stdout: "true"
Oct 19 16:29:43.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods update-demo-nautilus-mb2wl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 19 16:29:44.010: INFO: stderr: ""
Oct 19 16:29:44.010: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 19 16:29:44.010: INFO: validating pod update-demo-nautilus-mb2wl
Oct 19 16:29:44.066: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 19 16:29:44.066: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 19 16:29:44.066: INFO: update-demo-nautilus-mb2wl is verified up and running
STEP: using delete to clean up resources
Oct 19 16:29:44.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 delete --grace-period=0 --force -f -'
Oct 19 16:29:44.115: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 16:29:44.115: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 19 16:29:44.115: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get rc,svc -l name=update-demo --no-headers'
Oct 19 16:29:44.168: INFO: stderr: "No resources found in kubectl-6080 namespace.\n"
Oct 19 16:29:44.168: INFO: stdout: ""
Oct 19 16:29:44.168: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6080 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 19 16:29:44.224: INFO: stderr: ""
Oct 19 16:29:44.224: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:29:44.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6080" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":118,"skipped":2355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:29:44.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4555
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Oct 19 16:29:44.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Oct 19 16:29:55.559: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:29:58.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:30:11.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4555" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":119,"skipped":2395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:30:11.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:30:12.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:30:15.319: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:30:15.323: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7951-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:30:18.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-751" for this suite.
STEP: Destroying namespace "webhook-751-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":120,"skipped":2437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:30:18.649: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6243
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Oct 19 16:30:18.898: INFO: Waiting up to 5m0s for pod "pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb" in namespace "emptydir-6243" to be "Succeeded or Failed"
Oct 19 16:30:18.902: INFO: Pod "pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448454ms
Oct 19 16:30:20.906: INFO: Pod "pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007708244s
STEP: Saw pod success
Oct 19 16:30:20.906: INFO: Pod "pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb" satisfied condition "Succeeded or Failed"
Oct 19 16:30:20.910: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb container test-container: <nil>
STEP: delete the pod
Oct 19 16:30:20.924: INFO: Waiting for pod pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb to disappear
Oct 19 16:30:20.927: INFO: Pod pod-1d32dc06-cc34-4ccc-b0fd-a899811e15bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:30:20.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6243" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2464,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:30:20.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-0848a188-4394-419c-b737-6297089ce22f
STEP: Creating a pod to test consume secrets
Oct 19 16:30:21.086: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4" in namespace "projected-9298" to be "Succeeded or Failed"
Oct 19 16:30:21.090: INFO: Pod "pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.455622ms
Oct 19 16:30:23.095: INFO: Pod "pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008089647s
STEP: Saw pod success
Oct 19 16:30:23.095: INFO: Pod "pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4" satisfied condition "Succeeded or Failed"
Oct 19 16:30:23.098: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4 container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:30:23.113: INFO: Waiting for pod pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4 to disappear
Oct 19 16:30:23.116: INFO: Pod pod-projected-secrets-6748a0ea-2cc5-4e67-8ea4-92bdd47439b4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:30:23.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9298" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2475,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:30:23.125: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-341
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 19 16:30:23.261: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 19 16:30:23.289: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:30:25.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:30:27.293: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:30:29.295: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:30:31.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:30:33.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:30:35.294: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 19 16:30:35.302: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 19 16:30:37.329: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 19 16:30:37.329: INFO: Breadth first check of 100.96.0.150 on host 10.250.1.123...
Oct 19 16:30:37.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.151:9080/dial?request=hostname&protocol=udp&host=100.96.0.150&port=8081&tries=1'] Namespace:pod-network-test-341 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:30:37.333: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:30:37.526: INFO: Waiting for responses: map[]
Oct 19 16:30:37.526: INFO: reached 100.96.0.150 after 0/1 tries
Oct 19 16:30:37.526: INFO: Breadth first check of 100.96.1.64 on host 10.250.3.120...
Oct 19 16:30:37.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.151:9080/dial?request=hostname&protocol=udp&host=100.96.1.64&port=8081&tries=1'] Namespace:pod-network-test-341 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:30:37.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:30:37.700: INFO: Waiting for responses: map[]
Oct 19 16:30:37.700: INFO: reached 100.96.1.64 after 0/1 tries
Oct 19 16:30:37.700: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:30:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-341" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:30:37.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-1243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:36:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1243" for this suite.

• [SLOW TEST:324.181 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":124,"skipped":2528,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:36:01.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-95t6
STEP: Creating a pod to test atomic-volume-subpath
Oct 19 16:36:02.058: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-95t6" in namespace "subpath-1009" to be "Succeeded or Failed"
Oct 19 16:36:02.062: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425505ms
Oct 19 16:36:04.067: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009118318s
Oct 19 16:36:06.072: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 4.013886513s
Oct 19 16:36:08.077: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 6.018945703s
Oct 19 16:36:10.082: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 8.023675628s
Oct 19 16:36:12.086: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 10.028200889s
Oct 19 16:36:14.091: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 12.03281633s
Oct 19 16:36:16.096: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 14.037850032s
Oct 19 16:36:18.100: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 16.041846634s
Oct 19 16:36:20.104: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 18.046152735s
Oct 19 16:36:22.109: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Running", Reason="", readiness=true. Elapsed: 20.050909088s
Oct 19 16:36:24.114: INFO: Pod "pod-subpath-test-configmap-95t6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056386139s
STEP: Saw pod success
Oct 19 16:36:24.114: INFO: Pod "pod-subpath-test-configmap-95t6" satisfied condition "Succeeded or Failed"
Oct 19 16:36:24.118: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-subpath-test-configmap-95t6 container test-container-subpath-configmap-95t6: <nil>
STEP: delete the pod
Oct 19 16:36:24.137: INFO: Waiting for pod pod-subpath-test-configmap-95t6 to disappear
Oct 19 16:36:24.140: INFO: Pod pod-subpath-test-configmap-95t6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-95t6
Oct 19 16:36:24.140: INFO: Deleting pod "pod-subpath-test-configmap-95t6" in namespace "subpath-1009"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:36:24.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1009" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":125,"skipped":2534,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:36:24.152: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4285
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 19 16:36:24.290: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 19 16:36:24.320: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:36:26.325: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:36:28.326: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:36:30.325: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:36:32.325: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:36:34.324: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 19 16:36:36.325: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 19 16:36:36.332: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 19 16:36:38.371: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 19 16:36:38.371: INFO: Going to poll 100.96.0.155 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Oct 19 16:36:38.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.155:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4285 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:36:38.374: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:36:38.607: INFO: Found all 1 expected endpoints: [netserver-0]
Oct 19 16:36:38.607: INFO: Going to poll 100.96.1.65 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Oct 19 16:36:38.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4285 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 16:36:38.610: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:36:38.840: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:36:38.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4285" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2549,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:36:38.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:36:38.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5" in namespace "downward-api-3001" to be "Succeeded or Failed"
Oct 19 16:36:39.002: INFO: Pod "downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006208ms
Oct 19 16:36:41.015: INFO: Pod "downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016923156s
STEP: Saw pod success
Oct 19 16:36:41.015: INFO: Pod "downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5" satisfied condition "Succeeded or Failed"
Oct 19 16:36:41.018: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5 container client-container: <nil>
STEP: delete the pod
Oct 19 16:36:41.082: INFO: Waiting for pod downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5 to disappear
Oct 19 16:36:41.085: INFO: Pod downwardapi-volume-f50259db-129a-4ebd-931a-98d29a8cfcc5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:36:41.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3001" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2558,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:36:41.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-a013123e-b56e-4126-9e96-c35b576c7aad
STEP: Creating a pod to test consume secrets
Oct 19 16:36:41.242: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8" in namespace "projected-2447" to be "Succeeded or Failed"
Oct 19 16:36:41.248: INFO: Pod "pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.522254ms
Oct 19 16:36:43.253: INFO: Pod "pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010287396s
STEP: Saw pod success
Oct 19 16:36:43.253: INFO: Pod "pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8" satisfied condition "Succeeded or Failed"
Oct 19 16:36:43.256: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:36:43.270: INFO: Waiting for pod pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8 to disappear
Oct 19 16:36:43.272: INFO: Pod pod-projected-secrets-629d4e19-cf77-4c3a-98c6-9d6db23070a8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:36:43.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2447" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:36:43.282: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-9020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Oct 19 16:36:45.462: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:36:47.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9020" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":129,"skipped":2596,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:36:47.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-751
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:36:47.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR 
Oct 19 16:36:50.187: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-19T16:36:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-19T16:36:50Z]] name:name1 resourceVersion:21326 uid:857a0096-b6d4-491e-a1e0-655b68fa9295] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Oct 19 16:37:00.195: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-19T16:37:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-19T16:37:00Z]] name:name2 resourceVersion:21394 uid:b59befa8-381c-4a0e-b8c8-4f8a8f45e869] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Oct 19 16:37:10.274: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-19T16:36:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-19T16:37:10Z]] name:name1 resourceVersion:21439 uid:857a0096-b6d4-491e-a1e0-655b68fa9295] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Oct 19 16:37:20.283: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-19T16:37:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-19T16:37:20Z]] name:name2 resourceVersion:21482 uid:b59befa8-381c-4a0e-b8c8-4f8a8f45e869] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Oct 19 16:37:30.288: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-19T16:36:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-19T16:37:10Z]] name:name1 resourceVersion:21549 uid:857a0096-b6d4-491e-a1e0-655b68fa9295] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Oct 19 16:37:40.297: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-19T16:37:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-19T16:37:20Z]] name:name2 resourceVersion:21592 uid:b59befa8-381c-4a0e-b8c8-4f8a8f45e869] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:37:50.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-751" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":130,"skipped":2610,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:37:50.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6281
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:37:50.985: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 19 16:37:55.989: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Oct 19 16:37:55.996: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Oct 19 16:37:56.004: INFO: observed ReplicaSet test-rs in namespace replicaset-6281 with ReadyReplicas 1, AvailableReplicas 1
Oct 19 16:37:56.008: INFO: observed ReplicaSet test-rs in namespace replicaset-6281 with ReadyReplicas 1, AvailableReplicas 1
Oct 19 16:37:56.020: INFO: observed ReplicaSet test-rs in namespace replicaset-6281 with ReadyReplicas 1, AvailableReplicas 1
Oct 19 16:37:56.023: INFO: observed ReplicaSet test-rs in namespace replicaset-6281 with ReadyReplicas 1, AvailableReplicas 1
Oct 19 16:37:56.893: INFO: observed ReplicaSet test-rs in namespace replicaset-6281 with ReadyReplicas 2, AvailableReplicas 2
Oct 19 16:37:57.086: INFO: observed Replicaset test-rs in namespace replicaset-6281 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:37:57.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6281" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":131,"skipped":2613,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:37:57.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Oct 19 16:37:57.231: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-662 cluster-info'
Oct 19 16:37:57.297: INFO: stderr: ""
Oct 19 16:37:57.297: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:37:57.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-662" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":132,"skipped":2630,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:37:57.305: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 19 16:37:57.460: INFO: The status of Pod annotationupdatec1c4762b-a38c-4269-b451-5f0c4269a6d0 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:37:59.474: INFO: The status of Pod annotationupdatec1c4762b-a38c-4269-b451-5f0c4269a6d0 is Running (Ready = true)
Oct 19 16:38:00.041: INFO: Successfully updated pod "annotationupdatec1c4762b-a38c-4269-b451-5f0c4269a6d0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:38:02.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7032" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2639,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:38:02.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Oct 19 16:38:02.247: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8692 create -f -'
Oct 19 16:38:02.374: INFO: stderr: ""
Oct 19 16:38:02.374: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Oct 19 16:38:02.374: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8692 diff -f -'
Oct 19 16:38:02.514: INFO: rc: 1
Oct 19 16:38:02.514: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8692 delete -f -'
Oct 19 16:38:02.562: INFO: stderr: ""
Oct 19 16:38:02.562: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:38:02.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8692" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":134,"skipped":2645,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:38:02.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3582
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3582
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3582
Oct 19 16:38:02.742: INFO: Found 0 stateful pods, waiting for 1
Oct 19 16:38:12.746: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Oct 19 16:38:12.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 16:38:13.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 16:38:13.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 16:38:13.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 16:38:13.006: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 19 16:38:23.011: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 16:38:23.011: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:38:23.025: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Oct 19 16:38:23.025: INFO: ss-0  shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:02 +0000 UTC  }]
Oct 19 16:38:23.025: INFO: 
Oct 19 16:38:23.025: INFO: StatefulSet ss has not reached scale 3, at 1
Oct 19 16:38:24.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996747243s
Oct 19 16:38:25.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992721914s
Oct 19 16:38:26.041: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985292455s
Oct 19 16:38:27.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979996232s
Oct 19 16:38:28.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974915025s
Oct 19 16:38:29.055: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97099581s
Oct 19 16:38:30.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966285943s
Oct 19 16:38:31.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962025471s
Oct 19 16:38:32.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.801502ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3582
Oct 19 16:38:33.074: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 16:38:33.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 16:38:33.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 16:38:33.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 16:38:33.365: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 16:38:33.642: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 19 16:38:33.643: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 16:38:33.643: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 16:38:33.643: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 16:38:33.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 19 16:38:33.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 16:38:33.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 19 16:38:33.955: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 16:38:33.955: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 16:38:33.955: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Oct 19 16:38:33.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 16:38:34.177: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 16:38:34.177: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 16:38:34.177: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 16:38:34.177: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 16:38:34.400: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 16:38:34.400: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 16:38:34.400: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 16:38:34.400: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-3582 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 16:38:34.596: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 16:38:34.596: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 16:38:34.596: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 16:38:34.596: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:38:34.599: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Oct 19 16:38:44.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 16:38:44.610: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 16:38:44.610: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 19 16:38:44.623: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Oct 19 16:38:44.623: INFO: ss-0  shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:02 +0000 UTC  }]
Oct 19 16:38:44.623: INFO: ss-1  shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:23 +0000 UTC  }]
Oct 19 16:38:44.623: INFO: ss-2  shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:38:23 +0000 UTC  }]
Oct 19 16:38:44.623: INFO: 
Oct 19 16:38:44.623: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 19 16:38:45.627: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.994197394s
Oct 19 16:38:46.632: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990691521s
Oct 19 16:38:47.637: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986381241s
Oct 19 16:38:48.641: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981206337s
Oct 19 16:38:49.646: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976309304s
Oct 19 16:38:50.650: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972232658s
Oct 19 16:38:51.654: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968326943s
Oct 19 16:38:52.658: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964326105s
Oct 19 16:38:53.662: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.981109ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3582
Oct 19 16:38:54.667: INFO: Scaling statefulset ss to 0
Oct 19 16:38:54.679: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 16:38:54.682: INFO: Deleting all statefulset in ns statefulset-3582
Oct 19 16:38:54.685: INFO: Scaling statefulset ss to 0
Oct 19 16:38:54.695: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:38:54.698: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:38:54.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3582" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":135,"skipped":2647,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:38:54.720: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-80
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Oct 19 16:38:57.385: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-80 pod-service-account-428dfffe-f449-4120-9d0b-794c797f75ed -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Oct 19 16:38:57.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-80 pod-service-account-428dfffe-f449-4120-9d0b-794c797f75ed -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Oct 19 16:38:57.792: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-80 pod-service-account-428dfffe-f449-4120-9d0b-794c797f75ed -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:38:58.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-80" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":136,"skipped":2658,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:38:58.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:38:58.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1477" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":137,"skipped":2659,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:38:58.184: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1019 16:39:04.361875    4339 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Oct 19 16:39:04.362: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:39:04.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-635" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":138,"skipped":2660,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:39:04.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8245
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8245
STEP: Waiting until pod test-pod will start running in namespace statefulset-8245
STEP: Creating statefulset with conflicting port in namespace statefulset-8245
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8245
Oct 19 16:39:06.551: INFO: Observed stateful pod in namespace: statefulset-8245, name: ss-0, uid: 5befccb1-023a-4100-92a5-d8a0751ab488, status phase: Pending. Waiting for statefulset controller to delete.
Oct 19 16:39:06.561: INFO: Observed stateful pod in namespace: statefulset-8245, name: ss-0, uid: 5befccb1-023a-4100-92a5-d8a0751ab488, status phase: Failed. Waiting for statefulset controller to delete.
Oct 19 16:39:06.571: INFO: Observed stateful pod in namespace: statefulset-8245, name: ss-0, uid: 5befccb1-023a-4100-92a5-d8a0751ab488, status phase: Failed. Waiting for statefulset controller to delete.
Oct 19 16:39:06.572: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8245
STEP: Removing pod with conflicting port in namespace statefulset-8245
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8245 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 16:39:08.586: INFO: Deleting all statefulset in ns statefulset-8245
Oct 19 16:39:08.589: INFO: Scaling statefulset ss to 0
Oct 19 16:39:18.654: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:39:18.657: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:39:18.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8245" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":139,"skipped":2665,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:39:18.677: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 19 16:39:18.826: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 19 16:40:18.866: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Oct 19 16:40:18.888: INFO: Created pod: pod0-0-sched-preemption-low-priority
Oct 19 16:40:18.896: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Oct 19 16:40:18.912: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Oct 19 16:40:18.921: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:40:34.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6344" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":140,"skipped":2672,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:40:35.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:40:35.180: INFO: Create a RollingUpdate DaemonSet
Oct 19 16:40:35.184: INFO: Check that daemon pods launch on every node of the cluster
Oct 19 16:40:35.190: INFO: Number of nodes with available pods: 0
Oct 19 16:40:35.191: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 16:40:36.200: INFO: Number of nodes with available pods: 0
Oct 19 16:40:36.201: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 is running more than one daemon pod
Oct 19 16:40:37.201: INFO: Number of nodes with available pods: 2
Oct 19 16:40:37.201: INFO: Number of running nodes: 2, number of available pods: 2
Oct 19 16:40:37.201: INFO: Update the DaemonSet to trigger a rollout
Oct 19 16:40:37.210: INFO: Updating DaemonSet daemon-set
Oct 19 16:40:40.227: INFO: Roll back the DaemonSet before rollout is complete
Oct 19 16:40:40.233: INFO: Updating DaemonSet daemon-set
Oct 19 16:40:40.234: INFO: Make sure DaemonSet rollback is complete
Oct 19 16:40:40.237: INFO: Wrong image for pod: daemon-set-zz5jc. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Oct 19 16:40:40.237: INFO: Pod daemon-set-zz5jc is not available
Oct 19 16:40:44.247: INFO: Pod daemon-set-fj7xr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4533, will wait for the garbage collector to delete the pods
Oct 19 16:40:44.315: INFO: Deleting DaemonSet.extensions daemon-set took: 4.513237ms
Oct 19 16:40:44.415: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.131656ms
Oct 19 16:40:47.420: INFO: Number of nodes with available pods: 0
Oct 19 16:40:47.420: INFO: Number of running nodes: 0, number of available pods: 0
Oct 19 16:40:47.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23043"},"items":null}

Oct 19 16:40:47.426: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23043"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:40:47.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4533" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":141,"skipped":2674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:40:47.449: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7581
STEP: Creating secret with name secret-test-456b9ac2-2ddf-48aa-89b6-7f0cf9433784
STEP: Creating a pod to test consume secrets
Oct 19 16:40:47.730: INFO: Waiting up to 5m0s for pod "pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d" in namespace "secrets-1005" to be "Succeeded or Failed"
Oct 19 16:40:47.735: INFO: Pod "pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097973ms
Oct 19 16:40:49.740: INFO: Pod "pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009095808s
STEP: Saw pod success
Oct 19 16:40:49.740: INFO: Pod "pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d" satisfied condition "Succeeded or Failed"
Oct 19 16:40:49.743: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:40:49.804: INFO: Waiting for pod pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d to disappear
Oct 19 16:40:49.809: INFO: Pod pod-secrets-25db4e4b-b4ef-44d2-b167-1137d6fdf08d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:40:49.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1005" for this suite.
STEP: Destroying namespace "secret-namespace-7581" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2709,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:40:49.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:40:49.978: INFO: The status of Pod pod-secrets-30ecfdde-e603-4115-93d2-d50227f02c28 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:40:51.982: INFO: The status of Pod pod-secrets-30ecfdde-e603-4115-93d2-d50227f02c28 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:40:52.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-923" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":143,"skipped":2711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:40:52.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-75
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-4ff694e4-5744-4493-9efe-2f82f569e9c6
STEP: Creating secret with name s-test-opt-upd-ac67b378-1ccd-4f0e-ae22-5cf817514b56
STEP: Creating the pod
Oct 19 16:40:52.210: INFO: The status of Pod pod-secrets-3dd5967b-4e2a-4269-bc0e-f7e7969dd252 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:40:54.215: INFO: The status of Pod pod-secrets-3dd5967b-4e2a-4269-bc0e-f7e7969dd252 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-4ff694e4-5744-4493-9efe-2f82f569e9c6
STEP: Updating secret s-test-opt-upd-ac67b378-1ccd-4f0e-ae22-5cf817514b56
STEP: Creating secret with name s-test-opt-create-f8bb11ad-ede9-487e-ac1a-100b4cf55779
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:40:56.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-75" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2738,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:40:56.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:40:57.006: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:41:00.024: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:41:00.028: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:41:03.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8303" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":145,"skipped":2756,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:41:03.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:41:03.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad" in namespace "projected-8265" to be "Succeeded or Failed"
Oct 19 16:41:03.535: INFO: Pod "downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.071401ms
Oct 19 16:41:05.539: INFO: Pod "downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007832499s
STEP: Saw pod success
Oct 19 16:41:05.539: INFO: Pod "downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad" satisfied condition "Succeeded or Failed"
Oct 19 16:41:05.542: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad container client-container: <nil>
STEP: delete the pod
Oct 19 16:41:05.597: INFO: Waiting for pod downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad to disappear
Oct 19 16:41:05.600: INFO: Pod downwardapi-volume-22ab0e55-bad8-47ec-b500-fa44d9e6a3ad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:41:05.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8265" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2756,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:41:05.609: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4581
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:41:05.744: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Oct 19 16:41:09.105: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 create -f -'
Oct 19 16:41:09.390: INFO: stderr: ""
Oct 19 16:41:09.390: INFO: stdout: "e2e-test-crd-publish-openapi-8154-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 19 16:41:09.390: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 delete e2e-test-crd-publish-openapi-8154-crds test-foo'
Oct 19 16:41:09.452: INFO: stderr: ""
Oct 19 16:41:09.452: INFO: stdout: "e2e-test-crd-publish-openapi-8154-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Oct 19 16:41:09.452: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 apply -f -'
Oct 19 16:41:09.592: INFO: stderr: ""
Oct 19 16:41:09.592: INFO: stdout: "e2e-test-crd-publish-openapi-8154-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 19 16:41:09.592: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 delete e2e-test-crd-publish-openapi-8154-crds test-foo'
Oct 19 16:41:09.642: INFO: stderr: ""
Oct 19 16:41:09.642: INFO: stdout: "e2e-test-crd-publish-openapi-8154-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Oct 19 16:41:09.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 create -f -'
Oct 19 16:41:09.763: INFO: rc: 1
Oct 19 16:41:09.763: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 apply -f -'
Oct 19 16:41:09.897: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Oct 19 16:41:09.897: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 create -f -'
Oct 19 16:41:10.015: INFO: rc: 1
Oct 19 16:41:10.015: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 --namespace=crd-publish-openapi-4581 apply -f -'
Oct 19 16:41:10.131: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Oct 19 16:41:10.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 explain e2e-test-crd-publish-openapi-8154-crds'
Oct 19 16:41:10.252: INFO: stderr: ""
Oct 19 16:41:10.252: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8154-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Oct 19 16:41:10.252: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 explain e2e-test-crd-publish-openapi-8154-crds.metadata'
Oct 19 16:41:10.373: INFO: stderr: ""
Oct 19 16:41:10.373: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8154-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Oct 19 16:41:10.374: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 explain e2e-test-crd-publish-openapi-8154-crds.spec'
Oct 19 16:41:10.507: INFO: stderr: ""
Oct 19 16:41:10.507: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8154-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Oct 19 16:41:10.507: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 explain e2e-test-crd-publish-openapi-8154-crds.spec.bars'
Oct 19 16:41:10.625: INFO: stderr: ""
Oct 19 16:41:10.625: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8154-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Oct 19 16:41:10.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4581 explain e2e-test-crd-publish-openapi-8154-crds.spec.bars2'
Oct 19 16:41:10.745: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:41:13.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4581" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":147,"skipped":2776,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:41:13.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7997
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-9b545035-d9e5-46bf-a9b6-03bf8a64290d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:41:16.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7997" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2777,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:41:16.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2307
STEP: creating service affinity-clusterip in namespace services-2307
STEP: creating replication controller affinity-clusterip in namespace services-2307
I1019 16:41:16.224405    4339 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2307, replica count: 3
I1019 16:41:19.275619    4339 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 16:41:19.282: INFO: Creating new exec pod
Oct 19 16:41:22.302: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2307 exec execpod-affinityj9m7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Oct 19 16:41:22.593: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Oct 19 16:41:22.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:41:22.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2307 exec execpod-affinityj9m7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.64.65.26 80'
Oct 19 16:41:22.775: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.64.65.26 80\nConnection to 100.64.65.26 80 port [tcp/http] succeeded!\n"
Oct 19 16:41:22.775: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:41:22.775: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2307 exec execpod-affinityj9m7x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.65.26:80/ ; done'
Oct 19 16:41:23.051: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.65.26:80/\n"
Oct 19 16:41:23.051: INFO: stdout: "\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v\naffinity-clusterip-2684v"
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Received response from host: affinity-clusterip-2684v
Oct 19 16:41:23.051: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2307, will wait for the garbage collector to delete the pods
Oct 19 16:41:23.114: INFO: Deleting ReplicationController affinity-clusterip took: 3.781807ms
Oct 19 16:41:23.215: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.346372ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:41:25.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2307" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":149,"skipped":2778,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:41:25.533: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Oct 19 16:41:25.678: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23527 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:41:25.678: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23527 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Oct 19 16:41:35.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23601 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:41:35.686: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23601 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Oct 19 16:41:45.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23645 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:41:45.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23645 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Oct 19 16:41:55.704: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23687 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:41:55.705: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1597  e3bc99c4-3139-4695-87e8-f5852140a0e3 23687 0 2021-10-19 16:41:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-19 16:41:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Oct 19 16:42:05.713: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1597  44e3c7f5-e9ed-4b59-a32f-6cb135e5a8f5 23730 0 2021-10-19 16:42:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-19 16:42:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:42:05.713: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1597  44e3c7f5-e9ed-4b59-a32f-6cb135e5a8f5 23730 0 2021-10-19 16:42:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-19 16:42:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Oct 19 16:42:15.720: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1597  44e3c7f5-e9ed-4b59-a32f-6cb135e5a8f5 23775 0 2021-10-19 16:42:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-19 16:42:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:42:15.720: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1597  44e3c7f5-e9ed-4b59-a32f-6cb135e5a8f5 23775 0 2021-10-19 16:42:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-19 16:42:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:42:25.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1597" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":150,"skipped":2797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:42:25.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5038
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:42:25.870: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:42:32.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5038" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":151,"skipped":2859,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:42:32.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:42:33.270: INFO: The status of Pod busybox-host-aliasesb1b712a6-beba-4b4a-bd4b-2c57e4fe3c21 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:42:35.275: INFO: The status of Pod busybox-host-aliasesb1b712a6-beba-4b4a-bd4b-2c57e4fe3c21 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:42:35.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3514" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":152,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:42:35.478: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 19 16:42:35.685: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 19 16:42:35.693: INFO: Waiting for terminating namespaces to be deleted...
Oct 19 16:42:35.696: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 before test
Oct 19 16:42:35.705: INFO: addons-nginx-ingress-controller-6ccd9d5d4d-87wtm from kube-system started at 2021-10-19 16:20:45 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Oct 19 16:42:35.705: INFO: apiserver-proxy-ftftt from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container proxy ready: true, restart count 0
Oct 19 16:42:35.705: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 16:42:35.705: INFO: blackbox-exporter-65c549b94c-c5pzd from kube-system started at 2021-10-19 15:51:26 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container blackbox-exporter ready: true, restart count 0
Oct 19 16:42:35.705: INFO: calico-kube-controllers-86c64d79ff-hmgq6 from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 19 16:42:35.705: INFO: calico-node-gkqll from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 16:42:35.705: INFO: calico-typha-deploy-58b94ff46-kljnn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container calico-typha ready: true, restart count 0
Oct 19 16:42:35.705: INFO: csi-driver-node-twl5g from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 16:42:35.705: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 16:42:35.705: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 16:42:35.705: INFO: kube-proxy-hgtmc from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 16:42:35.705: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 16:42:35.705: INFO: node-exporter-v9h4r from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 16:42:35.705: INFO: node-problem-detector-2s6bt from kube-system started at 2021-10-19 16:11:27 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 16:42:35.705: INFO: busybox-host-aliasesb1b712a6-beba-4b4a-bd4b-2c57e4fe3c21 from kubelet-test-3514 started at 2021-10-19 16:42:33 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.705: INFO: 	Container busybox-host-aliasesb1b712a6-beba-4b4a-bd4b-2c57e4fe3c21 ready: true, restart count 0
Oct 19 16:42:35.705: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq before test
Oct 19 16:42:35.713: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-56d9d84c8c-ftj5w from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Oct 19 16:42:35.713: INFO: apiserver-proxy-r6qsz from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container proxy ready: true, restart count 0
Oct 19 16:42:35.713: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 16:42:35.713: INFO: calico-node-54s6z from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 16:42:35.713: INFO: calico-node-vertical-autoscaler-785b5f968-w77tx from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:42:35.713: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-bqq7q from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:42:35.713: INFO: calico-typha-vertical-autoscaler-5c9655cddd-w2d9c from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:42:35.713: INFO: coredns-9866fb499-7zgkw from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container coredns ready: true, restart count 0
Oct 19 16:42:35.713: INFO: coredns-9866fb499-kcm5k from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container coredns ready: true, restart count 0
Oct 19 16:42:35.713: INFO: csi-driver-node-ps5fs from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 16:42:35.713: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 16:42:35.713: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 16:42:35.713: INFO: kube-proxy-dpksr from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 16:42:35.713: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 16:42:35.713: INFO: metrics-server-7958497998-bdvjq from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container metrics-server ready: true, restart count 0
Oct 19 16:42:35.713: INFO: node-exporter-2xtzn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 16:42:35.713: INFO: node-problem-detector-6n9vb from kube-system started at 2021-10-19 16:11:28 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 16:42:35.713: INFO: vpn-shoot-6cdd4985bc-w7qgp from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container vpn-shoot ready: true, restart count 0
Oct 19 16:42:35.713: INFO: dashboard-metrics-scraper-7ccbfc448f-htlbk from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 19 16:42:35.713: INFO: kubernetes-dashboard-847f4ffdcd-6s4nf from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:42:35.713: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fbaf47de-9e00-4ac8-8154-91806a5f581b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.1.123 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-fbaf47de-9e00-4ac8-8154-91806a5f581b off the node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fbaf47de-9e00-4ac8-8154-91806a5f581b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:47:39.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7751" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.402 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":153,"skipped":2893,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:47:39.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:47:40.056: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b39da181-a180-4e8c-a94c-9a3c0a044f62", Controller:(*bool)(0xc0060adde6), BlockOwnerDeletion:(*bool)(0xc0060adde7)}}
Oct 19 16:47:40.062: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"37271d33-8a11-4d41-a52a-b165e95bba39", Controller:(*bool)(0xc00601d73e), BlockOwnerDeletion:(*bool)(0xc00601d73f)}}
Oct 19 16:47:40.067: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1a2e34dd-1913-4d57-b9e8-cb920c97ee8a", Controller:(*bool)(0xc0060568de), BlockOwnerDeletion:(*bool)(0xc0060568df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:47:45.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1587" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":154,"skipped":2908,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:47:45.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:47:45.443: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:47:48.554: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:47:48.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-372" for this suite.
STEP: Destroying namespace "webhook-372-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":155,"skipped":2922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:47:48.792: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:47:50.950: INFO: Deleting pod "var-expansion-3760ab10-5fbd-454f-890d-23d4f764bfed" in namespace "var-expansion-5807"
Oct 19 16:47:50.955: INFO: Wait up to 5m0s for pod "var-expansion-3760ab10-5fbd-454f-890d-23d4f764bfed" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:47:54.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5807" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":156,"skipped":2945,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:47:54.973: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Oct 19 16:47:55.121: INFO: created test-pod-1
Oct 19 16:47:55.129: INFO: created test-pod-2
Oct 19 16:47:55.138: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Oct 19 16:47:55.160: INFO: Pod quantity 3 is different from expected quantity 0
Oct 19 16:47:56.165: INFO: Pod quantity 3 is different from expected quantity 0
Oct 19 16:47:57.164: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:47:58.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7102" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":157,"skipped":2954,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:47:58.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:47:58.570: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:48:01.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:13.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3325" for this suite.
STEP: Destroying namespace "webhook-3325-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":158,"skipped":2973,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:13.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 19 16:48:14.100: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 19 16:48:14.107: INFO: Waiting for terminating namespaces to be deleted...
Oct 19 16:48:14.110: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 before test
Oct 19 16:48:14.118: INFO: addons-nginx-ingress-controller-6ccd9d5d4d-87wtm from kube-system started at 2021-10-19 16:20:45 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Oct 19 16:48:14.118: INFO: apiserver-proxy-ftftt from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container proxy ready: true, restart count 0
Oct 19 16:48:14.118: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 16:48:14.118: INFO: blackbox-exporter-65c549b94c-c5pzd from kube-system started at 2021-10-19 15:51:26 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container blackbox-exporter ready: true, restart count 0
Oct 19 16:48:14.118: INFO: calico-kube-controllers-86c64d79ff-hmgq6 from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 19 16:48:14.118: INFO: calico-node-gkqll from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 16:48:14.118: INFO: calico-typha-deploy-58b94ff46-kljnn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container calico-typha ready: true, restart count 0
Oct 19 16:48:14.118: INFO: csi-driver-node-twl5g from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 16:48:14.118: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 16:48:14.118: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 16:48:14.118: INFO: kube-proxy-hgtmc from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 16:48:14.118: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 16:48:14.118: INFO: node-exporter-v9h4r from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 16:48:14.118: INFO: node-problem-detector-2s6bt from kube-system started at 2021-10-19 16:11:27 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.118: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 16:48:14.118: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq before test
Oct 19 16:48:14.125: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-56d9d84c8c-ftj5w from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.125: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Oct 19 16:48:14.125: INFO: apiserver-proxy-r6qsz from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:14.125: INFO: 	Container proxy ready: true, restart count 0
Oct 19 16:48:14.125: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 16:48:14.125: INFO: calico-node-54s6z from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.125: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 16:48:14.125: INFO: calico-node-vertical-autoscaler-785b5f968-w77tx from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.125: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:48:14.125: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-bqq7q from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.125: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:48:14.125: INFO: calico-typha-vertical-autoscaler-5c9655cddd-w2d9c from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.125: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:48:14.125: INFO: coredns-9866fb499-7zgkw from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container coredns ready: true, restart count 0
Oct 19 16:48:14.126: INFO: coredns-9866fb499-kcm5k from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container coredns ready: true, restart count 0
Oct 19 16:48:14.126: INFO: csi-driver-node-ps5fs from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 16:48:14.126: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 16:48:14.126: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 16:48:14.126: INFO: kube-proxy-dpksr from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 16:48:14.126: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 16:48:14.126: INFO: metrics-server-7958497998-bdvjq from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container metrics-server ready: true, restart count 0
Oct 19 16:48:14.126: INFO: node-exporter-2xtzn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 16:48:14.126: INFO: node-problem-detector-6n9vb from kube-system started at 2021-10-19 16:11:28 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 16:48:14.126: INFO: vpn-shoot-6cdd4985bc-w7qgp from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container vpn-shoot ready: true, restart count 0
Oct 19 16:48:14.126: INFO: dashboard-metrics-scraper-7ccbfc448f-htlbk from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 19 16:48:14.126: INFO: kubernetes-dashboard-847f4ffdcd-6s4nf from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:14.126: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16af7c8389cdb5dc], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:15.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5993" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":159,"skipped":2974,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:15.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-nwlq
STEP: Creating a pod to test atomic-volume-subpath
Oct 19 16:48:15.324: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nwlq" in namespace "subpath-5768" to be "Succeeded or Failed"
Oct 19 16:48:15.327: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868844ms
Oct 19 16:48:17.332: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007634818s
Oct 19 16:48:19.337: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 4.0129426s
Oct 19 16:48:21.342: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 6.017872404s
Oct 19 16:48:23.370: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 8.046486913s
Oct 19 16:48:25.375: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 10.051270406s
Oct 19 16:48:27.380: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 12.055862305s
Oct 19 16:48:29.384: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 14.059944143s
Oct 19 16:48:31.388: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 16.06454987s
Oct 19 16:48:33.394: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 18.06974306s
Oct 19 16:48:35.398: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Running", Reason="", readiness=true. Elapsed: 20.07432155s
Oct 19 16:48:37.408: INFO: Pod "pod-subpath-test-downwardapi-nwlq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.084349701s
STEP: Saw pod success
Oct 19 16:48:37.408: INFO: Pod "pod-subpath-test-downwardapi-nwlq" satisfied condition "Succeeded or Failed"
Oct 19 16:48:37.416: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-subpath-test-downwardapi-nwlq container test-container-subpath-downwardapi-nwlq: <nil>
STEP: delete the pod
Oct 19 16:48:37.439: INFO: Waiting for pod pod-subpath-test-downwardapi-nwlq to disappear
Oct 19 16:48:37.442: INFO: Pod pod-subpath-test-downwardapi-nwlq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nwlq
Oct 19 16:48:37.442: INFO: Deleting pod "pod-subpath-test-downwardapi-nwlq" in namespace "subpath-5768"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:37.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5768" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":160,"skipped":2976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:37.456: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Oct 19 16:48:47.622: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:47.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1019 16:48:47.622636    4339 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-820" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":161,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:47.631: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Oct 19 16:48:47.780: INFO: Waiting up to 5m0s for pod "var-expansion-80af9759-cee2-4ade-a88e-0467498c364a" in namespace "var-expansion-6858" to be "Succeeded or Failed"
Oct 19 16:48:47.783: INFO: Pod "var-expansion-80af9759-cee2-4ade-a88e-0467498c364a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234978ms
Oct 19 16:48:49.787: INFO: Pod "var-expansion-80af9759-cee2-4ade-a88e-0467498c364a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007636821s
STEP: Saw pod success
Oct 19 16:48:49.787: INFO: Pod "var-expansion-80af9759-cee2-4ade-a88e-0467498c364a" satisfied condition "Succeeded or Failed"
Oct 19 16:48:49.791: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod var-expansion-80af9759-cee2-4ade-a88e-0467498c364a container dapi-container: <nil>
STEP: delete the pod
Oct 19 16:48:49.806: INFO: Waiting for pod var-expansion-80af9759-cee2-4ade-a88e-0467498c364a to disappear
Oct 19 16:48:49.809: INFO: Pod var-expansion-80af9759-cee2-4ade-a88e-0467498c364a no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:49.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6858" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":162,"skipped":3103,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:49.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Oct 19 16:48:49.954: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6298 api-versions'
Oct 19 16:48:50.030: INFO: stderr: ""
Oct 19 16:48:50.030: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:50.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6298" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":163,"skipped":3106,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:50.038: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 19 16:48:50.171: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 19 16:48:50.178: INFO: Waiting for terminating namespaces to be deleted...
Oct 19 16:48:50.181: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 before test
Oct 19 16:48:50.189: INFO: addons-nginx-ingress-controller-6ccd9d5d4d-87wtm from kube-system started at 2021-10-19 16:20:45 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Oct 19 16:48:50.189: INFO: apiserver-proxy-ftftt from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container proxy ready: true, restart count 0
Oct 19 16:48:50.189: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 16:48:50.189: INFO: blackbox-exporter-65c549b94c-c5pzd from kube-system started at 2021-10-19 15:51:26 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container blackbox-exporter ready: true, restart count 0
Oct 19 16:48:50.189: INFO: calico-kube-controllers-86c64d79ff-hmgq6 from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 19 16:48:50.189: INFO: calico-node-gkqll from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 16:48:50.189: INFO: calico-typha-deploy-58b94ff46-kljnn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container calico-typha ready: true, restart count 0
Oct 19 16:48:50.189: INFO: csi-driver-node-twl5g from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 16:48:50.189: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 16:48:50.189: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 16:48:50.189: INFO: kube-proxy-hgtmc from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 16:48:50.189: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 16:48:50.189: INFO: node-exporter-v9h4r from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 16:48:50.189: INFO: node-problem-detector-2s6bt from kube-system started at 2021-10-19 16:11:27 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.189: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 16:48:50.189: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq before test
Oct 19 16:48:50.196: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-56d9d84c8c-ftj5w from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Oct 19 16:48:50.196: INFO: apiserver-proxy-r6qsz from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container proxy ready: true, restart count 0
Oct 19 16:48:50.196: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 16:48:50.196: INFO: calico-node-54s6z from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 16:48:50.196: INFO: calico-node-vertical-autoscaler-785b5f968-w77tx from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:48:50.196: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-bqq7q from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:48:50.196: INFO: calico-typha-vertical-autoscaler-5c9655cddd-w2d9c from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 16:48:50.196: INFO: coredns-9866fb499-7zgkw from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container coredns ready: true, restart count 0
Oct 19 16:48:50.196: INFO: coredns-9866fb499-kcm5k from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container coredns ready: true, restart count 0
Oct 19 16:48:50.196: INFO: csi-driver-node-ps5fs from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 16:48:50.196: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 16:48:50.196: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 16:48:50.196: INFO: kube-proxy-dpksr from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 16:48:50.196: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 16:48:50.196: INFO: metrics-server-7958497998-bdvjq from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container metrics-server ready: true, restart count 0
Oct 19 16:48:50.196: INFO: node-exporter-2xtzn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 16:48:50.196: INFO: node-problem-detector-6n9vb from kube-system started at 2021-10-19 16:11:28 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 16:48:50.196: INFO: vpn-shoot-6cdd4985bc-w7qgp from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container vpn-shoot ready: true, restart count 0
Oct 19 16:48:50.196: INFO: dashboard-metrics-scraper-7ccbfc448f-htlbk from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 19 16:48:50.196: INFO: kubernetes-dashboard-847f4ffdcd-6s4nf from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 16:48:50.196: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
STEP: verifying the node has the label node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod addons-nginx-ingress-controller-6ccd9d5d4d-87wtm requesting resource cpu=100m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-56d9d84c8c-ftj5w requesting resource cpu=0m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod apiserver-proxy-ftftt requesting resource cpu=40m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod apiserver-proxy-r6qsz requesting resource cpu=40m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod blackbox-exporter-65c549b94c-c5pzd requesting resource cpu=11m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod calico-kube-controllers-86c64d79ff-hmgq6 requesting resource cpu=10m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod calico-node-54s6z requesting resource cpu=250m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod calico-node-gkqll requesting resource cpu=250m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod calico-node-vertical-autoscaler-785b5f968-w77tx requesting resource cpu=10m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod calico-typha-deploy-58b94ff46-kljnn requesting resource cpu=200m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod calico-typha-horizontal-autoscaler-5b58bb446c-bqq7q requesting resource cpu=10m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod calico-typha-vertical-autoscaler-5c9655cddd-w2d9c requesting resource cpu=10m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod coredns-9866fb499-7zgkw requesting resource cpu=50m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod coredns-9866fb499-kcm5k requesting resource cpu=50m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod csi-driver-node-ps5fs requesting resource cpu=40m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod csi-driver-node-twl5g requesting resource cpu=40m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod kube-proxy-dpksr requesting resource cpu=34m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod kube-proxy-hgtmc requesting resource cpu=34m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod metrics-server-7958497998-bdvjq requesting resource cpu=50m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod node-exporter-2xtzn requesting resource cpu=50m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod node-exporter-v9h4r requesting resource cpu=50m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod node-problem-detector-2s6bt requesting resource cpu=11m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.235: INFO: Pod node-problem-detector-6n9vb requesting resource cpu=11m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod vpn-shoot-6cdd4985bc-w7qgp requesting resource cpu=100m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod dashboard-metrics-scraper-7ccbfc448f-htlbk requesting resource cpu=0m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
Oct 19 16:48:50.235: INFO: Pod kubernetes-dashboard-847f4ffdcd-6s4nf requesting resource cpu=50m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
STEP: Starting Pods to consume most of the cluster CPU.
Oct 19 16:48:50.235: INFO: Creating a pod which consumes cpu=821m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
Oct 19 16:48:50.244: INFO: Creating a pod which consumes cpu=815m on Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe.16af7c8bf18a4365], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5853/filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe to shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe.16af7c8c128e15c6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe.16af7c8c1424bc78], Reason = [Created], Message = [Created container filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe.16af7c8c17495431], Reason = [Started], Message = [Started container filler-pod-b6a7f7cc-c710-435d-b1f0-3cf8ef5d4ffe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf.16af7c8bf11ac0b0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5853/filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf to shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf.16af7c8c0f3d1295], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf.16af7c8c10a258ae], Reason = [Created], Message = [Created container filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf.16af7c8c137ee7ab], Reason = [Started], Message = [Started container filler-pod-f4cc41b0-d0fc-4ab1-b81d-af3995ba37cf]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16af7c8c6a423eac], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:53.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5853" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":164,"skipped":3111,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:53.319: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Oct 19 16:48:53.485: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1601  d7f00be5-db18-4797-8d93-195f8081103f 26136 0 2021-10-19 16:48:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-10-19 16:48:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 16:48:53.485: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1601  d7f00be5-db18-4797-8d93-195f8081103f 26137 0 2021-10-19 16:48:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-10-19 16:48:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:53.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1601" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":165,"skipped":3124,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:53.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:48:53.639: INFO: The status of Pod server-envvars-47b81e70-7ade-4b38-85d7-256c0aec631c is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:48:55.646: INFO: The status of Pod server-envvars-47b81e70-7ade-4b38-85d7-256c0aec631c is Running (Ready = true)
Oct 19 16:48:55.665: INFO: Waiting up to 5m0s for pod "client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b" in namespace "pods-5391" to be "Succeeded or Failed"
Oct 19 16:48:55.668: INFO: Pod "client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.228283ms
Oct 19 16:48:57.674: INFO: Pod "client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00847096s
STEP: Saw pod success
Oct 19 16:48:57.674: INFO: Pod "client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b" satisfied condition "Succeeded or Failed"
Oct 19 16:48:57.677: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b container env3cont: <nil>
STEP: delete the pod
Oct 19 16:48:57.692: INFO: Waiting for pod client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b to disappear
Oct 19 16:48:57.697: INFO: Pod client-envvars-18b3cb81-c4c5-49c3-9786-aceca154ea0b no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:57.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5391" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":3129,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:57.706: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-e5d7fa88-2989-4d28-a9c2-8fe882d165e6
STEP: Creating a pod to test consume configMaps
Oct 19 16:48:57.854: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8" in namespace "projected-5169" to be "Succeeded or Failed"
Oct 19 16:48:57.858: INFO: Pod "pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.388924ms
Oct 19 16:48:59.862: INFO: Pod "pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007448442s
STEP: Saw pod success
Oct 19 16:48:59.862: INFO: Pod "pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8" satisfied condition "Succeeded or Failed"
Oct 19 16:48:59.865: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:48:59.885: INFO: Waiting for pod pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8 to disappear
Oct 19 16:48:59.888: INFO: Pod pod-projected-configmaps-571a03b0-cc3f-46ac-a9fb-272dc66ac6c8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:48:59.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5169" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":3142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:48:59.898: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7656, will wait for the garbage collector to delete the pods
Oct 19 16:49:02.101: INFO: Deleting Job.batch foo took: 4.065333ms
Oct 19 16:49:02.201: INFO: Terminating Job.batch foo pods took: 100.939236ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:49:34.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7656" for this suite.
•{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":168,"skipped":3194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:49:34.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9245
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Oct 19 16:49:34.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 16:49:37.522: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:49:49.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9245" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":169,"skipped":3234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:49:49.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-9980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:49:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9980" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":170,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:49:49.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8a70e98f-8539-4408-8c58-a28c683014f7
STEP: Creating a pod to test consume secrets
Oct 19 16:49:49.707: INFO: Waiting up to 5m0s for pod "pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026" in namespace "secrets-6357" to be "Succeeded or Failed"
Oct 19 16:49:49.712: INFO: Pod "pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599867ms
Oct 19 16:49:51.717: INFO: Pod "pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010163948s
STEP: Saw pod success
Oct 19 16:49:51.717: INFO: Pod "pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026" satisfied condition "Succeeded or Failed"
Oct 19 16:49:51.722: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026 container secret-env-test: <nil>
STEP: delete the pod
Oct 19 16:49:51.738: INFO: Waiting for pod pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026 to disappear
Oct 19 16:49:51.741: INFO: Pod pod-secrets-573a3e54-a5f0-4253-b15b-0812fe912026 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:49:51.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6357" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3302,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:49:51.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5992
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Oct 19 16:49:51.895: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 19 16:49:56.899: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:49:56.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5992" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":172,"skipped":3316,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:49:56.938: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 19 16:49:57.082: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3578 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Oct 19 16:49:57.147: INFO: stderr: ""
Oct 19 16:49:57.147: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Oct 19 16:50:02.197: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3578 get pod e2e-test-httpd-pod -o json'
Oct 19 16:50:02.246: INFO: stderr: ""
Oct 19 16:50:02.246: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.0.209/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.96.0.209/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-10-19T16:49:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3578\",\n        \"resourceVersion\": \"26677\",\n        \"uid\": \"1b2242cc-c170-4cc2-bbba-92006ffa93e8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmhay-ddd.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vvrtb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vvrtb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-19T16:49:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-19T16:49:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-19T16:49:58Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-19T16:49:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c89596c5b178f74cb286df49c745e255202af7da79cbf6bf53ed2b72f3b4ca5e\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-10-19T16:49:57Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.1.123\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.0.209\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.0.209\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-10-19T16:49:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct 19 16:50:02.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3578 replace -f -'
Oct 19 16:50:02.408: INFO: stderr: ""
Oct 19 16:50:02.408: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Oct 19 16:50:02.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3578 delete pods e2e-test-httpd-pod'
Oct 19 16:50:04.461: INFO: stderr: ""
Oct 19 16:50:04.461: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:04.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3578" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":173,"skipped":3320,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:04.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:04.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3475" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":174,"skipped":3329,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:04.649: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:50:04.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296" in namespace "projected-4805" to be "Succeeded or Failed"
Oct 19 16:50:04.797: INFO: Pod "downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296": Phase="Pending", Reason="", readiness=false. Elapsed: 3.236121ms
Oct 19 16:50:06.801: INFO: Pod "downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007302688s
STEP: Saw pod success
Oct 19 16:50:06.801: INFO: Pod "downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296" satisfied condition "Succeeded or Failed"
Oct 19 16:50:06.805: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296 container client-container: <nil>
STEP: delete the pod
Oct 19 16:50:06.818: INFO: Waiting for pod downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296 to disappear
Oct 19 16:50:06.821: INFO: Pod downwardapi-volume-10bb23a4-87a3-4a6e-a872-2ced1a7b9296 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4805" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:06.830: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-2aa36a3e-cc1e-4665-8b3b-f32aa60c8b84
STEP: Creating a pod to test consume configMaps
Oct 19 16:50:06.977: INFO: Waiting up to 5m0s for pod "pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee" in namespace "configmap-2120" to be "Succeeded or Failed"
Oct 19 16:50:06.981: INFO: Pod "pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234855ms
Oct 19 16:50:08.985: INFO: Pod "pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00830815s
STEP: Saw pod success
Oct 19 16:50:08.985: INFO: Pod "pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee" satisfied condition "Succeeded or Failed"
Oct 19 16:50:08.989: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:50:09.003: INFO: Waiting for pod pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee to disappear
Oct 19 16:50:09.006: INFO: Pod pod-configmaps-7917c0d3-75d4-4827-aad7-706f6fb9f0ee no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:09.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2120" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3376,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:09.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:50:09.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e" in namespace "downward-api-233" to be "Succeeded or Failed"
Oct 19 16:50:09.162: INFO: Pod "downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38277ms
Oct 19 16:50:11.166: INFO: Pod "downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007567341s
STEP: Saw pod success
Oct 19 16:50:11.166: INFO: Pod "downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e" satisfied condition "Succeeded or Failed"
Oct 19 16:50:11.170: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e container client-container: <nil>
STEP: delete the pod
Oct 19 16:50:11.184: INFO: Waiting for pod downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e to disappear
Oct 19 16:50:11.190: INFO: Pod downwardapi-volume-e241e7d9-ba10-45bc-b085-bed8d61bff2e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:11.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-233" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3381,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:11.211: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:50:11.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:50:15.077: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:50:15.081: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3030-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:18.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5501" for this suite.
STEP: Destroying namespace "webhook-5501-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":178,"skipped":3396,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:18.453: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:50:19.387: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:50:22.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:50:22.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9033" for this suite.
STEP: Destroying namespace "webhook-9033-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":179,"skipped":3411,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:50:22.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9622
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-dd81c875-9cf1-4aac-9f18-3f7fc196a13f
STEP: Creating the pod
Oct 19 16:50:22.718: INFO: The status of Pod pod-configmaps-8383a956-d16d-4f6d-8f10-03009f47157c is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:50:24.722: INFO: The status of Pod pod-configmaps-8383a956-d16d-4f6d-8f10-03009f47157c is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-dd81c875-9cf1-4aac-9f18-3f7fc196a13f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:51:31.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9622" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:51:31.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4810
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-4810
Oct 19 16:51:31.315: INFO: Found 0 stateful pods, waiting for 1
Oct 19 16:51:41.324: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Oct 19 16:51:41.342: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Oct 19 16:51:41.350: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Oct 19 16:51:41.353: INFO: Observed &StatefulSet event: ADDED
Oct 19 16:51:41.353: INFO: Found Statefulset ss in namespace statefulset-4810 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Oct 19 16:51:41.353: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Oct 19 16:51:41.353: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Oct 19 16:51:41.363: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Oct 19 16:51:41.367: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 16:51:41.367: INFO: Deleting all statefulset in ns statefulset-4810
Oct 19 16:51:41.370: INFO: Scaling statefulset ss to 0
Oct 19 16:51:51.391: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:51:51.395: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:51:51.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4810" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":181,"skipped":3470,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:51:51.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:51:51.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a" in namespace "projected-6624" to be "Succeeded or Failed"
Oct 19 16:51:51.565: INFO: Pod "downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745442ms
Oct 19 16:51:53.569: INFO: Pod "downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009211904s
STEP: Saw pod success
Oct 19 16:51:53.569: INFO: Pod "downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a" satisfied condition "Succeeded or Failed"
Oct 19 16:51:53.573: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a container client-container: <nil>
STEP: delete the pod
Oct 19 16:51:53.628: INFO: Waiting for pod downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a to disappear
Oct 19 16:51:53.631: INFO: Pod downwardapi-volume-00a8c669-1a71-4606-a817-7649c472522a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:51:53.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6624" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3473,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:51:53.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-5a72eb2f-f321-4c4e-b15e-bb7769beda0a
STEP: Creating a pod to test consume configMaps
Oct 19 16:51:53.794: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f" in namespace "configmap-2815" to be "Succeeded or Failed"
Oct 19 16:51:53.797: INFO: Pod "pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.001052ms
Oct 19 16:51:55.802: INFO: Pod "pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007813774s
STEP: Saw pod success
Oct 19 16:51:55.802: INFO: Pod "pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f" satisfied condition "Succeeded or Failed"
Oct 19 16:51:55.805: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:51:55.865: INFO: Waiting for pod pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f to disappear
Oct 19 16:51:55.868: INFO: Pod pod-configmaps-f1e3e559-d832-4c85-8207-3e5ab28ba73f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:51:55.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2815" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3473,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:51:55.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 19 16:51:56.021: INFO: Waiting up to 5m0s for pod "pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4" in namespace "emptydir-9741" to be "Succeeded or Failed"
Oct 19 16:51:56.024: INFO: Pod "pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158345ms
Oct 19 16:51:58.028: INFO: Pod "pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007023261s
STEP: Saw pod success
Oct 19 16:51:58.028: INFO: Pod "pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4" satisfied condition "Succeeded or Failed"
Oct 19 16:51:58.031: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4 container test-container: <nil>
STEP: delete the pod
Oct 19 16:51:58.087: INFO: Waiting for pod pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4 to disappear
Oct 19 16:51:58.090: INFO: Pod pod-bcb1b91e-5013-490d-8760-e8d6b2e3dbf4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:51:58.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9741" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3482,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:51:58.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 19 16:51:58.243: INFO: Waiting up to 5m0s for pod "pod-e88591fd-7347-48cc-9e8b-e8c546ce6338" in namespace "emptydir-7624" to be "Succeeded or Failed"
Oct 19 16:51:58.247: INFO: Pod "pod-e88591fd-7347-48cc-9e8b-e8c546ce6338": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696934ms
Oct 19 16:52:00.250: INFO: Pod "pod-e88591fd-7347-48cc-9e8b-e8c546ce6338": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007054328s
STEP: Saw pod success
Oct 19 16:52:00.250: INFO: Pod "pod-e88591fd-7347-48cc-9e8b-e8c546ce6338" satisfied condition "Succeeded or Failed"
Oct 19 16:52:00.254: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-e88591fd-7347-48cc-9e8b-e8c546ce6338 container test-container: <nil>
STEP: delete the pod
Oct 19 16:52:00.267: INFO: Waiting for pod pod-e88591fd-7347-48cc-9e8b-e8c546ce6338 to disappear
Oct 19 16:52:00.270: INFO: Pod pod-e88591fd-7347-48cc-9e8b-e8c546ce6338 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:52:00.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7624" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:52:00.280: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:52:00.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-993" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":186,"skipped":3510,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:52:00.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 19 16:52:00.572: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 19 16:53:00.610: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Oct 19 16:53:00.631: INFO: Created pod: pod0-0-sched-preemption-low-priority
Oct 19 16:53:00.639: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Oct 19 16:53:00.655: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Oct 19 16:53:00.662: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:06.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5946" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":187,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:06.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:53:06.899: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Oct 19 16:53:06.906: INFO: Number of nodes with available pods: 0
Oct 19 16:53:06.906: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Oct 19 16:53:06.923: INFO: Number of nodes with available pods: 0
Oct 19 16:53:06.923: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq is running more than one daemon pod
Oct 19 16:53:07.927: INFO: Number of nodes with available pods: 1
Oct 19 16:53:07.927: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Oct 19 16:53:07.944: INFO: Number of nodes with available pods: 1
Oct 19 16:53:07.944: INFO: Number of running nodes: 0, number of available pods: 1
Oct 19 16:53:08.949: INFO: Number of nodes with available pods: 0
Oct 19 16:53:08.949: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Oct 19 16:53:08.958: INFO: Number of nodes with available pods: 0
Oct 19 16:53:08.958: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq is running more than one daemon pod
Oct 19 16:53:09.962: INFO: Number of nodes with available pods: 0
Oct 19 16:53:09.962: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq is running more than one daemon pod
Oct 19 16:53:10.962: INFO: Number of nodes with available pods: 0
Oct 19 16:53:10.962: INFO: Node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq is running more than one daemon pod
Oct 19 16:53:11.971: INFO: Number of nodes with available pods: 1
Oct 19 16:53:11.971: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9620, will wait for the garbage collector to delete the pods
Oct 19 16:53:12.073: INFO: Deleting DaemonSet.extensions daemon-set took: 40.726302ms
Oct 19 16:53:12.174: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.9632ms
Oct 19 16:53:14.878: INFO: Number of nodes with available pods: 0
Oct 19 16:53:14.878: INFO: Number of running nodes: 0, number of available pods: 0
Oct 19 16:53:14.881: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28133"},"items":null}

Oct 19 16:53:14.884: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28133"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:14.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9620" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":188,"skipped":3546,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:14.911: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:22.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7748" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":189,"skipped":3548,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:22.067: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:22.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5428" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":190,"skipped":3554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:22.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-cfd1c437-5bea-449f-bea9-b6cae86b92a3
STEP: Creating a pod to test consume secrets
Oct 19 16:53:22.380: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee" in namespace "projected-9508" to be "Succeeded or Failed"
Oct 19 16:53:22.383: INFO: Pod "pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.757521ms
Oct 19 16:53:24.387: INFO: Pod "pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00678083s
STEP: Saw pod success
Oct 19 16:53:24.387: INFO: Pod "pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee" satisfied condition "Succeeded or Failed"
Oct 19 16:53:24.390: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:53:24.405: INFO: Waiting for pod pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee to disappear
Oct 19 16:53:24.408: INFO: Pod pod-projected-secrets-4cc017a6-7805-419e-97cf-da8c244eceee no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9508" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3594,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:24.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Oct 19 16:53:25.222: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W1019 16:53:25.222405    4339 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Oct 19 16:53:25.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2256" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":192,"skipped":3596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:25.231: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:53:25.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39" in namespace "projected-4580" to be "Succeeded or Failed"
Oct 19 16:53:25.379: INFO: Pod "downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.353412ms
Oct 19 16:53:27.383: INFO: Pod "downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007698508s
STEP: Saw pod success
Oct 19 16:53:27.383: INFO: Pod "downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39" satisfied condition "Succeeded or Failed"
Oct 19 16:53:27.386: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39 container client-container: <nil>
STEP: delete the pod
Oct 19 16:53:27.400: INFO: Waiting for pod downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39 to disappear
Oct 19 16:53:27.403: INFO: Pod downwardapi-volume-4129e446-d20b-43c2-86a3-80e9bb551e39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:27.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4580" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:27.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9394.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9394.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9394.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9394.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9394.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 16.194.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.194.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.194.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.194.16_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9394.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9394.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9394.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9394.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9394.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9394.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 16.194.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.194.16_udp@PTR;check="$$(dig +tcp +noall +answer +search 16.194.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.194.16_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 16:53:29.636: INFO: Unable to read wheezy_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.644: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.701: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.751: INFO: Unable to read jessie_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.758: INFO: Unable to read jessie_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.764: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.769: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:29.803: INFO: Lookups using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 failed for: [wheezy_udp@dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_udp@dns-test-service.dns-9394.svc.cluster.local jessie_tcp@dns-test-service.dns-9394.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local]

Oct 19 16:53:34.810: INFO: Unable to read wheezy_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.815: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.821: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.866: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.949: INFO: Unable to read jessie_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.958: INFO: Unable to read jessie_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.965: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:34.975: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:35.013: INFO: Lookups using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 failed for: [wheezy_udp@dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_udp@dns-test-service.dns-9394.svc.cluster.local jessie_tcp@dns-test-service.dns-9394.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local]

Oct 19 16:53:39.811: INFO: Unable to read wheezy_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.817: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.823: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.829: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.911: INFO: Unable to read jessie_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.923: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.928: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:39.962: INFO: Lookups using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 failed for: [wheezy_udp@dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_udp@dns-test-service.dns-9394.svc.cluster.local jessie_tcp@dns-test-service.dns-9394.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local]

Oct 19 16:53:44.811: INFO: Unable to read wheezy_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.816: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.822: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.827: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.904: INFO: Unable to read jessie_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.916: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.921: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:44.954: INFO: Lookups using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 failed for: [wheezy_udp@dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_udp@dns-test-service.dns-9394.svc.cluster.local jessie_tcp@dns-test-service.dns-9394.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local]

Oct 19 16:53:49.811: INFO: Unable to read wheezy_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.817: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.826: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.870: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.909: INFO: Unable to read jessie_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.919: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.924: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:49.961: INFO: Lookups using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 failed for: [wheezy_udp@dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_udp@dns-test-service.dns-9394.svc.cluster.local jessie_tcp@dns-test-service.dns-9394.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local]

Oct 19 16:53:54.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.863: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.869: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.914: INFO: Unable to read jessie_udp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.919: INFO: Unable to read jessie_tcp@dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.925: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.931: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local from pod dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814: the server could not find the requested resource (get pods dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814)
Oct 19 16:53:54.970: INFO: Lookups using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 failed for: [wheezy_udp@dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@dns-test-service.dns-9394.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_udp@dns-test-service.dns-9394.svc.cluster.local jessie_tcp@dns-test-service.dns-9394.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9394.svc.cluster.local]

Oct 19 16:53:59.956: INFO: DNS probes using dns-9394/dns-test-fb922dbb-a09f-4a92-bd10-2dd1c28a0814 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:53:59.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9394" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":194,"skipped":3656,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:53:59.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:00.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3752" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":195,"skipped":3657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:00.155: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Oct 19 16:54:00.300: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:54:02.304: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:03.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1947" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":196,"skipped":3695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:03.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 16:54:03.475: INFO: Waiting up to 5m0s for pod "downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72" in namespace "projected-8777" to be "Succeeded or Failed"
Oct 19 16:54:03.478: INFO: Pod "downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72": Phase="Pending", Reason="", readiness=false. Elapsed: 3.301698ms
Oct 19 16:54:05.482: INFO: Pod "downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007433235s
STEP: Saw pod success
Oct 19 16:54:05.483: INFO: Pod "downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72" satisfied condition "Succeeded or Failed"
Oct 19 16:54:05.486: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72 container client-container: <nil>
STEP: delete the pod
Oct 19 16:54:05.501: INFO: Waiting for pod downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72 to disappear
Oct 19 16:54:05.504: INFO: Pod downwardapi-volume-900d9d99-6f84-48a2-9fec-6fc1cb7dbb72 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:05.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8777" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3726,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:05.513: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:54:05.994: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:54:09.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:09.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-606" for this suite.
STEP: Destroying namespace "webhook-606-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":198,"skipped":3726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:09.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:54:09.647: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:54:12.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Oct 19 16:54:14.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-1673 attach --namespace=webhook-1673 to-be-attached-pod -i -c=container1'
Oct 19 16:54:15.011: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:15.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1673" for this suite.
STEP: Destroying namespace "webhook-1673-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":199,"skipped":3750,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:15.065: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:31.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6872" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":200,"skipped":3750,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:31.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3962
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3962
STEP: creating replication controller externalsvc in namespace services-3962
I1019 16:54:31.439661    4339 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3962, replica count: 2
I1019 16:54:34.490715    4339 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Oct 19 16:54:34.505: INFO: Creating new exec pod
Oct 19 16:54:36.527: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3962 exec execpodx26bt -- /bin/sh -x -c nslookup clusterip-service.services-3962.svc.cluster.local'
Oct 19 16:54:36.772: INFO: stderr: "+ nslookup clusterip-service.services-3962.svc.cluster.local\n"
Oct 19 16:54:36.772: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-3962.svc.cluster.local\tcanonical name = externalsvc.services-3962.svc.cluster.local.\nName:\texternalsvc.services-3962.svc.cluster.local\nAddress: 100.66.171.10\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3962, will wait for the garbage collector to delete the pods
Oct 19 16:54:36.848: INFO: Deleting ReplicationController externalsvc took: 22.260111ms
Oct 19 16:54:36.949: INFO: Terminating ReplicationController externalsvc pods took: 100.794785ms
Oct 19 16:54:39.158: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:54:39.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3962" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":201,"skipped":3756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:54:39.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:07.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6613" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":202,"skipped":3791,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:07.357: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:10.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1988" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":203,"skipped":3795,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:10.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-dac77384-d5dd-4864-980a-79f0d0752e6b
STEP: Creating a pod to test consume secrets
Oct 19 16:55:10.264: INFO: Waiting up to 5m0s for pod "pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87" in namespace "secrets-6185" to be "Succeeded or Failed"
Oct 19 16:55:10.268: INFO: Pod "pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884727ms
Oct 19 16:55:12.272: INFO: Pod "pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007869158s
STEP: Saw pod success
Oct 19 16:55:12.272: INFO: Pod "pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87" satisfied condition "Succeeded or Failed"
Oct 19 16:55:12.275: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87 container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:55:12.288: INFO: Waiting for pod pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87 to disappear
Oct 19 16:55:12.291: INFO: Pod pod-secrets-666805c5-f2b0-4a41-a931-9d5db3d96b87 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:12.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6185" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:12.300: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-6329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Oct 19 16:55:12.438: INFO: Major version: 1
STEP: Confirm minor version
Oct 19 16:55:12.438: INFO: cleanMinorVersion: 22
Oct 19 16:55:12.438: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:12.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6329" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":205,"skipped":3827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:12.446: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3807/configmap-test-13ac21df-3d13-4875-8708-b7e2f30065c5
STEP: Creating a pod to test consume configMaps
Oct 19 16:55:12.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af" in namespace "configmap-3807" to be "Succeeded or Failed"
Oct 19 16:55:12.599: INFO: Pod "pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.017843ms
Oct 19 16:55:14.604: INFO: Pod "pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009822249s
STEP: Saw pod success
Oct 19 16:55:14.604: INFO: Pod "pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af" satisfied condition "Succeeded or Failed"
Oct 19 16:55:14.607: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af container env-test: <nil>
STEP: delete the pod
Oct 19 16:55:14.621: INFO: Waiting for pod pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af to disappear
Oct 19 16:55:14.624: INFO: Pod pod-configmaps-d50866f6-851a-4826-968b-8b383637e0af no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:14.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3807" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:14.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 16:55:14.799: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Oct 19 16:55:19.803: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 19 16:55:19.803: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 16:55:21.829: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8407  bc92723f-c922-4b22-9bfa-133f6fdb422c 29392 1 2021-10-19 16:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-10-19 16:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 16:55:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f81038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-10-19 16:55:19 +0000 UTC,LastTransitionTime:2021-10-19 16:55:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2021-10-19 16:55:21 +0000 UTC,LastTransitionTime:2021-10-19 16:55:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 19 16:55:21.833: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-8407  beea8636-c48f-49aa-99e5-73d410a7bcfd 29385 1 2021-10-19 16:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment bc92723f-c922-4b22-9bfa-133f6fdb422c 0xc002f813f7 0xc002f813f8}] []  [{kube-controller-manager Update apps/v1 2021-10-19 16:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc92723f-c922-4b22-9bfa-133f6fdb422c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 16:55:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f814a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 19 16:55:21.836: INFO: Pod "test-cleanup-deployment-5b4d99b59b-brltn" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-brltn test-cleanup-deployment-5b4d99b59b- deployment-8407  5dfae453-cbcb-451a-8117-f73a3d70ac85 29384 0 2021-10-19 16:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[cni.projectcalico.org/podIP:100.96.0.239/32 cni.projectcalico.org/podIPs:100.96.0.239/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b beea8636-c48f-49aa-99e5-73d410a7bcfd 0xc002f81867 0xc002f81868}] []  [{kube-controller-manager Update v1 2021-10-19 16:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"beea8636-c48f-49aa-99e5-73d410a7bcfd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 16:55:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 16:55:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zjqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zjqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:55:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 16:55:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.239,StartTime:2021-10-19 16:55:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 16:55:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://5ea7eb8a92ffd75c82371e3aaa8ecae27a56fc074c9169c8896f1ac0b10255bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:21.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8407" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":207,"skipped":3894,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:21.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7379
Oct 19 16:55:21.992: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:55:23.997: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Oct 19 16:55:24.000: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 19 16:55:24.302: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Oct 19 16:55:24.302: INFO: stdout: "iptables"
Oct 19 16:55:24.302: INFO: proxyMode: iptables
Oct 19 16:55:24.311: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 19 16:55:24.315: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7379
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7379
I1019 16:55:24.332238    4339 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7379, replica count: 3
I1019 16:55:27.383400    4339 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 16:55:27.394: INFO: Creating new exec pod
Oct 19 16:55:30.413: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Oct 19 16:55:30.663: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Oct 19 16:55:30.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:55:30.664: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.64.223.117 80'
Oct 19 16:55:30.889: INFO: stderr: "+ nc -v -t -w 2 100.64.223.117 80\nConnection to 100.64.223.117 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Oct 19 16:55:30.889: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:55:30.889: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.123 30883'
Oct 19 16:55:31.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.123 30883\nConnection to 10.250.1.123 30883 port [tcp/*] succeeded!\n"
Oct 19 16:55:31.109: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:55:31.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.120 30883'
Oct 19 16:55:31.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.120 30883\nConnection to 10.250.3.120 30883 port [tcp/*] succeeded!\n"
Oct 19 16:55:31.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:55:31.286: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.123:30883/ ; done'
Oct 19 16:55:31.546: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n"
Oct 19 16:55:31.546: INFO: stdout: "\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc\naffinity-nodeport-timeout-gkkqc"
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Received response from host: affinity-nodeport-timeout-gkkqc
Oct 19 16:55:31.546: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.1.123:30883/'
Oct 19 16:55:31.792: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n"
Oct 19 16:55:31.792: INFO: stdout: "affinity-nodeport-timeout-gkkqc"
Oct 19 16:55:51.792: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7379 exec execpod-affinityp8mh8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.1.123:30883/'
Oct 19 16:55:52.095: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.1.123:30883/\n"
Oct 19 16:55:52.095: INFO: stdout: "affinity-nodeport-timeout-v6flx"
Oct 19 16:55:52.095: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7379, will wait for the garbage collector to delete the pods
Oct 19 16:55:52.158: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.915942ms
Oct 19 16:55:52.259: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.577207ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:54.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7379" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":208,"skipped":3899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:54.279: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Oct 19 16:55:54.423: INFO: Pod name sample-pod: Found 0 pods out of 3
Oct 19 16:55:59.427: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Oct 19 16:55:59.430: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:55:59.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7999" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":209,"skipped":3921,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:55:59.449: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Oct 19 16:55:59.616: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:01.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3487" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":210,"skipped":3940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:01.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6793e87f-50bb-42c0-ab8c-edede24a5e7a
STEP: Creating a pod to test consume secrets
Oct 19 16:56:01.875: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5" in namespace "projected-3412" to be "Succeeded or Failed"
Oct 19 16:56:01.897: INFO: Pod "pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.525207ms
Oct 19 16:56:03.902: INFO: Pod "pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026043615s
STEP: Saw pod success
Oct 19 16:56:03.902: INFO: Pod "pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5" satisfied condition "Succeeded or Failed"
Oct 19 16:56:03.905: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:56:03.960: INFO: Waiting for pod pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5 to disappear
Oct 19 16:56:03.963: INFO: Pod pod-projected-secrets-0c68af79-91f6-4ef2-b1fa-d892364b01e5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:03.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3412" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3973,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:03.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8039
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-8039
Oct 19 16:56:04.124: INFO: Found 0 stateful pods, waiting for 1
Oct 19 16:56:14.130: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 16:56:14.154: INFO: Deleting all statefulset in ns statefulset-8039
Oct 19 16:56:14.157: INFO: Scaling statefulset ss to 0
Oct 19 16:56:24.200: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 16:56:24.203: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:24.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8039" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":212,"skipped":3977,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:24.221: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 19 16:56:24.369: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:56:26.373: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 19 16:56:26.386: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:56:28.391: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 19 16:56:28.406: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 19 16:56:28.415: INFO: Pod pod-with-poststart-http-hook still exists
Oct 19 16:56:30.416: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 19 16:56:30.419: INFO: Pod pod-with-poststart-http-hook still exists
Oct 19 16:56:32.416: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 19 16:56:32.419: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:32.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4780" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":3982,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:32.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:32.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2166" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":214,"skipped":4001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:32.599: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Oct 19 16:56:32.750: INFO: observed Pod pod-test in namespace pods-9739 in phase Pending with labels: map[test-pod-static:true] & conditions []
Oct 19 16:56:32.753: INFO: observed Pod pod-test in namespace pods-9739 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  }]
Oct 19 16:56:32.780: INFO: observed Pod pod-test in namespace pods-9739 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  }]
Oct 19 16:56:33.177: INFO: observed Pod pod-test in namespace pods-9739 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  }]
Oct 19 16:56:34.338: INFO: Found Pod pod-test in namespace pods-9739 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-19 16:56:32 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Oct 19 16:56:34.346: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Oct 19 16:56:34.366: INFO: observed event type ADDED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:34.366: INFO: observed event type MODIFIED
Oct 19 16:56:36.343: INFO: observed event type MODIFIED
Oct 19 16:56:36.484: INFO: observed event type MODIFIED
Oct 19 16:56:37.347: INFO: observed event type MODIFIED
Oct 19 16:56:37.353: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:37.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9739" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":215,"skipped":4030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:37.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Oct 19 16:56:37.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 create -f -'
Oct 19 16:56:37.658: INFO: stderr: ""
Oct 19 16:56:37.658: INFO: stdout: "pod/pause created\n"
Oct 19 16:56:37.658: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct 19 16:56:37.658: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6064" to be "running and ready"
Oct 19 16:56:37.661: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.749898ms
Oct 19 16:56:39.665: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006657418s
Oct 19 16:56:39.665: INFO: Pod "pause" satisfied condition "running and ready"
Oct 19 16:56:39.665: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Oct 19 16:56:39.665: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 label pods pause testing-label=testing-label-value'
Oct 19 16:56:39.721: INFO: stderr: ""
Oct 19 16:56:39.721: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct 19 16:56:39.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 get pod pause -L testing-label'
Oct 19 16:56:39.769: INFO: stderr: ""
Oct 19 16:56:39.769: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct 19 16:56:39.769: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 label pods pause testing-label-'
Oct 19 16:56:39.841: INFO: stderr: ""
Oct 19 16:56:39.841: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct 19 16:56:39.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 get pod pause -L testing-label'
Oct 19 16:56:39.890: INFO: stderr: ""
Oct 19 16:56:39.890: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Oct 19 16:56:39.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 delete --grace-period=0 --force -f -'
Oct 19 16:56:39.949: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 19 16:56:39.949: INFO: stdout: "pod \"pause\" force deleted\n"
Oct 19 16:56:39.949: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 get rc,svc -l name=pause --no-headers'
Oct 19 16:56:39.999: INFO: stderr: "No resources found in kubectl-6064 namespace.\n"
Oct 19 16:56:39.999: INFO: stdout: ""
Oct 19 16:56:39.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6064 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 19 16:56:40.043: INFO: stderr: ""
Oct 19 16:56:40.044: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:56:40.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6064" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":216,"skipped":4053,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:56:40.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 19 16:56:40.189: INFO: PodSpec: initContainers in spec.initContainers
Oct 19 16:57:19.445: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8ef1d248-afdd-4bbd-8521-caa06dcd482c", GenerateName:"", Namespace:"init-container-5838", SelfLink:"", UID:"5f897c4e-3e06-4839-b8e5-5aa8e1984859", ResourceVersion:"30345", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63770259400, loc:(*time.Location)(0xa09bc80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"189841001"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.0.252/32", "cni.projectcalico.org/podIPs":"100.96.0.252/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001df37d0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001df37e8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001df3800), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001df3818), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001df3848), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001df3860), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-vhmd4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0025108e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmhay-ddd.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vhmd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmhay-ddd.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vhmd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmhay-ddd.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-vhmd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0060d8d08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002b9b730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0060d8d80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0060d8da0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0060d8da8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0060d8dac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005daa910), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259400, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259400, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259400, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259400, loc:(*time.Location)(0xa09bc80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.1.123", PodIP:"100.96.0.252", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.0.252"}}, StartTime:(*v1.Time)(0xc001df3890), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b9b810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b9b880)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://39d07dfc175d0b4cc397c9f87bf36cdf7c1aa3f9e49450ffbadd3838c3de61d1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002510960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002510940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0060d8e2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:57:19.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5838" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":217,"skipped":4056,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:57:19.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-395
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 19 16:57:19.598: INFO: Waiting up to 5m0s for pod "pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7" in namespace "emptydir-395" to be "Succeeded or Failed"
Oct 19 16:57:19.600: INFO: Pod "pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.806276ms
Oct 19 16:57:21.605: INFO: Pod "pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007415163s
STEP: Saw pod success
Oct 19 16:57:21.605: INFO: Pod "pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7" satisfied condition "Succeeded or Failed"
Oct 19 16:57:21.608: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7 container test-container: <nil>
STEP: delete the pod
Oct 19 16:57:21.638: INFO: Waiting for pod pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7 to disappear
Oct 19 16:57:21.641: INFO: Pod pod-7dbaf83a-3647-4f71-a4c8-34c1f3d9aef7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:57:21.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-395" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4056,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:57:21.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-315
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-e3d9ce3f-c54d-4fbc-aea6-ea24ff68851b
STEP: Creating secret with name s-test-opt-upd-7fb9e720-b272-4eb6-8885-f76da511b9f0
STEP: Creating the pod
Oct 19 16:57:21.842: INFO: The status of Pod pod-projected-secrets-444e8c45-fb68-401a-a72d-8c474dfd5569 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:57:23.910: INFO: The status of Pod pod-projected-secrets-444e8c45-fb68-401a-a72d-8c474dfd5569 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:57:25.847: INFO: The status of Pod pod-projected-secrets-444e8c45-fb68-401a-a72d-8c474dfd5569 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-e3d9ce3f-c54d-4fbc-aea6-ea24ff68851b
STEP: Updating secret s-test-opt-upd-7fb9e720-b272-4eb6-8885-f76da511b9f0
STEP: Creating secret with name s-test-opt-create-ed988ca5-dc2d-465c-b18f-a48ddf06df61
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:58:40.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-315" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":4062,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:58:40.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-2hqb
STEP: Creating a pod to test atomic-volume-subpath
Oct 19 16:58:40.702: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2hqb" in namespace "subpath-2818" to be "Succeeded or Failed"
Oct 19 16:58:40.706: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.694649ms
Oct 19 16:58:42.710: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007857413s
Oct 19 16:58:44.716: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 4.013181937s
Oct 19 16:58:46.720: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 6.017689319s
Oct 19 16:58:48.725: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 8.022415483s
Oct 19 16:58:50.730: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 10.027555889s
Oct 19 16:58:52.734: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 12.031747406s
Oct 19 16:58:54.739: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 14.036518917s
Oct 19 16:58:56.744: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 16.041642811s
Oct 19 16:58:58.749: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 18.046674003s
Oct 19 16:59:00.754: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Running", Reason="", readiness=true. Elapsed: 20.051561644s
Oct 19 16:59:02.759: INFO: Pod "pod-subpath-test-projected-2hqb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056284048s
STEP: Saw pod success
Oct 19 16:59:02.759: INFO: Pod "pod-subpath-test-projected-2hqb" satisfied condition "Succeeded or Failed"
Oct 19 16:59:02.762: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq pod pod-subpath-test-projected-2hqb container test-container-subpath-projected-2hqb: <nil>
STEP: delete the pod
Oct 19 16:59:02.781: INFO: Waiting for pod pod-subpath-test-projected-2hqb to disappear
Oct 19 16:59:02.784: INFO: Pod pod-subpath-test-projected-2hqb no longer exists
STEP: Deleting pod pod-subpath-test-projected-2hqb
Oct 19 16:59:02.784: INFO: Deleting pod "pod-subpath-test-projected-2hqb" in namespace "subpath-2818"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:02.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2818" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":220,"skipped":4076,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:02.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-19
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-19
STEP: creating service affinity-nodeport in namespace services-19
STEP: creating replication controller affinity-nodeport in namespace services-19
I1019 16:59:02.950178    4339 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-19, replica count: 3
I1019 16:59:06.001655    4339 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 16:59:06.012: INFO: Creating new exec pod
Oct 19 16:59:09.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-19 exec execpod-affinity86gk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Oct 19 16:59:09.308: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Oct 19 16:59:09.308: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:59:09.308: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-19 exec execpod-affinity86gk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.109.84 80'
Oct 19 16:59:09.569: INFO: stderr: "+ nc -v -t -w 2 100.67.109.84 80\n+ echo hostName\nConnection to 100.67.109.84 80 port [tcp/http] succeeded!\n"
Oct 19 16:59:09.569: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:59:09.569: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-19 exec execpod-affinity86gk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.123 31229'
Oct 19 16:59:09.792: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.123 31229\nConnection to 10.250.1.123 31229 port [tcp/*] succeeded!\n"
Oct 19 16:59:09.792: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:59:09.792: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-19 exec execpod-affinity86gk6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.120 31229'
Oct 19 16:59:10.012: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.120 31229\nConnection to 10.250.3.120 31229 port [tcp/*] succeeded!\n"
Oct 19 16:59:10.012: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:59:10.012: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-19 exec execpod-affinity86gk6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.123:31229/ ; done'
Oct 19 16:59:10.317: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31229/\n"
Oct 19 16:59:10.317: INFO: stdout: "\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk\naffinity-nodeport-dnztk"
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Received response from host: affinity-nodeport-dnztk
Oct 19 16:59:10.317: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-19, will wait for the garbage collector to delete the pods
Oct 19 16:59:10.386: INFO: Deleting ReplicationController affinity-nodeport took: 3.74126ms
Oct 19 16:59:10.486: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.515771ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:12.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-19" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":221,"skipped":4090,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:12.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 16:59:13.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 16:59:16.245: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Oct 19 16:59:16.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7563" for this suite.
STEP: Destroying namespace "webhook-7563-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":222,"skipped":4107,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:16.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-0f5ed138-3445-499d-a16f-686280de5396
STEP: Creating a pod to test consume secrets
Oct 19 16:59:16.580: INFO: Waiting up to 5m0s for pod "pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713" in namespace "secrets-3212" to be "Succeeded or Failed"
Oct 19 16:59:16.583: INFO: Pod "pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713": Phase="Pending", Reason="", readiness=false. Elapsed: 3.126423ms
Oct 19 16:59:18.587: INFO: Pod "pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006821818s
STEP: Saw pod success
Oct 19 16:59:18.587: INFO: Pod "pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713" satisfied condition "Succeeded or Failed"
Oct 19 16:59:18.589: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713 container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:59:18.605: INFO: Waiting for pod pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713 to disappear
Oct 19 16:59:18.608: INFO: Pod pod-secrets-e277a9f3-cf70-4461-a039-6c3f73d11713 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:18.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3212" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:18.617: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:18.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8501" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":224,"skipped":4178,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:18.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 19 16:59:18.930: INFO: Waiting up to 5m0s for pod "downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475" in namespace "downward-api-1540" to be "Succeeded or Failed"
Oct 19 16:59:18.935: INFO: Pod "downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475": Phase="Pending", Reason="", readiness=false. Elapsed: 4.884002ms
Oct 19 16:59:20.939: INFO: Pod "downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008997265s
STEP: Saw pod success
Oct 19 16:59:20.939: INFO: Pod "downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475" satisfied condition "Succeeded or Failed"
Oct 19 16:59:20.969: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475 container dapi-container: <nil>
STEP: delete the pod
Oct 19 16:59:20.983: INFO: Waiting for pod downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475 to disappear
Oct 19 16:59:20.986: INFO: Pod downward-api-4fa8ed73-a554-4a49-b1a4-b34a7b8aa475 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:20.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1540" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:20.995: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Oct 19 16:59:21.130: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5845 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:21.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5845" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":226,"skipped":4231,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:21.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3344
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-f039f143-7b08-4137-aca8-7db9b35594b7
STEP: Creating a pod to test consume configMaps
Oct 19 16:59:21.323: INFO: Waiting up to 5m0s for pod "pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30" in namespace "configmap-3344" to be "Succeeded or Failed"
Oct 19 16:59:21.327: INFO: Pod "pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237909ms
Oct 19 16:59:23.331: INFO: Pod "pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008310979s
STEP: Saw pod success
Oct 19 16:59:23.331: INFO: Pod "pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30" satisfied condition "Succeeded or Failed"
Oct 19 16:59:23.334: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 16:59:23.388: INFO: Waiting for pod pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30 to disappear
Oct 19 16:59:23.391: INFO: Pod pod-configmaps-6703a088-352e-4f7c-bd94-e76a57128e30 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:23.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3344" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4234,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:23.399: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-937a44a3-9520-4392-a811-2d6e23d03522
STEP: Creating a pod to test consume secrets
Oct 19 16:59:23.547: INFO: Waiting up to 5m0s for pod "pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf" in namespace "secrets-2964" to be "Succeeded or Failed"
Oct 19 16:59:23.550: INFO: Pod "pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.82142ms
Oct 19 16:59:25.554: INFO: Pod "pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006420664s
STEP: Saw pod success
Oct 19 16:59:25.554: INFO: Pod "pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf" satisfied condition "Succeeded or Failed"
Oct 19 16:59:25.557: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:59:25.571: INFO: Waiting for pod pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf to disappear
Oct 19 16:59:25.574: INFO: Pod pod-secrets-753346ff-59d1-4a74-80e0-11b348153abf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:25.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2964" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":228,"skipped":4241,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:25.583: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9585
Oct 19 16:59:25.726: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Oct 19 16:59:27.731: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Oct 19 16:59:27.734: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9585 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 19 16:59:27.951: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Oct 19 16:59:27.951: INFO: stdout: "iptables"
Oct 19 16:59:27.951: INFO: proxyMode: iptables
Oct 19 16:59:27.958: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 19 16:59:27.961: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9585
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9585
I1019 16:59:27.973272    4339 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9585, replica count: 3
I1019 16:59:31.024594    4339 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 16:59:31.031: INFO: Creating new exec pod
Oct 19 16:59:34.046: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9585 exec execpod-affinityczzcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Oct 19 16:59:34.218: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Oct 19 16:59:34.218: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:59:34.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9585 exec execpod-affinityczzcg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.155.17 80'
Oct 19 16:59:34.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.155.17 80\nConnection to 100.71.155.17 80 port [tcp/http] succeeded!\n"
Oct 19 16:59:34.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 16:59:34.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9585 exec execpod-affinityczzcg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.155.17:80/ ; done'
Oct 19 16:59:34.724: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n"
Oct 19 16:59:34.724: INFO: stdout: "\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f\naffinity-clusterip-timeout-bll5f"
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Received response from host: affinity-clusterip-timeout-bll5f
Oct 19 16:59:34.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9585 exec execpod-affinityczzcg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.71.155.17:80/'
Oct 19 16:59:34.915: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n"
Oct 19 16:59:34.915: INFO: stdout: "affinity-clusterip-timeout-bll5f"
Oct 19 16:59:54.915: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-9585 exec execpod-affinityczzcg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.71.155.17:80/'
Oct 19 16:59:55.185: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.71.155.17:80/\n"
Oct 19 16:59:55.185: INFO: stdout: "affinity-clusterip-timeout-84hx5"
Oct 19 16:59:55.185: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9585, will wait for the garbage collector to delete the pods
Oct 19 16:59:55.254: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.266793ms
Oct 19 16:59:55.355: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.181344ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:56.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9585" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":229,"skipped":4249,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:57.001: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8a5c317f-16b7-4871-bbc9-2373e74ff2ee
STEP: Creating a pod to test consume secrets
Oct 19 16:59:57.147: INFO: Waiting up to 5m0s for pod "pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983" in namespace "secrets-7746" to be "Succeeded or Failed"
Oct 19 16:59:57.151: INFO: Pod "pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541706ms
Oct 19 16:59:59.155: INFO: Pod "pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007693854s
STEP: Saw pod success
Oct 19 16:59:59.155: INFO: Pod "pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983" satisfied condition "Succeeded or Failed"
Oct 19 16:59:59.158: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983 container secret-volume-test: <nil>
STEP: delete the pod
Oct 19 16:59:59.173: INFO: Waiting for pod pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983 to disappear
Oct 19 16:59:59.176: INFO: Pod pod-secrets-d23dad26-bfdf-46a9-96b6-e7c44f8a6983 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 16:59:59.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7746" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4254,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 16:59:59.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-166
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Oct 19 16:59:59.332: INFO: Found 0 stateful pods, waiting for 3
Oct 19 17:00:09.336: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 17:00:09.336: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 17:00:09.336: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 17:00:09.346: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-166 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 17:00:09.564: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 17:00:09.564: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 17:00:09.564: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Oct 19 17:00:19.601: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Oct 19 17:00:29.621: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-166 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 17:00:29.853: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 17:00:29.853: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 17:00:29.853: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Oct 19 17:00:39.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-166 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 19 17:00:40.143: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 19 17:00:40.143: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 19 17:00:40.143: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 19 17:00:50.176: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Oct 19 17:01:00.197: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-166 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 19 17:01:00.428: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 19 17:01:00.428: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 19 17:01:00.428: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 17:01:10.448: INFO: Deleting all statefulset in ns statefulset-166
Oct 19 17:01:10.451: INFO: Scaling statefulset ss2 to 0
Oct 19 17:01:20.470: INFO: Waiting for statefulset status.replicas updated to 0
Oct 19 17:01:20.473: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:20.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-166" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":231,"skipped":4254,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:20.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5942
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-b21d9735-0d0a-4cc4-9e3a-7913f0970bd9
STEP: Creating configMap with name cm-test-opt-upd-5ab115b9-574f-430a-88b5-8ed9f2d45145
STEP: Creating the pod
Oct 19 17:01:20.652: INFO: The status of Pod pod-configmaps-8f1379ad-a74e-4ccc-99b1-57506f6bc87c is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:01:22.657: INFO: The status of Pod pod-configmaps-8f1379ad-a74e-4ccc-99b1-57506f6bc87c is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-b21d9735-0d0a-4cc4-9e3a-7913f0970bd9
STEP: Updating configmap cm-test-opt-upd-5ab115b9-574f-430a-88b5-8ed9f2d45145
STEP: Creating configMap with name cm-test-opt-create-e188eeed-876f-4dfd-bc32-8954c88bcde2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:24.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5942" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:24.844: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:01:24.999: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-9647fa52-f6e4-41d6-956b-8695235472d6" in namespace "security-context-test-4647" to be "Succeeded or Failed"
Oct 19 17:01:25.008: INFO: Pod "busybox-readonly-false-9647fa52-f6e4-41d6-956b-8695235472d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.650334ms
Oct 19 17:01:27.012: INFO: Pod "busybox-readonly-false-9647fa52-f6e4-41d6-956b-8695235472d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012849707s
Oct 19 17:01:27.012: INFO: Pod "busybox-readonly-false-9647fa52-f6e4-41d6-956b-8695235472d6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:27.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4647" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":4293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:27.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:01:27.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87" in namespace "downward-api-6548" to be "Succeeded or Failed"
Oct 19 17:01:27.165: INFO: Pod "downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.427301ms
Oct 19 17:01:29.169: INFO: Pod "downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006989826s
STEP: Saw pod success
Oct 19 17:01:29.169: INFO: Pod "downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87" satisfied condition "Succeeded or Failed"
Oct 19 17:01:29.172: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq pod downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87 container client-container: <nil>
STEP: delete the pod
Oct 19 17:01:29.232: INFO: Waiting for pod downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87 to disappear
Oct 19 17:01:29.236: INFO: Pod downwardapi-volume-f513f0f8-5524-43c6-94d7-83053f3b8e87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:29.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6548" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:29.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1100
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:01:29.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 19 17:01:32.244: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-1100 --namespace=crd-publish-openapi-1100 create -f -'
Oct 19 17:01:32.553: INFO: stderr: ""
Oct 19 17:01:32.553: INFO: stdout: "e2e-test-crd-publish-openapi-5488-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 19 17:01:32.553: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-1100 --namespace=crd-publish-openapi-1100 delete e2e-test-crd-publish-openapi-5488-crds test-cr'
Oct 19 17:01:32.606: INFO: stderr: ""
Oct 19 17:01:32.606: INFO: stdout: "e2e-test-crd-publish-openapi-5488-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Oct 19 17:01:32.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-1100 --namespace=crd-publish-openapi-1100 apply -f -'
Oct 19 17:01:32.736: INFO: stderr: ""
Oct 19 17:01:32.736: INFO: stdout: "e2e-test-crd-publish-openapi-5488-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 19 17:01:32.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-1100 --namespace=crd-publish-openapi-1100 delete e2e-test-crd-publish-openapi-5488-crds test-cr'
Oct 19 17:01:32.788: INFO: stderr: ""
Oct 19 17:01:32.788: INFO: stdout: "e2e-test-crd-publish-openapi-5488-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 19 17:01:32.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-1100 explain e2e-test-crd-publish-openapi-5488-crds'
Oct 19 17:01:32.907: INFO: stderr: ""
Oct 19 17:01:32.907: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5488-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:35.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1100" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":235,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:35.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:35.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-498" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":236,"skipped":4391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:35.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Oct 19 17:01:36.105: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:01:38.109: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Oct 19 17:01:38.125: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:01:40.128: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct 19 17:01:40.131: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:40.131: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:40.291: INFO: Exec stderr: ""
Oct 19 17:01:40.291: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:40.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:40.499: INFO: Exec stderr: ""
Oct 19 17:01:40.499: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:40.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:40.706: INFO: Exec stderr: ""
Oct 19 17:01:40.706: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:40.706: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:40.867: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct 19 17:01:40.867: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:40.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:41.123: INFO: Exec stderr: ""
Oct 19 17:01:41.123: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:41.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:41.302: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct 19 17:01:41.302: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:41.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:41.512: INFO: Exec stderr: ""
Oct 19 17:01:41.512: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:41.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:41.720: INFO: Exec stderr: ""
Oct 19 17:01:41.720: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:41.720: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:41.927: INFO: Exec stderr: ""
Oct 19 17:01:41.927: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2798 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:01:41.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:01:42.129: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:42.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2798" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4414,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:42.137: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 17:01:42.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 17:01:45.610: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6394" for this suite.
STEP: Destroying namespace "webhook-6394-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":238,"skipped":4415,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:45.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-3650
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:01:47.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3650" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":239,"skipped":4418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:01:47.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-191bc7f0-2ec4-48e5-bc15-4b76c5fe8a5b in namespace container-probe-2227
Oct 19 17:01:50.044: INFO: Started pod liveness-191bc7f0-2ec4-48e5-bc15-4b76c5fe8a5b in namespace container-probe-2227
STEP: checking the pod's current state and verifying that restartCount is present
Oct 19 17:01:50.047: INFO: Initial restart count of pod liveness-191bc7f0-2ec4-48e5-bc15-4b76c5fe8a5b is 0
Oct 19 17:02:10.100: INFO: Restart count of pod container-probe-2227/liveness-191bc7f0-2ec4-48e5-bc15-4b76c5fe8a5b is now 1 (20.052177148s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:02:10.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2227" for this suite.
•{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:02:10.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7799
STEP: creating service affinity-clusterip-transition in namespace services-7799
STEP: creating replication controller affinity-clusterip-transition in namespace services-7799
I1019 17:02:10.467370    4339 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7799, replica count: 3
I1019 17:02:13.518389    4339 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 17:02:13.524: INFO: Creating new exec pod
Oct 19 17:02:16.538: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7799 exec execpod-affinityqgjrj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Oct 19 17:02:16.825: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Oct 19 17:02:16.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 17:02:16.825: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7799 exec execpod-affinityqgjrj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.64.33.93 80'
Oct 19 17:02:17.032: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.64.33.93 80\nConnection to 100.64.33.93 80 port [tcp/http] succeeded!\n"
Oct 19 17:02:17.032: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 17:02:17.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7799 exec execpod-affinityqgjrj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.33.93:80/ ; done'
Oct 19 17:02:17.305: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n"
Oct 19 17:02:17.305: INFO: stdout: "\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj"
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:17.305: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.305: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7799 exec execpod-affinityqgjrj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.33.93:80/ ; done'
Oct 19 17:02:47.638: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n"
Oct 19 17:02:47.639: INFO: stdout: "\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-nrcdj\naffinity-clusterip-transition-2djbw\naffinity-clusterip-transition-ndbfk"
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-nrcdj
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-2djbw
Oct 19 17:02:47.639: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:47.649: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7799 exec execpod-affinityqgjrj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.33.93:80/ ; done'
Oct 19 17:02:48.002: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.33.93:80/\n"
Oct 19 17:02:48.002: INFO: stdout: "\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk\naffinity-clusterip-transition-ndbfk"
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Received response from host: affinity-clusterip-transition-ndbfk
Oct 19 17:02:48.002: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7799, will wait for the garbage collector to delete the pods
Oct 19 17:02:48.066: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.046817ms
Oct 19 17:02:48.167: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.858816ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:02:50.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7799" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":241,"skipped":4468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:02:50.285: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:02:54.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4861" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4509,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:02:54.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-tfg2
STEP: Creating a pod to test atomic-volume-subpath
Oct 19 17:02:54.592: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tfg2" in namespace "subpath-5079" to be "Succeeded or Failed"
Oct 19 17:02:54.596: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338844ms
Oct 19 17:02:56.600: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007707817s
Oct 19 17:02:58.604: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 4.012090015s
Oct 19 17:03:00.608: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 6.016079196s
Oct 19 17:03:02.621: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 8.029161408s
Oct 19 17:03:04.625: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 10.033063146s
Oct 19 17:03:06.629: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 12.036924854s
Oct 19 17:03:08.634: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 14.041504717s
Oct 19 17:03:10.638: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 16.045968549s
Oct 19 17:03:12.642: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 18.050160679s
Oct 19 17:03:14.647: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Running", Reason="", readiness=true. Elapsed: 20.055239163s
Oct 19 17:03:16.652: INFO: Pod "pod-subpath-test-secret-tfg2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06015939s
STEP: Saw pod success
Oct 19 17:03:16.652: INFO: Pod "pod-subpath-test-secret-tfg2" satisfied condition "Succeeded or Failed"
Oct 19 17:03:16.656: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-subpath-test-secret-tfg2 container test-container-subpath-secret-tfg2: <nil>
STEP: delete the pod
Oct 19 17:03:16.671: INFO: Waiting for pod pod-subpath-test-secret-tfg2 to disappear
Oct 19 17:03:16.673: INFO: Pod pod-subpath-test-secret-tfg2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-tfg2
Oct 19 17:03:16.673: INFO: Deleting pod "pod-subpath-test-secret-tfg2" in namespace "subpath-5079"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:16.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5079" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":243,"skipped":4515,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:16.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8550
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8550
STEP: creating replication controller externalsvc in namespace services-8550
I1019 17:03:16.908490    4339 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8550, replica count: 2
I1019 17:03:19.960864    4339 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Oct 19 17:03:19.976: INFO: Creating new exec pod
Oct 19 17:03:21.990: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8550 exec execpod28dgn -- /bin/sh -x -c nslookup nodeport-service.services-8550.svc.cluster.local'
Oct 19 17:03:22.269: INFO: stderr: "+ nslookup nodeport-service.services-8550.svc.cluster.local\n"
Oct 19 17:03:22.269: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-8550.svc.cluster.local\tcanonical name = externalsvc.services-8550.svc.cluster.local.\nName:\texternalsvc.services-8550.svc.cluster.local\nAddress: 100.70.132.29\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8550, will wait for the garbage collector to delete the pods
Oct 19 17:03:22.326: INFO: Deleting ReplicationController externalsvc took: 3.804356ms
Oct 19 17:03:22.427: INFO: Terminating ReplicationController externalsvc pods took: 100.296342ms
Oct 19 17:03:24.335: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:24.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8550" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":244,"skipped":4515,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:24.350: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-33
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:41.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-33" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":245,"skipped":4529,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:41.550: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-7526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Oct 19 17:03:41.701: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Oct 19 17:03:43.773: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:45.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7526" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":246,"skipped":4542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:45.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 17:03:46.298: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 17:03:49.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:49.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9285" for this suite.
STEP: Destroying namespace "webhook-9285-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":247,"skipped":4579,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:49.559: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 17:03:50.141: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 17:03:53.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:53.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8724" for this suite.
STEP: Destroying namespace "webhook-8724-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":248,"skipped":4588,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:53.368: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5392
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct 19 17:03:53.515: INFO: Waiting up to 5m0s for pod "pod-a3766eb1-aede-4975-8be8-4aea311de008" in namespace "emptydir-5392" to be "Succeeded or Failed"
Oct 19 17:03:53.519: INFO: Pod "pod-a3766eb1-aede-4975-8be8-4aea311de008": Phase="Pending", Reason="", readiness=false. Elapsed: 3.499667ms
Oct 19 17:03:55.523: INFO: Pod "pod-a3766eb1-aede-4975-8be8-4aea311de008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00764008s
STEP: Saw pod success
Oct 19 17:03:55.523: INFO: Pod "pod-a3766eb1-aede-4975-8be8-4aea311de008" satisfied condition "Succeeded or Failed"
Oct 19 17:03:55.526: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-a3766eb1-aede-4975-8be8-4aea311de008 container test-container: <nil>
STEP: delete the pod
Oct 19 17:03:55.540: INFO: Waiting for pod pod-a3766eb1-aede-4975-8be8-4aea311de008 to disappear
Oct 19 17:03:55.543: INFO: Pod pod-a3766eb1-aede-4975-8be8-4aea311de008 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:03:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5392" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4592,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:03:55.552: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9961.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9961.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 17:03:57.843: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.850: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.857: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.903: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.919: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.925: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.930: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.936: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:03:57.946: INFO: Lookups using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local]

Oct 19 17:04:02.954: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:02.961: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:02.975: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:03.019: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:03.035: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:03.040: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:03.045: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:03.050: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:03.061: INFO: Lookups using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local]

Oct 19 17:04:07.954: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:07.960: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.005: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.013: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.031: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.037: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.042: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.049: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:08.061: INFO: Lookups using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local]

Oct 19 17:04:12.957: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:12.962: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:12.968: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:13.011: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:13.039: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:13.044: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:13.049: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:13.054: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:13.064: INFO: Lookups using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local]

Oct 19 17:04:17.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:17.958: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.003: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.009: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.025: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.030: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.035: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.041: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:18.052: INFO: Lookups using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local]

Oct 19 17:04:22.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:22.999: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.005: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.010: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.064: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.077: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.084: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.089: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local from pod dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07: the server could not find the requested resource (get pods dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07)
Oct 19 17:04:23.100: INFO: Lookups using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9961.svc.cluster.local jessie_udp@dns-test-service-2.dns-9961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9961.svc.cluster.local]

Oct 19 17:04:28.062: INFO: DNS probes using dns-9961/dns-test-284f68e9-5176-48c3-9521-871a8c1f0d07 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:04:28.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9961" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":250,"skipped":4600,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:04:28.101: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Oct 19 17:04:28.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Oct 19 17:04:28.552: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Oct 19 17:04:30.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 17:04:32.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 17:04:34.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63770259868, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 19 17:04:37.723: INFO: Waited 1.117903342s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Oct 19 17:04:37.971: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:04:38.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3460" for this suite.
•{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":251,"skipped":4610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:04:38.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Oct 19 17:04:38.587: INFO: Waiting up to 5m0s for pod "client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556" in namespace "containers-2757" to be "Succeeded or Failed"
Oct 19 17:04:38.594: INFO: Pod "client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279103ms
Oct 19 17:04:40.601: INFO: Pod "client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013296572s
STEP: Saw pod success
Oct 19 17:04:40.601: INFO: Pod "client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556" satisfied condition "Succeeded or Failed"
Oct 19 17:04:40.609: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 17:04:40.626: INFO: Waiting for pod client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556 to disappear
Oct 19 17:04:40.629: INFO: Pod client-containers-71b8294a-9a72-40d8-80d8-3d4c0ebb8556 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:04:40.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2757" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4646,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:04:40.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:04:42.789: INFO: Deleting pod "var-expansion-f4d4f511-2028-44cf-b306-4427df63b09d" in namespace "var-expansion-2111"
Oct 19 17:04:42.794: INFO: Wait up to 5m0s for pod "var-expansion-f4d4f511-2028-44cf-b306-4427df63b09d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:04:44.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2111" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":253,"skipped":4658,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:04:44.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Oct 19 17:04:44.955: INFO: Waiting up to 5m0s for pod "var-expansion-e9cedac1-966f-470f-96d5-7434589cb198" in namespace "var-expansion-5476" to be "Succeeded or Failed"
Oct 19 17:04:44.958: INFO: Pod "var-expansion-e9cedac1-966f-470f-96d5-7434589cb198": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675492ms
Oct 19 17:04:46.963: INFO: Pod "var-expansion-e9cedac1-966f-470f-96d5-7434589cb198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007814365s
STEP: Saw pod success
Oct 19 17:04:46.963: INFO: Pod "var-expansion-e9cedac1-966f-470f-96d5-7434589cb198" satisfied condition "Succeeded or Failed"
Oct 19 17:04:46.966: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod var-expansion-e9cedac1-966f-470f-96d5-7434589cb198 container dapi-container: <nil>
STEP: delete the pod
Oct 19 17:04:46.988: INFO: Waiting for pod var-expansion-e9cedac1-966f-470f-96d5-7434589cb198 to disappear
Oct 19 17:04:46.994: INFO: Pod var-expansion-e9cedac1-966f-470f-96d5-7434589cb198 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:04:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5476" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4678,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:04:47.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8359
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:04:47.159: INFO: created pod
Oct 19 17:04:47.159: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8359" to be "Succeeded or Failed"
Oct 19 17:04:47.165: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365649ms
Oct 19 17:04:49.170: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011740196s
STEP: Saw pod success
Oct 19 17:04:49.170: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Oct 19 17:05:19.172: INFO: polling logs
Oct 19 17:05:19.181: INFO: Pod logs: 
2021/10/19 17:04:47 OK: Got token
2021/10/19 17:04:47 validating with in-cluster discovery
2021/10/19 17:04:47 OK: got issuer https://api.tmhay-ddd.it.internal.staging.k8s.ondemand.com
2021/10/19 17:04:47 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmhay-ddd.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-8359:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1634663687, NotBefore:1634663087, IssuedAt:1634663087, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8359", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0812ad02-1af8-4219-a4d8-4c4bd1b9610a"}}}
2021/10/19 17:04:47 OK: Constructed OIDC provider for issuer https://api.tmhay-ddd.it.internal.staging.k8s.ondemand.com
2021/10/19 17:04:47 OK: Validated signature on JWT
2021/10/19 17:04:47 OK: Got valid claims from token!
2021/10/19 17:04:47 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmhay-ddd.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-8359:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1634663687, NotBefore:1634663087, IssuedAt:1634663087, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8359", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"0812ad02-1af8-4219-a4d8-4c4bd1b9610a"}}}

Oct 19 17:05:19.181: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:19.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8359" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":255,"skipped":4684,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:19.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Oct 19 17:05:19.364: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5474  a141ec39-2cfc-42b9-a192-9e3d861916ad 34233 0 2021-10-19 17:05:19 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-10-19 17:05:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-656kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-656kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 17:05:19.367: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:05:21.372: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Oct 19 17:05:21.372: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5474 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:05:21.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Verifying customized DNS server is configured on pod...
Oct 19 17:05:21.580: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5474 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 19 17:05:21.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:05:21.830: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:21.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5474" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":256,"skipped":4689,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:21.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-4073ded6-43b1-453d-a45f-bf1f1d48b611
STEP: Creating a pod to test consume configMaps
Oct 19 17:05:22.000: INFO: Waiting up to 5m0s for pod "pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb" in namespace "configmap-1833" to be "Succeeded or Failed"
Oct 19 17:05:22.004: INFO: Pod "pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51002ms
Oct 19 17:05:24.008: INFO: Pod "pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008086424s
STEP: Saw pod success
Oct 19 17:05:24.008: INFO: Pod "pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb" satisfied condition "Succeeded or Failed"
Oct 19 17:05:24.012: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb container agnhost-container: <nil>
STEP: delete the pod
Oct 19 17:05:24.026: INFO: Waiting for pod pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb to disappear
Oct 19 17:05:24.029: INFO: Pod pod-configmaps-84721dc5-74a3-4c96-a5fa-344c8eb9c2bb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:24.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1833" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:24.038: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:05:24.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4" in namespace "downward-api-4173" to be "Succeeded or Failed"
Oct 19 17:05:24.183: INFO: Pod "downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785268ms
Oct 19 17:05:26.188: INFO: Pod "downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007493074s
STEP: Saw pod success
Oct 19 17:05:26.188: INFO: Pod "downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4" satisfied condition "Succeeded or Failed"
Oct 19 17:05:26.191: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4 container client-container: <nil>
STEP: delete the pod
Oct 19 17:05:26.206: INFO: Waiting for pod downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4 to disappear
Oct 19 17:05:26.209: INFO: Pod downwardapi-volume-15c85fab-e2cf-4005-a028-b38ee0c110b4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:26.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4173" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4722,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:26.217: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-2545
STEP: creating replication controller nodeport-test in namespace services-2545
I1019 17:05:26.362977    4339 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2545, replica count: 2
I1019 17:05:29.414482    4339 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 17:05:29.414: INFO: Creating new exec pod
Oct 19 17:05:32.438: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2545 exec execpodrtpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Oct 19 17:05:32.724: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Oct 19 17:05:32.724: INFO: stdout: ""
Oct 19 17:05:33.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2545 exec execpodrtpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Oct 19 17:05:33.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Oct 19 17:05:33.939: INFO: stdout: "nodeport-test-268wp"
Oct 19 17:05:33.940: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2545 exec execpodrtpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.68.186.103 80'
Oct 19 17:05:34.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.68.186.103 80\nConnection to 100.68.186.103 80 port [tcp/http] succeeded!\n"
Oct 19 17:05:34.149: INFO: stdout: "nodeport-test-nvm88"
Oct 19 17:05:34.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2545 exec execpodrtpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.123 31958'
Oct 19 17:05:34.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.123 31958\nConnection to 10.250.1.123 31958 port [tcp/*] succeeded!\n"
Oct 19 17:05:34.359: INFO: stdout: ""
Oct 19 17:05:35.359: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2545 exec execpodrtpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.123 31958'
Oct 19 17:05:35.588: INFO: stderr: "+ nc -v -t -w 2 10.250.1.123 31958\n+ echo hostName\nConnection to 10.250.1.123 31958 port [tcp/*] succeeded!\n"
Oct 19 17:05:35.588: INFO: stdout: "nodeport-test-268wp"
Oct 19 17:05:35.588: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-2545 exec execpodrtpd7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.120 31958'
Oct 19 17:05:35.869: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.120 31958\nConnection to 10.250.3.120 31958 port [tcp/*] succeeded!\n"
Oct 19 17:05:35.869: INFO: stdout: "nodeport-test-268wp"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:35.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2545" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":259,"skipped":4724,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:35.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 17:05:36.399: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 17:05:39.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:39.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3878" for this suite.
STEP: Destroying namespace "webhook-3878-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":260,"skipped":4726,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:39.986: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 19 17:05:40.120: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 19 17:05:40.128: INFO: Waiting for terminating namespaces to be deleted...
Oct 19 17:05:40.139: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 before test
Oct 19 17:05:40.148: INFO: addons-nginx-ingress-controller-6ccd9d5d4d-87wtm from kube-system started at 2021-10-19 16:20:45 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Oct 19 17:05:40.148: INFO: apiserver-proxy-ftftt from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container proxy ready: true, restart count 0
Oct 19 17:05:40.148: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 17:05:40.148: INFO: blackbox-exporter-65c549b94c-c5pzd from kube-system started at 2021-10-19 15:51:26 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container blackbox-exporter ready: true, restart count 0
Oct 19 17:05:40.148: INFO: calico-kube-controllers-86c64d79ff-hmgq6 from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct 19 17:05:40.148: INFO: calico-node-gkqll from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 17:05:40.148: INFO: calico-typha-deploy-58b94ff46-kljnn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container calico-typha ready: true, restart count 0
Oct 19 17:05:40.148: INFO: csi-driver-node-twl5g from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 17:05:40.148: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 17:05:40.148: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 17:05:40.148: INFO: kube-proxy-hgtmc from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 17:05:40.148: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 17:05:40.148: INFO: node-exporter-v9h4r from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 17:05:40.148: INFO: node-problem-detector-2s6bt from kube-system started at 2021-10-19 16:11:27 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 17:05:40.148: INFO: execpodrtpd7 from services-2545 started at 2021-10-19 17:05:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container agnhost-container ready: true, restart count 0
Oct 19 17:05:40.148: INFO: nodeport-test-nvm88 from services-2545 started at 2021-10-19 17:05:26 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.148: INFO: 	Container nodeport-test ready: true, restart count 0
Oct 19 17:05:40.148: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq before test
Oct 19 17:05:40.159: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-56d9d84c8c-ftj5w from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Oct 19 17:05:40.159: INFO: apiserver-proxy-r6qsz from kube-system started at 2021-10-19 15:45:29 +0000 UTC (2 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container proxy ready: true, restart count 0
Oct 19 17:05:40.159: INFO: 	Container sidecar ready: true, restart count 0
Oct 19 17:05:40.159: INFO: calico-node-54s6z from kube-system started at 2021-10-19 15:46:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container calico-node ready: true, restart count 0
Oct 19 17:05:40.159: INFO: calico-node-vertical-autoscaler-785b5f968-w77tx from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 17:05:40.159: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-bqq7q from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 17:05:40.159: INFO: calico-typha-vertical-autoscaler-5c9655cddd-w2d9c from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container autoscaler ready: true, restart count 0
Oct 19 17:05:40.159: INFO: coredns-9866fb499-7zgkw from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container coredns ready: true, restart count 0
Oct 19 17:05:40.159: INFO: coredns-9866fb499-kcm5k from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container coredns ready: true, restart count 0
Oct 19 17:05:40.159: INFO: csi-driver-node-ps5fs from kube-system started at 2021-10-19 15:45:29 +0000 UTC (3 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container csi-driver ready: true, restart count 0
Oct 19 17:05:40.159: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Oct 19 17:05:40.159: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Oct 19 17:05:40.159: INFO: kube-proxy-dpksr from kube-system started at 2021-10-19 15:47:27 +0000 UTC (2 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container conntrack-fix ready: true, restart count 0
Oct 19 17:05:40.159: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 19 17:05:40.159: INFO: metrics-server-7958497998-bdvjq from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container metrics-server ready: true, restart count 0
Oct 19 17:05:40.159: INFO: node-exporter-2xtzn from kube-system started at 2021-10-19 15:45:29 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container node-exporter ready: true, restart count 0
Oct 19 17:05:40.159: INFO: node-problem-detector-6n9vb from kube-system started at 2021-10-19 16:11:28 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 19 17:05:40.159: INFO: vpn-shoot-6cdd4985bc-w7qgp from kube-system started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container vpn-shoot ready: true, restart count 0
Oct 19 17:05:40.159: INFO: dashboard-metrics-scraper-7ccbfc448f-htlbk from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 19 17:05:40.159: INFO: kubernetes-dashboard-847f4ffdcd-6s4nf from kubernetes-dashboard started at 2021-10-19 15:45:49 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Oct 19 17:05:40.159: INFO: nodeport-test-268wp from services-2545 started at 2021-10-19 17:05:26 +0000 UTC (1 container statuses recorded)
Oct 19 17:05:40.159: INFO: 	Container nodeport-test ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bfb19d70-ce65-4e07-b41d-a3b8f015af16 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-bfb19d70-ce65-4e07-b41d-a3b8f015af16 off the node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bfb19d70-ce65-4e07-b41d-a3b8f015af16
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8632" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":261,"skipped":4728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:44.263: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-5b1f108e-2013-41c5-86f2-5435a2f6858e
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:44.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-950" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":262,"skipped":4753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:44.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 19 17:05:44.567: INFO: The status of Pod labelsupdatec4b09307-b190-4c94-a757-2e6d2cbe3ebd is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:05:46.572: INFO: The status of Pod labelsupdatec4b09307-b190-4c94-a757-2e6d2cbe3ebd is Running (Ready = true)
Oct 19 17:05:47.096: INFO: Successfully updated pod "labelsupdatec4b09307-b190-4c94-a757-2e6d2cbe3ebd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:51.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2670" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4802,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:51.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 19 17:05:51.289: INFO: Waiting up to 5m0s for pod "pod-42a75f01-7017-44f4-bab4-8754860c5dc5" in namespace "emptydir-9191" to be "Succeeded or Failed"
Oct 19 17:05:51.292: INFO: Pod "pod-42a75f01-7017-44f4-bab4-8754860c5dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.149218ms
Oct 19 17:05:53.296: INFO: Pod "pod-42a75f01-7017-44f4-bab4-8754860c5dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007129696s
STEP: Saw pod success
Oct 19 17:05:53.296: INFO: Pod "pod-42a75f01-7017-44f4-bab4-8754860c5dc5" satisfied condition "Succeeded or Failed"
Oct 19 17:05:53.299: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-42a75f01-7017-44f4-bab4-8754860c5dc5 container test-container: <nil>
STEP: delete the pod
Oct 19 17:05:53.318: INFO: Waiting for pod pod-42a75f01-7017-44f4-bab4-8754860c5dc5 to disappear
Oct 19 17:05:53.321: INFO: Pod pod-42a75f01-7017-44f4-bab4-8754860c5dc5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:53.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9191" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4806,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:53.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 19 17:05:53.464: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:05:55.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1170" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":265,"skipped":4822,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:05:55.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-6004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Oct 19 17:05:55.899: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 19 17:06:55.931: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:06:55.935: INFO: Starting informer...
STEP: Starting pods...
Oct 19 17:06:56.155: INFO: Pod1 is running on shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9. Tainting Node
Oct 19 17:06:58.393: INFO: Pod2 is running on shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Oct 19 17:07:03.911: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Oct 19 17:07:23.949: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:23.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6004" for this suite.
•{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":266,"skipped":4843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:23.971: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:07:24.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0" in namespace "downward-api-7759" to be "Succeeded or Failed"
Oct 19 17:07:24.121: INFO: Pod "downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.087775ms
Oct 19 17:07:26.124: INFO: Pod "downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00881062s
STEP: Saw pod success
Oct 19 17:07:26.124: INFO: Pod "downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0" satisfied condition "Succeeded or Failed"
Oct 19 17:07:26.127: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0 container client-container: <nil>
STEP: delete the pod
Oct 19 17:07:26.142: INFO: Waiting for pod downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0 to disappear
Oct 19 17:07:26.145: INFO: Pod downwardapi-volume-170dde95-9acf-427f-bd99-b13bb3edf7d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:26.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7759" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4894,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:26.153: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 17:07:26.889: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 17:07:29.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:07:29.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1669-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:33.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6778" for this suite.
STEP: Destroying namespace "webhook-6778-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":268,"skipped":4906,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:33.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 19 17:07:33.308: INFO: Waiting up to 5m0s for pod "pod-0cc40e3d-be39-41a1-8582-bca1048ff82c" in namespace "emptydir-6789" to be "Succeeded or Failed"
Oct 19 17:07:33.312: INFO: Pod "pod-0cc40e3d-be39-41a1-8582-bca1048ff82c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502795ms
Oct 19 17:07:35.316: INFO: Pod "pod-0cc40e3d-be39-41a1-8582-bca1048ff82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007477802s
STEP: Saw pod success
Oct 19 17:07:35.316: INFO: Pod "pod-0cc40e3d-be39-41a1-8582-bca1048ff82c" satisfied condition "Succeeded or Failed"
Oct 19 17:07:35.320: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-0cc40e3d-be39-41a1-8582-bca1048ff82c container test-container: <nil>
STEP: delete the pod
Oct 19 17:07:35.336: INFO: Waiting for pod pod-0cc40e3d-be39-41a1-8582-bca1048ff82c to disappear
Oct 19 17:07:35.339: INFO: Pod pod-0cc40e3d-be39-41a1-8582-bca1048ff82c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6789" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":4907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:35.350: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:07:35.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 create -f -'
Oct 19 17:07:35.671: INFO: stderr: ""
Oct 19 17:07:35.671: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Oct 19 17:07:35.671: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 create -f -'
Oct 19 17:07:35.806: INFO: stderr: ""
Oct 19 17:07:35.806: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 19 17:07:36.811: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:07:36.811: INFO: Found 0 / 1
Oct 19 17:07:37.811: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:07:37.811: INFO: Found 1 / 1
Oct 19 17:07:37.811: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 19 17:07:37.814: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:07:37.814: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 19 17:07:37.814: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 describe pod agnhost-primary-jd2cz'
Oct 19 17:07:37.874: INFO: stderr: ""
Oct 19 17:07:37.874: INFO: stdout: "Name:         agnhost-primary-jd2cz\nNamespace:    kubectl-1208\nPriority:     0\nNode:         shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9/10.250.1.123\nStart Time:   Tue, 19 Oct 2021 17:07:35 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 100.96.0.59/32\n              cni.projectcalico.org/podIPs: 100.96.0.59/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.96.0.59\nIPs:\n  IP:           100.96.0.59\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://e05ec63c55b2258e50d2d624bc721cb58476093f8a80750a829317e70373d07c\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 19 Oct 2021 17:07:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmhay-ddd.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-79cld (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-79cld:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1208/agnhost-primary-jd2cz to shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Oct 19 17:07:37.874: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 describe rc agnhost-primary'
Oct 19 17:07:37.932: INFO: stderr: ""
Oct 19 17:07:37.932: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1208\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-jd2cz\n"
Oct 19 17:07:37.932: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 describe service agnhost-primary'
Oct 19 17:07:37.985: INFO: stderr: ""
Oct 19 17:07:37.985: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1208\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.68.83.45\nIPs:               100.68.83.45\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.0.59:6379\nSession Affinity:  None\nEvents:            <none>\n"
Oct 19 17:07:37.991: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 describe node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9'
Oct 19 17:07:38.065: INFO: stderr: ""
Oct 19 17:07:38.065: INFO: stdout: "Name:               shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=g_c2_m4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-nl-1\n                    failure-domain.beta.kubernetes.io/zone=eu-nl-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=g_c2_m4\n                    node.kubernetes.io/role=node\n                    topology.cinder.csi.openstack.org/zone=eu-nl-1a\n                    topology.kubernetes.io/region=eu-nl-1\n                    topology.kubernetes.io/zone=eu-nl-1a\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/cri-name=containerd\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 7dcdb79015812d0f299e0cd2f6c071df7574a7954fb364249c228bb1bab45557\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"6cfd0557-93c6-4986-a02a-5a999fe510f4\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"node.kubernetes.io/role\":\"node\",\"worker.garden.sapcloud.io/group\":\"worker-1\",\"worker.gard...\n                    projectcalico.org/IPv4Address: 10.250.1.123/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.96.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 19 Oct 2021 15:45:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 19 Oct 2021 17:07:33 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  ReadonlyFilesystem            False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Tue, 19 Oct 2021 17:06:37 +0000   Tue, 19 Oct 2021 16:11:29 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  NetworkUnavailable            False   Tue, 19 Oct 2021 15:46:33 +0000   Tue, 19 Oct 2021 15:46:33 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Tue, 19 Oct 2021 17:07:35 +0000   Tue, 19 Oct 2021 15:45:21 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 19 Oct 2021 17:07:35 +0000   Tue, 19 Oct 2021 15:45:21 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 19 Oct 2021 17:07:35 +0000   Tue, 19 Oct 2021 15:45:21 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 19 Oct 2021 17:07:35 +0000   Tue, 19 Oct 2021 15:45:51 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.1.123\n  Hostname:    shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    65006904Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               4019288Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1920m\n  ephemeral-storage:    63238716162\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               2868312Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 14ca72d7ef7545faae6a2a73c16b4a24\n  System UUID:                f1750142-575d-5968-e9cd-11d266eb65c8\n  Boot ID:                    eab3fbf5-19f2-4757-a04a-46d123b526f6\n  Kernel Version:             5.4.0-7-cloud-amd64\n  OS Image:                   Garden Linux 318.9\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.4\n  Kubelet Version:            v1.22.2\n  Kube-Proxy Version:         v1.22.2\nPodCIDR:                      100.96.0.0/24\nPodCIDRs:                     100.96.0.0/24\nProviderID:                   openstack:///6cfd0557-93c6-4986-a02a-5a999fe510f4\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits   Age\n  ---------                   ----                                        ------------  ----------  ---------------  -------------   ---\n  kube-system                 apiserver-proxy-ftftt                       40m (2%)      400m (20%)  40Mi (1%)        500Mi (17%)     82m\n  kube-system                 blackbox-exporter-65c549b94c-c5pzd          11m (0%)      44m (2%)    23574998 (0%)    94299992 (3%)   76m\n  kube-system                 calico-kube-controllers-86c64d79ff-hmgq6    10m (0%)      50m (2%)    50Mi (1%)        100Mi (3%)      84m\n  kube-system                 calico-node-gkqll                           250m (13%)    800m (41%)  100Mi (3%)       700Mi (24%)     81m\n  kube-system                 calico-typha-deploy-58b94ff46-kljnn         200m (10%)    500m (26%)  100Mi (3%)       700Mi (24%)     84m\n  kube-system                 csi-driver-node-twl5g                       40m (2%)      110m (5%)   114Mi (4%)       180Mi (6%)      82m\n  kube-system                 kube-proxy-hgtmc                            34m (1%)      92m (4%)    47753748 (1%)    145014992 (4%)  80m\n  kube-system                 node-exporter-v9h4r                         50m (2%)      150m (7%)   50Mi (1%)        150Mi (5%)      82m\n  kube-system                 node-problem-detector-2s6bt                 11m (0%)      44m (2%)    23574998 (0%)    94299992 (3%)   56m\n  kubectl-1208                agnhost-primary-jd2cz                       0 (0%)        0 (0%)      0 (0%)           0 (0%)          3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests         Limits\n  --------             --------         ------\n  cpu                  646m (33%)       2190m (114%)\n  memory               570957248 (19%)  2776797056 (94%)\n  ephemeral-storage    0 (0%)           0 (0%)\n  hugepages-1Gi        0 (0%)           0 (0%)\n  hugepages-2Mi        0 (0%)           0 (0%)\n  example.com/fakecpu  0                0\nEvents:                <none>\n"
Oct 19 17:07:38.066: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1208 describe namespace kubectl-1208'
Oct 19 17:07:38.119: INFO: stderr: ""
Oct 19 17:07:38.119: INFO: stdout: "Name:         kubectl-1208\nLabels:       e2e-framework=kubectl\n              e2e-run=53c206ff-763e-4b70-8a0f-781602aa468c\n              kubernetes.io/metadata.name=kubectl-1208\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:38.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1208" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":270,"skipped":4934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:38.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:07:38.265: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 19 17:07:38.276: INFO: The status of Pod pod-exec-websocket-8f1a4558-ae4c-486c-8494-df6fed08e46a is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:07:40.280: INFO: The status of Pod pod-exec-websocket-8f1a4558-ae4c-486c-8494-df6fed08e46a is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:40.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8873" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":4978,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:40.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7128
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9362
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:53.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5798" for this suite.
STEP: Destroying namespace "nsdeletetest-7128" for this suite.
Oct 19 17:07:53.864: INFO: Namespace nsdeletetest-7128 was already deleted
STEP: Destroying namespace "nsdeletetest-9362" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":272,"skipped":4979,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:53.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:54.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8230" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":273,"skipped":4986,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:54.038: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 19 17:07:54.180: INFO: Waiting up to 5m0s for pod "downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757" in namespace "downward-api-2703" to be "Succeeded or Failed"
Oct 19 17:07:54.192: INFO: Pod "downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757": Phase="Pending", Reason="", readiness=false. Elapsed: 11.456498ms
Oct 19 17:07:56.197: INFO: Pod "downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016886307s
STEP: Saw pod success
Oct 19 17:07:56.197: INFO: Pod "downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757" satisfied condition "Succeeded or Failed"
Oct 19 17:07:56.202: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757 container dapi-container: <nil>
STEP: delete the pod
Oct 19 17:07:56.222: INFO: Waiting for pod downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757 to disappear
Oct 19 17:07:56.225: INFO: Pod downward-api-84d1ca4e-5f0f-4af9-b228-6a77eb08a757 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2703" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":4988,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:56.244: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 19 17:07:56.388: INFO: Waiting up to 5m0s for pod "downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee" in namespace "downward-api-3240" to be "Succeeded or Failed"
Oct 19 17:07:56.392: INFO: Pod "downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215863ms
Oct 19 17:07:58.408: INFO: Pod "downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019533154s
STEP: Saw pod success
Oct 19 17:07:58.408: INFO: Pod "downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee" satisfied condition "Succeeded or Failed"
Oct 19 17:07:58.411: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee container dapi-container: <nil>
STEP: delete the pod
Oct 19 17:07:58.424: INFO: Waiting for pod downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee to disappear
Oct 19 17:07:58.427: INFO: Pod downward-api-465f586e-64a4-4d90-b649-ccd6e4a428ee no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:07:58.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3240" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4999,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:07:58.436: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:07:58.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4" in namespace "downward-api-553" to be "Succeeded or Failed"
Oct 19 17:07:58.584: INFO: Pod "downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710299ms
Oct 19 17:08:00.588: INFO: Pod "downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007727254s
STEP: Saw pod success
Oct 19 17:08:00.588: INFO: Pod "downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4" satisfied condition "Succeeded or Failed"
Oct 19 17:08:00.591: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4 container client-container: <nil>
STEP: delete the pod
Oct 19 17:08:00.609: INFO: Waiting for pod downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4 to disappear
Oct 19 17:08:00.612: INFO: Pod downwardapi-volume-47a2e72e-2bf7-4a48-beec-8dad1d2742f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:08:00.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-553" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5006,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:08:00.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-86b86e39-5086-46af-8a23-185c6b192222 in namespace container-probe-267
Oct 19 17:08:02.774: INFO: Started pod busybox-86b86e39-5086-46af-8a23-185c6b192222 in namespace container-probe-267
STEP: checking the pod's current state and verifying that restartCount is present
Oct 19 17:08:02.777: INFO: Initial restart count of pod busybox-86b86e39-5086-46af-8a23-185c6b192222 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:12:03.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-267" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":5013,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:12:03.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 19 17:12:03.620: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 19 17:12:03.626: INFO: starting watch
STEP: patching
STEP: updating
Oct 19 17:12:03.642: INFO: waiting for watch events with expected annotations
Oct 19 17:12:03.642: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:12:03.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8272" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":278,"skipped":5015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:12:03.680: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:12:03.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd" in namespace "downward-api-9041" to be "Succeeded or Failed"
Oct 19 17:12:03.836: INFO: Pod "downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.250699ms
Oct 19 17:12:05.840: INFO: Pod "downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010091297s
STEP: Saw pod success
Oct 19 17:12:05.840: INFO: Pod "downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd" satisfied condition "Succeeded or Failed"
Oct 19 17:12:05.843: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd container client-container: <nil>
STEP: delete the pod
Oct 19 17:12:05.859: INFO: Waiting for pod downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd to disappear
Oct 19 17:12:05.862: INFO: Pod downwardapi-volume-b93df1e0-2e10-4ba4-95ba-1da31be4e8dd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:12:05.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9041" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":5037,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:12:05.871: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:12:06.015: INFO: Waiting up to 5m0s for pod "busybox-user-65534-af328c10-507f-4840-9377-3504a9e27d04" in namespace "security-context-test-8841" to be "Succeeded or Failed"
Oct 19 17:12:06.018: INFO: Pod "busybox-user-65534-af328c10-507f-4840-9377-3504a9e27d04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200963ms
Oct 19 17:12:08.023: INFO: Pod "busybox-user-65534-af328c10-507f-4840-9377-3504a9e27d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007694917s
Oct 19 17:12:08.023: INFO: Pod "busybox-user-65534-af328c10-507f-4840-9377-3504a9e27d04" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:12:08.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8841" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":5055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:12:08.032: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7590
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:12:08.169: INFO: Creating simple deployment test-new-deployment
Oct 19 17:12:08.179: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 17:12:10.214: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7590  c2fa4509-0693-4416-b13d-23f9433d85ad 36939 3 2021-10-19 17:12:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2021-10-19 17:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00293a868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-10-19 17:12:09 +0000 UTC,LastTransitionTime:2021-10-19 17:12:08 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-10-19 17:12:10 +0000 UTC,LastTransitionTime:2021-10-19 17:12:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 19 17:12:10.218: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-7590  69a065cf-85ae-4a01-96f9-9d0044ec18c7 36945 3 2021-10-19 17:12:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c2fa4509-0693-4416-b13d-23f9433d85ad 0xc00293ac67 0xc00293ac68}] []  [{kube-controller-manager Update apps/v1 2021-10-19 17:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2fa4509-0693-4416-b13d-23f9433d85ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:12:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00293acf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 19 17:12:10.221: INFO: Pod "test-new-deployment-847dcfb7fb-7dcqw" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-7dcqw test-new-deployment-847dcfb7fb- deployment-7590  b35882a9-4e0c-4393-aa65-c3ed855b322a 36942 0 2021-10-19 17:12:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 69a065cf-85ae-4a01-96f9-9d0044ec18c7 0xc00293b0a7 0xc00293b0a8}] []  [{kube-controller-manager Update v1 2021-10-19 17:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69a065cf-85ae-4a01-96f9-9d0044ec18c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hr6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hr6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-zh8gq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:12:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 19 17:12:10.221: INFO: Pod "test-new-deployment-847dcfb7fb-b8kvv" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-b8kvv test-new-deployment-847dcfb7fb- deployment-7590  b50ea315-8a1c-4e84-a81b-868ff1c40e66 36933 0 2021-10-19 17:12:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.68/32 cni.projectcalico.org/podIPs:100.96.0.68/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 69a065cf-85ae-4a01-96f9-9d0044ec18c7 0xc00293b220 0xc00293b221}] []  [{calico Update v1 2021-10-19 17:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2021-10-19 17:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69a065cf-85ae-4a01-96f9-9d0044ec18c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 17:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hxpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hxpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:12:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.68,StartTime:2021-10-19 17:12:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 17:12:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://3795e68d97659968db3103785f78b7f25dc231e9ece7fcf5652e70a0c6eb816f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:12:10.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7590" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":281,"skipped":5080,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:12:10.231: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 19 17:12:10.386: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:12:13.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8491" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":282,"skipped":5099,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:12:13.543: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc in namespace container-probe-6289
Oct 19 17:12:15.693: INFO: Started pod liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc in namespace container-probe-6289
STEP: checking the pod's current state and verifying that restartCount is present
Oct 19 17:12:15.695: INFO: Initial restart count of pod liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc is 0
Oct 19 17:12:35.744: INFO: Restart count of pod container-probe-6289/liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc is now 1 (20.048850105s elapsed)
Oct 19 17:12:55.820: INFO: Restart count of pod container-probe-6289/liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc is now 2 (40.124416489s elapsed)
Oct 19 17:13:15.878: INFO: Restart count of pod container-probe-6289/liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc is now 3 (1m0.182767051s elapsed)
Oct 19 17:13:35.947: INFO: Restart count of pod container-probe-6289/liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc is now 4 (1m20.251970869s elapsed)
Oct 19 17:14:42.200: INFO: Restart count of pod container-probe-6289/liveness-a7d1a5a3-46e5-4680-a00f-3d8a9c1125bc is now 5 (2m26.504255946s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:14:42.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6289" for this suite.
•{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5129,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:14:42.217: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-2903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2903" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":284,"skipped":5132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:00.392: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 19 17:16:00.535: INFO: Waiting up to 5m0s for pod "pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7" in namespace "emptydir-4854" to be "Succeeded or Failed"
Oct 19 17:16:00.538: INFO: Pod "pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905573ms
Oct 19 17:16:02.542: INFO: Pod "pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006658659s
STEP: Saw pod success
Oct 19 17:16:02.542: INFO: Pod "pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7" satisfied condition "Succeeded or Failed"
Oct 19 17:16:02.545: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7 container test-container: <nil>
STEP: delete the pod
Oct 19 17:16:02.601: INFO: Waiting for pod pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7 to disappear
Oct 19 17:16:02.604: INFO: Pod pod-aa8b90e3-9887-4f43-93f4-57ba53676cc7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:02.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4854" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:02.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2133" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":286,"skipped":5184,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:02.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Oct 19 17:16:02.926: INFO: Found Service test-service-gv2vj in namespace services-4115 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Oct 19 17:16:02.926: INFO: Service test-service-gv2vj created
STEP: Getting /status
Oct 19 17:16:02.929: INFO: Service test-service-gv2vj has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Oct 19 17:16:02.936: INFO: observed Service test-service-gv2vj in namespace services-4115 with annotations: map[] & LoadBalancer: {[]}
Oct 19 17:16:02.936: INFO: Found Service test-service-gv2vj in namespace services-4115 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Oct 19 17:16:02.936: INFO: Service test-service-gv2vj has service status patched
STEP: updating the ServiceStatus
Oct 19 17:16:02.943: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Oct 19 17:16:02.946: INFO: Observed Service test-service-gv2vj in namespace services-4115 with annotations: map[] & Conditions: {[]}
Oct 19 17:16:02.946: INFO: Observed event: &Service{ObjectMeta:{test-service-gv2vj  services-4115  2f1cbc5d-db78-402e-921f-6858ff4ec111 38178 0 2021-10-19 17:16:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-10-19 17:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2021-10-19 17:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.71.8.132,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.71.8.132],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Oct 19 17:16:02.946: INFO: Found Service test-service-gv2vj in namespace services-4115 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Oct 19 17:16:02.946: INFO: Service test-service-gv2vj has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Oct 19 17:16:02.953: INFO: observed Service test-service-gv2vj in namespace services-4115 with labels: map[test-service-static:true]
Oct 19 17:16:02.953: INFO: observed Service test-service-gv2vj in namespace services-4115 with labels: map[test-service-static:true]
Oct 19 17:16:02.953: INFO: observed Service test-service-gv2vj in namespace services-4115 with labels: map[test-service-static:true]
Oct 19 17:16:02.953: INFO: Found Service test-service-gv2vj in namespace services-4115 with labels: map[test-service:patched test-service-static:true]
Oct 19 17:16:02.953: INFO: Service test-service-gv2vj patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Oct 19 17:16:02.962: INFO: Observed event: ADDED
Oct 19 17:16:02.962: INFO: Observed event: MODIFIED
Oct 19 17:16:02.962: INFO: Observed event: MODIFIED
Oct 19 17:16:02.962: INFO: Observed event: MODIFIED
Oct 19 17:16:02.962: INFO: Found Service test-service-gv2vj in namespace services-4115 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Oct 19 17:16:02.962: INFO: Service test-service-gv2vj deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:02.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4115" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":287,"skipped":5198,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:02.969: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-1939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:03.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1939" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":288,"skipped":5211,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:03.146: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:16:03.278: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:04.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6259" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":289,"skipped":5216,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:04.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:06.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3932" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:06.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6596
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-5622a299-b4bd-4b8a-91a8-e1fd487a962a
STEP: Creating configMap with name cm-test-opt-upd-0c3e323c-c609-4825-b6ef-f1461288ec5d
STEP: Creating the pod
Oct 19 17:16:06.776: INFO: The status of Pod pod-projected-configmaps-5c148cf9-542d-4b7d-aa1f-9477a53376e8 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:16:08.780: INFO: The status of Pod pod-projected-configmaps-5c148cf9-542d-4b7d-aa1f-9477a53376e8 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-5622a299-b4bd-4b8a-91a8-e1fd487a962a
STEP: Updating configmap cm-test-opt-upd-0c3e323c-c609-4825-b6ef-f1461288ec5d
STEP: Creating configMap with name cm-test-opt-create-c88d0487-cb8c-4a4e-a0a0-fe46d09f4c9b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:10.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6596" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5269,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:10.933: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:11.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2661" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":292,"skipped":5277,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:11.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 19 17:16:11.274: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 19 17:16:11.280: INFO: starting watch
STEP: patching
STEP: updating
Oct 19 17:16:11.293: INFO: waiting for watch events with expected annotations
Oct 19 17:16:11.293: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:11.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1871" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":293,"skipped":5278,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:11.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:32.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9772" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5291,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:32.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Oct 19 17:16:32.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7241  f9b09db1-2ac8-42f7-b30f-99391d6d5d54 38545 0 2021-10-19 17:16:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-19 17:16:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 17:16:32.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7241  f9b09db1-2ac8-42f7-b30f-99391d6d5d54 38546 0 2021-10-19 17:16:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-19 17:16:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Oct 19 17:16:32.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7241  f9b09db1-2ac8-42f7-b30f-99391d6d5d54 38547 0 2021-10-19 17:16:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-19 17:16:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 19 17:16:32.851: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7241  f9b09db1-2ac8-42f7-b30f-99391d6d5d54 38548 0 2021-10-19 17:16:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-19 17:16:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7241" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":295,"skipped":5300,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:32.859: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-814f874d-5252-476b-ac15-77dd359d8f48
STEP: Creating a pod to test consume configMaps
Oct 19 17:16:33.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5" in namespace "configmap-3521" to be "Succeeded or Failed"
Oct 19 17:16:33.008: INFO: Pod "pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.334125ms
Oct 19 17:16:35.013: INFO: Pod "pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008143248s
STEP: Saw pod success
Oct 19 17:16:35.013: INFO: Pod "pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5" satisfied condition "Succeeded or Failed"
Oct 19 17:16:35.016: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 17:16:35.029: INFO: Waiting for pod pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5 to disappear
Oct 19 17:16:35.031: INFO: Pod pod-configmaps-d28ef9d5-7487-4397-a687-7499581a39f5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:35.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3521" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:35.040: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 19 17:16:35.183: INFO: Waiting up to 5m0s for pod "downward-api-be273bf6-9c6f-4755-9136-2df38482809c" in namespace "downward-api-6171" to be "Succeeded or Failed"
Oct 19 17:16:35.186: INFO: Pod "downward-api-be273bf6-9c6f-4755-9136-2df38482809c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.892464ms
Oct 19 17:16:37.190: INFO: Pod "downward-api-be273bf6-9c6f-4755-9136-2df38482809c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006787528s
STEP: Saw pod success
Oct 19 17:16:37.190: INFO: Pod "downward-api-be273bf6-9c6f-4755-9136-2df38482809c" satisfied condition "Succeeded or Failed"
Oct 19 17:16:37.193: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downward-api-be273bf6-9c6f-4755-9136-2df38482809c container dapi-container: <nil>
STEP: delete the pod
Oct 19 17:16:37.204: INFO: Waiting for pod downward-api-be273bf6-9c6f-4755-9136-2df38482809c to disappear
Oct 19 17:16:37.207: INFO: Pod downward-api-be273bf6-9c6f-4755-9136-2df38482809c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:37.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6171" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":5340,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:37.216: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:16:37.351: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Oct 19 17:16:38.376: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:16:38.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-557" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":298,"skipped":5361,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:16:38.389: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1088
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1088
STEP: creating service affinity-nodeport-transition in namespace services-1088
STEP: creating replication controller affinity-nodeport-transition in namespace services-1088
I1019 17:16:38.543533    4339 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1088, replica count: 3
I1019 17:16:41.594988    4339 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 17:16:41.608: INFO: Creating new exec pod
Oct 19 17:16:44.629: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Oct 19 17:16:45.058: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Oct 19 17:16:45.058: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 17:16:45.058: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.65.77.58 80'
Oct 19 17:16:45.321: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.65.77.58 80\nConnection to 100.65.77.58 80 port [tcp/http] succeeded!\n"
Oct 19 17:16:45.321: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 17:16:45.321: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.123 31845'
Oct 19 17:16:45.510: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.123 31845\nConnection to 10.250.1.123 31845 port [tcp/*] succeeded!\n"
Oct 19 17:16:45.510: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 17:16:45.510: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.120 31845'
Oct 19 17:16:45.666: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.120 31845\nConnection to 10.250.3.120 31845 port [tcp/*] succeeded!\n"
Oct 19 17:16:45.666: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 19 17:16:45.674: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.123:31845/ ; done'
Oct 19 17:16:45.893: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n"
Oct 19 17:16:45.893: INFO: stdout: "\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6"
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:16:45.893: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:15.894: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.123:31845/ ; done'
Oct 19 17:17:16.227: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n"
Oct 19 17:17:16.227: INFO: stdout: "\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-s6smm"
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.227: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.237: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.123:31845/ ; done'
Oct 19 17:17:16.527: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n"
Oct 19 17:17:16.527: INFO: stdout: "\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-5jwb7\naffinity-nodeport-transition-2pbg6\naffinity-nodeport-transition-2pbg6"
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-5jwb7
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:16.527: INFO: Received response from host: affinity-nodeport-transition-2pbg6
Oct 19 17:17:46.528: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1088 exec execpod-affinityw9px5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.1.123:31845/ ; done'
Oct 19 17:17:46.762: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.1.123:31845/\n"
Oct 19 17:17:46.762: INFO: stdout: "\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm\naffinity-nodeport-transition-s6smm"
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Received response from host: affinity-nodeport-transition-s6smm
Oct 19 17:17:46.762: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1088, will wait for the garbage collector to delete the pods
Oct 19 17:17:46.826: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.948859ms
Oct 19 17:17:46.926: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.568992ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:17:48.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1088" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":299,"skipped":5366,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:17:48.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:17:48.787: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-869c6a21-db8f-4bca-89da-43042199867a" in namespace "security-context-test-8540" to be "Succeeded or Failed"
Oct 19 17:17:48.790: INFO: Pod "alpine-nnp-false-869c6a21-db8f-4bca-89da-43042199867a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.15801ms
Oct 19 17:17:50.819: INFO: Pod "alpine-nnp-false-869c6a21-db8f-4bca-89da-43042199867a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032001276s
Oct 19 17:17:52.823: INFO: Pod "alpine-nnp-false-869c6a21-db8f-4bca-89da-43042199867a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035857253s
Oct 19 17:17:52.823: INFO: Pod "alpine-nnp-false-869c6a21-db8f-4bca-89da-43042199867a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:17:52.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8540" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5377,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:17:52.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3006
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-f913fb11-a3f6-4ea2-a9ec-32a7be1a5599
STEP: Creating a pod to test consume configMaps
Oct 19 17:17:53.026: INFO: Waiting up to 5m0s for pod "pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7" in namespace "configmap-3006" to be "Succeeded or Failed"
Oct 19 17:17:53.030: INFO: Pod "pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.514919ms
Oct 19 17:17:55.035: INFO: Pod "pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009569783s
STEP: Saw pod success
Oct 19 17:17:55.035: INFO: Pod "pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7" satisfied condition "Succeeded or Failed"
Oct 19 17:17:55.039: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7 container configmap-volume-test: <nil>
STEP: delete the pod
Oct 19 17:17:55.094: INFO: Waiting for pod pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7 to disappear
Oct 19 17:17:55.097: INFO: Pod pod-configmaps-126d5969-3489-417d-8dca-653acb9835e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:17:55.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3006" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:17:55.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 19 17:17:55.257: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:17:57.262: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 19 17:17:57.281: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:17:59.285: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Oct 19 17:17:59.294: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 19 17:17:59.297: INFO: Pod pod-with-prestop-http-hook still exists
Oct 19 17:18:01.297: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 19 17:18:01.300: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3825" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:01.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6100
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6100
I1019 17:18:01.470094    4339 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6100, replica count: 2
I1019 17:18:04.521005    4339 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 17:18:04.521: INFO: Creating new exec pod
Oct 19 17:18:07.541: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6100 exec execpod42qpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Oct 19 17:18:07.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 19 17:18:07.758: INFO: stdout: "externalname-service-l6ns9"
Oct 19 17:18:07.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6100 exec execpod42qpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.55.178 80'
Oct 19 17:18:07.981: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.55.178 80\nConnection to 100.71.55.178 80 port [tcp/http] succeeded!\n"
Oct 19 17:18:07.981: INFO: stdout: ""
Oct 19 17:18:08.981: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6100 exec execpod42qpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.55.178 80'
Oct 19 17:18:09.225: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.55.178 80\nConnection to 100.71.55.178 80 port [tcp/http] succeeded!\n"
Oct 19 17:18:09.225: INFO: stdout: "externalname-service-9bt9g"
Oct 19 17:18:09.225: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6100 exec execpod42qpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.1.123 31905'
Oct 19 17:18:09.396: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.1.123 31905\nConnection to 10.250.1.123 31905 port [tcp/*] succeeded!\n"
Oct 19 17:18:09.396: INFO: stdout: "externalname-service-9bt9g"
Oct 19 17:18:09.396: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6100 exec execpod42qpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.3.120 31905'
Oct 19 17:18:09.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.3.120 31905\nConnection to 10.250.3.120 31905 port [tcp/*] succeeded!\n"
Oct 19 17:18:09.608: INFO: stdout: "externalname-service-l6ns9"
Oct 19 17:18:09.608: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:09.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6100" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":303,"skipped":5481,"failed":0}
SSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:09.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Oct 19 17:18:11.791: INFO: &Pod{ObjectMeta:{send-events-12a37a2f-f2b0-4e6c-9bcb-34f20a4ba3f3  events-9430  b4eaab6a-b539-46ac-bfbb-25c4db76f55a 39320 0 2021-10-19 17:18:09 +0000 UTC <nil> <nil> map[name:foo time:768951455] map[cni.projectcalico.org/podIP:100.96.0.90/32 cni.projectcalico.org/podIPs:100.96.0.90/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-10-19 17:18:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 17:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 17:18:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdssz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdssz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:18:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:18:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.90,StartTime:2021-10-19 17:18:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 17:18:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://e6b91a6b23cd9e3bc54dacfd43a38a1a3a6124a8939e7be53449708bd4b4d01d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Oct 19 17:18:13.796: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Oct 19 17:18:15.800: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:15.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9430" for this suite.
•{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":304,"skipped":5484,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:15.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-52
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:18:15.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09" in namespace "downward-api-52" to be "Succeeded or Failed"
Oct 19 17:18:15.962: INFO: Pod "downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.760317ms
Oct 19 17:18:17.966: INFO: Pod "downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008450207s
STEP: Saw pod success
Oct 19 17:18:17.966: INFO: Pod "downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09" satisfied condition "Succeeded or Failed"
Oct 19 17:18:17.969: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09 container client-container: <nil>
STEP: delete the pod
Oct 19 17:18:17.981: INFO: Waiting for pod downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09 to disappear
Oct 19 17:18:17.984: INFO: Pod downwardapi-volume-b274ab88-78f9-4a09-b333-16ffd179ac09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:17.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-52" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":305,"skipped":5491,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:17.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Oct 19 17:18:28.200: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1019 17:18:28.199989    4339 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Oct 19 17:18:28.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fh6c" in namespace "gc-9902"
Oct 19 17:18:28.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-74q4m" in namespace "gc-9902"
Oct 19 17:18:28.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jg5j" in namespace "gc-9902"
Oct 19 17:18:28.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-fksfn" in namespace "gc-9902"
Oct 19 17:18:28.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkcbs" in namespace "gc-9902"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:28.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9902" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":306,"skipped":5491,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:28.298: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Oct 19 17:18:28.703: INFO: Pod name wrapped-volume-race-fcc804c8-85ab-4556-a68f-b08db63f46dd: Found 1 pods out of 5
Oct 19 17:18:33.713: INFO: Pod name wrapped-volume-race-fcc804c8-85ab-4556-a68f-b08db63f46dd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fcc804c8-85ab-4556-a68f-b08db63f46dd in namespace emptydir-wrapper-7593, will wait for the garbage collector to delete the pods
Oct 19 17:18:33.789: INFO: Deleting ReplicationController wrapped-volume-race-fcc804c8-85ab-4556-a68f-b08db63f46dd took: 4.416104ms
Oct 19 17:18:33.890: INFO: Terminating ReplicationController wrapped-volume-race-fcc804c8-85ab-4556-a68f-b08db63f46dd pods took: 100.973757ms
STEP: Creating RC which spawns configmap-volume pods
Oct 19 17:18:35.012: INFO: Pod name wrapped-volume-race-9da21a43-6d5c-47ed-8fef-d51f639bf4f5: Found 0 pods out of 5
Oct 19 17:18:40.023: INFO: Pod name wrapped-volume-race-9da21a43-6d5c-47ed-8fef-d51f639bf4f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9da21a43-6d5c-47ed-8fef-d51f639bf4f5 in namespace emptydir-wrapper-7593, will wait for the garbage collector to delete the pods
Oct 19 17:18:40.099: INFO: Deleting ReplicationController wrapped-volume-race-9da21a43-6d5c-47ed-8fef-d51f639bf4f5 took: 4.400867ms
Oct 19 17:18:40.200: INFO: Terminating ReplicationController wrapped-volume-race-9da21a43-6d5c-47ed-8fef-d51f639bf4f5 pods took: 101.11093ms
STEP: Creating RC which spawns configmap-volume pods
Oct 19 17:18:41.615: INFO: Pod name wrapped-volume-race-567b4189-de15-4f93-bc75-74834fe0ed07: Found 0 pods out of 5
Oct 19 17:18:46.626: INFO: Pod name wrapped-volume-race-567b4189-de15-4f93-bc75-74834fe0ed07: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-567b4189-de15-4f93-bc75-74834fe0ed07 in namespace emptydir-wrapper-7593, will wait for the garbage collector to delete the pods
Oct 19 17:18:46.702: INFO: Deleting ReplicationController wrapped-volume-race-567b4189-de15-4f93-bc75-74834fe0ed07 took: 5.582416ms
Oct 19 17:18:46.803: INFO: Terminating ReplicationController wrapped-volume-race-567b4189-de15-4f93-bc75-74834fe0ed07 pods took: 100.965532ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:48.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7593" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":307,"skipped":5502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:48.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 19 17:18:50.250: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:50.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-491" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:50.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-621
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:18:50.403: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:50.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-621" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":309,"skipped":5563,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:50.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 19 17:18:51.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28" in namespace "projected-7135" to be "Succeeded or Failed"
Oct 19 17:18:51.123: INFO: Pod "downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.071125ms
Oct 19 17:18:53.128: INFO: Pod "downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007799069s
STEP: Saw pod success
Oct 19 17:18:53.128: INFO: Pod "downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28" satisfied condition "Succeeded or Failed"
Oct 19 17:18:53.131: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28 container client-container: <nil>
STEP: delete the pod
Oct 19 17:18:53.144: INFO: Waiting for pod downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28 to disappear
Oct 19 17:18:53.147: INFO: Pod downwardapi-volume-d9d27fa5-45d6-4ccf-aefc-16cee8f06e28 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:18:53.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7135" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5570,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:18:53.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-34b23671-7bf5-471d-b6b1-59aa09bfac68 in namespace container-probe-5352
Oct 19 17:18:55.308: INFO: Started pod busybox-34b23671-7bf5-471d-b6b1-59aa09bfac68 in namespace container-probe-5352
STEP: checking the pod's current state and verifying that restartCount is present
Oct 19 17:18:55.311: INFO: Initial restart count of pod busybox-34b23671-7bf5-471d-b6b1-59aa09bfac68 is 0
Oct 19 17:19:45.470: INFO: Restart count of pod container-probe-5352/busybox-34b23671-7bf5-471d-b6b1-59aa09bfac68 is now 1 (50.158731369s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:19:45.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5352" for this suite.
•{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5572,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:19:45.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 19 17:19:45.654: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 19 17:19:45.661: INFO: starting watch
STEP: patching
STEP: updating
Oct 19 17:19:45.673: INFO: waiting for watch events with expected annotations
Oct 19 17:19:45.673: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:19:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9351" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":312,"skipped":5582,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:19:45.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Oct 19 17:19:45.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5929 create -f -'
Oct 19 17:19:46.030: INFO: stderr: ""
Oct 19 17:19:46.030: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 19 17:19:47.034: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:19:47.034: INFO: Found 0 / 1
Oct 19 17:19:48.035: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:19:48.035: INFO: Found 1 / 1
Oct 19 17:19:48.035: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct 19 17:19:48.038: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:19:48.038: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 19 17:19:48.038: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5929 patch pod agnhost-primary-gxbvh -p {"metadata":{"annotations":{"x":"y"}}}'
Oct 19 17:19:48.090: INFO: stderr: ""
Oct 19 17:19:48.090: INFO: stdout: "pod/agnhost-primary-gxbvh patched\n"
STEP: checking annotations
Oct 19 17:19:48.094: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 19 17:19:48.094: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:19:48.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5929" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":313,"skipped":5595,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:19:48.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 19 17:19:48.358: INFO: Waiting up to 5m0s for pod "downward-api-632bb7a8-0899-4701-b646-a937207f8b06" in namespace "downward-api-3384" to be "Succeeded or Failed"
Oct 19 17:19:48.361: INFO: Pod "downward-api-632bb7a8-0899-4701-b646-a937207f8b06": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165029ms
Oct 19 17:19:50.367: INFO: Pod "downward-api-632bb7a8-0899-4701-b646-a937207f8b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008693859s
STEP: Saw pod success
Oct 19 17:19:50.367: INFO: Pod "downward-api-632bb7a8-0899-4701-b646-a937207f8b06" satisfied condition "Succeeded or Failed"
Oct 19 17:19:50.370: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod downward-api-632bb7a8-0899-4701-b646-a937207f8b06 container dapi-container: <nil>
STEP: delete the pod
Oct 19 17:19:50.384: INFO: Waiting for pod downward-api-632bb7a8-0899-4701-b646-a937207f8b06 to disappear
Oct 19 17:19:50.387: INFO: Pod downward-api-632bb7a8-0899-4701-b646-a937207f8b06 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:19:50.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3384" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":5595,"failed":0}
S
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:19:50.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:19:50.548: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:19:52.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:19:54.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:19:56.554: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:19:58.553: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:00.554: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:02.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:04.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:06.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:08.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:10.563: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = false)
Oct 19 17:20:12.552: INFO: The status of Pod test-webserver-53f06641-09e3-444d-9d21-017e5c30a532 is Running (Ready = true)
Oct 19 17:20:12.555: INFO: Container started at 2021-10-19 17:19:51 +0000 UTC, pod became ready at 2021-10-19 17:20:10 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:20:12.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1792" for this suite.
•{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:20:12.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-8e198a7f-f737-450f-b437-70bd991a1219
STEP: Creating a pod to test consume secrets
Oct 19 17:20:12.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98" in namespace "projected-9441" to be "Succeeded or Failed"
Oct 19 17:20:12.719: INFO: Pod "pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.530033ms
Oct 19 17:20:14.722: INFO: Pod "pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007180228s
STEP: Saw pod success
Oct 19 17:20:14.722: INFO: Pod "pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98" satisfied condition "Succeeded or Failed"
Oct 19 17:20:14.726: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 19 17:20:14.739: INFO: Waiting for pod pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98 to disappear
Oct 19 17:20:14.742: INFO: Pod pod-projected-secrets-c5627b2a-ab49-4cc8-b50f-a330ddf71e98 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:20:14.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9441" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":5630,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:20:14.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4722 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4722;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4722 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4722;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4722.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4722.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4722.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4722.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 118.112.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.112.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.112.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.112.118_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4722 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4722;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4722 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4722;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4722.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4722.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4722.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4722.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 118.112.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.112.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.112.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.112.118_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 17:20:16.962: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.051: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.058: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.062: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.066: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.071: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.111: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.115: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.120: INFO: Unable to read jessie_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.124: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.128: INFO: Unable to read jessie_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.133: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:17.168: INFO: Lookups using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4722 wheezy_tcp@dns-test-service.dns-4722 wheezy_udp@dns-test-service.dns-4722.svc wheezy_tcp@dns-test-service.dns-4722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4722 jessie_tcp@dns-test-service.dns-4722 jessie_udp@dns-test-service.dns-4722.svc jessie_tcp@dns-test-service.dns-4722.svc]

Oct 19 17:20:22.174: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.179: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.227: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.233: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.247: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.285: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.289: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.293: INFO: Unable to read jessie_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.298: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.302: INFO: Unable to read jessie_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.306: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:22.342: INFO: Lookups using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4722 wheezy_tcp@dns-test-service.dns-4722 wheezy_udp@dns-test-service.dns-4722.svc wheezy_tcp@dns-test-service.dns-4722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4722 jessie_tcp@dns-test-service.dns-4722 jessie_udp@dns-test-service.dns-4722.svc jessie_tcp@dns-test-service.dns-4722.svc]

Oct 19 17:20:27.173: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.178: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.223: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.228: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.233: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.275: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.320: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.324: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.328: INFO: Unable to read jessie_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.333: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.337: INFO: Unable to read jessie_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.343: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:27.382: INFO: Lookups using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4722 wheezy_tcp@dns-test-service.dns-4722 wheezy_udp@dns-test-service.dns-4722.svc wheezy_tcp@dns-test-service.dns-4722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4722 jessie_tcp@dns-test-service.dns-4722 jessie_udp@dns-test-service.dns-4722.svc jessie_tcp@dns-test-service.dns-4722.svc]

Oct 19 17:20:32.174: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.179: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.192: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.235: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.239: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.244: INFO: Unable to read jessie_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.248: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.253: INFO: Unable to read jessie_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.257: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:32.291: INFO: Lookups using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4722 wheezy_tcp@dns-test-service.dns-4722 wheezy_udp@dns-test-service.dns-4722.svc wheezy_tcp@dns-test-service.dns-4722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4722 jessie_tcp@dns-test-service.dns-4722 jessie_udp@dns-test-service.dns-4722.svc jessie_tcp@dns-test-service.dns-4722.svc]

Oct 19 17:20:37.175: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.179: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.226: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.240: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.248: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.296: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.300: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.305: INFO: Unable to read jessie_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.310: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.315: INFO: Unable to read jessie_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.320: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:37.361: INFO: Lookups using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4722 wheezy_tcp@dns-test-service.dns-4722 wheezy_udp@dns-test-service.dns-4722.svc wheezy_tcp@dns-test-service.dns-4722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4722 jessie_tcp@dns-test-service.dns-4722 jessie_udp@dns-test-service.dns-4722.svc jessie_tcp@dns-test-service.dns-4722.svc]

Oct 19 17:20:42.174: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.180: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.223: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.228: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.233: INFO: Unable to read wheezy_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.275: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.315: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.319: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.324: INFO: Unable to read jessie_udp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722 from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.332: INFO: Unable to read jessie_udp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.337: INFO: Unable to read jessie_tcp@dns-test-service.dns-4722.svc from pod dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d: the server could not find the requested resource (get pods dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d)
Oct 19 17:20:42.371: INFO: Lookups using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4722 wheezy_tcp@dns-test-service.dns-4722 wheezy_udp@dns-test-service.dns-4722.svc wheezy_tcp@dns-test-service.dns-4722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4722 jessie_tcp@dns-test-service.dns-4722 jessie_udp@dns-test-service.dns-4722.svc jessie_tcp@dns-test-service.dns-4722.svc]

Oct 19 17:20:47.356: INFO: DNS probes using dns-4722/dns-test-e6b860ea-1dae-4209-941c-860795ef3e6d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:20:47.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4722" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":317,"skipped":5636,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:20:47.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-3073ef71-cfad-48e4-ace1-1c7c3e47d000
STEP: Creating a pod to test consume secrets
Oct 19 17:20:47.538: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338" in namespace "projected-3836" to be "Succeeded or Failed"
Oct 19 17:20:47.542: INFO: Pod "pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303708ms
Oct 19 17:20:49.546: INFO: Pod "pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007831945s
STEP: Saw pod success
Oct 19 17:20:49.546: INFO: Pod "pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338" satisfied condition "Succeeded or Failed"
Oct 19 17:20:49.548: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 19 17:20:49.602: INFO: Waiting for pod pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338 to disappear
Oct 19 17:20:49.605: INFO: Pod pod-projected-secrets-3e8d6a15-49e1-40bf-81e9-c2ed8a7fd338 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:20:49.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3836" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":5636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:20:49.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 19 17:20:50.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 19 17:20:53.079: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:20:53.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:20:56.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6491" for this suite.
STEP: Destroying namespace "webhook-6491-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":319,"skipped":5686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:20:56.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Oct 19 17:20:59.064: INFO: Successfully updated pod "adopt-release--1-6wrfl"
STEP: Checking that the Job readopts the Pod
Oct 19 17:20:59.064: INFO: Waiting up to 15m0s for pod "adopt-release--1-6wrfl" in namespace "job-1485" to be "adopted"
Oct 19 17:20:59.068: INFO: Pod "adopt-release--1-6wrfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.580055ms
Oct 19 17:21:01.073: INFO: Pod "adopt-release--1-6wrfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009459331s
Oct 19 17:21:01.073: INFO: Pod "adopt-release--1-6wrfl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Oct 19 17:21:01.583: INFO: Successfully updated pod "adopt-release--1-6wrfl"
STEP: Checking that the Job releases the Pod
Oct 19 17:21:01.583: INFO: Waiting up to 15m0s for pod "adopt-release--1-6wrfl" in namespace "job-1485" to be "released"
Oct 19 17:21:01.585: INFO: Pod "adopt-release--1-6wrfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.773333ms
Oct 19 17:21:03.590: INFO: Pod "adopt-release--1-6wrfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007320865s
Oct 19 17:21:03.590: INFO: Pod "adopt-release--1-6wrfl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:03.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1485" for this suite.
•{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":320,"skipped":5724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:03.600: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:21:03.738: INFO: Got root ca configmap in namespace "svcaccounts-9760"
Oct 19 17:21:03.742: INFO: Deleted root ca configmap in namespace "svcaccounts-9760"
STEP: waiting for a new root ca configmap created
Oct 19 17:21:04.246: INFO: Recreated root ca configmap in namespace "svcaccounts-9760"
Oct 19 17:21:04.249: INFO: Updated root ca configmap in namespace "svcaccounts-9760"
STEP: waiting for the root ca configmap reconciled
Oct 19 17:21:04.754: INFO: Reconciled root ca configmap in namespace "svcaccounts-9760"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:04.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9760" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":321,"skipped":5750,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:04.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1217
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3282
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:11.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9586" for this suite.
STEP: Destroying namespace "nsdeletetest-1217" for this suite.
Oct 19 17:21:11.196: INFO: Namespace nsdeletetest-1217 was already deleted
STEP: Destroying namespace "nsdeletetest-3282" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":322,"skipped":5759,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:11.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Oct 19 17:21:11.361: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:15.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4954" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":5765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:15.967: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Oct 19 17:21:16.112: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:21:18.117: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Oct 19 17:21:19.135: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:20.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7210" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":324,"skipped":5814,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:20.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:21:20.296: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Oct 19 17:21:20.304: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 19 17:21:25.474: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 19 17:21:25.474: INFO: Creating deployment "test-rolling-update-deployment"
Oct 19 17:21:25.479: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Oct 19 17:21:25.572: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Oct 19 17:21:27.579: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Oct 19 17:21:27.582: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 17:21:27.591: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6254  52ad064e-fccf-4c16-b9ca-5a925d6a41d1 41259 1 2021-10-19 17:21:25 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-10-19 17:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00616ee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-10-19 17:21:25 +0000 UTC,LastTransitionTime:2021-10-19 17:21:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-10-19 17:21:26 +0000 UTC,LastTransitionTime:2021-10-19 17:21:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 19 17:21:27.594: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-6254  29f16186-b9ce-4bef-9aac-48a75fe696d5 41252 1 2021-10-19 17:21:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 52ad064e-fccf-4c16-b9ca-5a925d6a41d1 0xc00616f2f7 0xc00616f2f8}] []  [{kube-controller-manager Update apps/v1 2021-10-19 17:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ad064e-fccf-4c16-b9ca-5a925d6a41d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:21:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00616f3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 19 17:21:27.594: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Oct 19 17:21:27.594: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6254  3495bcab-9eaa-4c60-8648-3e69ddf0bfea 41258 2 2021-10-19 17:21:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 52ad064e-fccf-4c16-b9ca-5a925d6a41d1 0xc00616f1c7 0xc00616f1c8}] []  [{e2e.test Update apps/v1 2021-10-19 17:21:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52ad064e-fccf-4c16-b9ca-5a925d6a41d1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:21:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00616f288 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 19 17:21:27.597: INFO: Pod "test-rolling-update-deployment-585b757574-t7w7t" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-t7w7t test-rolling-update-deployment-585b757574- deployment-6254  c91d5788-2f91-490a-af60-18af3d3da3bc 41251 0 2021-10-19 17:21:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:100.96.0.125/32 cni.projectcalico.org/podIPs:100.96.0.125/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 29f16186-b9ce-4bef-9aac-48a75fe696d5 0xc00616f807 0xc00616f808}] []  [{kube-controller-manager Update v1 2021-10-19 17:21:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29f16186-b9ce-4bef-9aac-48a75fe696d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-10-19 17:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-10-19 17:21:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6mcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6mcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:21:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:21:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:21:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:21:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:100.96.0.125,StartTime:2021-10-19 17:21:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-19 17:21:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://fccf4cde7e374dce030e38de8aa0bf9ba78c2f47e18250b2692f2dedb6bd816a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:27.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6254" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":325,"skipped":5817,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:27.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-e0765b68-ddad-4ebe-8033-4c8789e89c5b
STEP: Creating a pod to test consume configMaps
Oct 19 17:21:27.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313" in namespace "configmap-450" to be "Succeeded or Failed"
Oct 19 17:21:27.754: INFO: Pod "pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313": Phase="Pending", Reason="", readiness=false. Elapsed: 3.553732ms
Oct 19 17:21:29.759: INFO: Pod "pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00870257s
STEP: Saw pod success
Oct 19 17:21:29.759: INFO: Pod "pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313" satisfied condition "Succeeded or Failed"
Oct 19 17:21:29.762: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313 container agnhost-container: <nil>
STEP: delete the pod
Oct 19 17:21:29.822: INFO: Waiting for pod pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313 to disappear
Oct 19 17:21:29.825: INFO: Pod pod-configmaps-1dcecc23-5a95-43f2-8c5a-1032ad3e8313 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:29.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-450" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":5822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:29.833: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 19 17:21:31.995: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:32.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3896" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5849,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:32.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4977
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Oct 19 17:21:32.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Oct 19 17:21:35.040: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:46.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4977" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":328,"skipped":5854,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:46.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:21:57.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3708" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":329,"skipped":5868,"failed":0}

------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:21:57.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 19 17:21:57.651: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:21:59.655: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 19 17:21:59.669: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 19 17:22:01.673: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 19 17:22:01.686: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 19 17:22:01.690: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 19 17:22:03.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 19 17:22:03.694: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 19 17:22:05.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 19 17:22:05.695: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:05.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7875" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":5868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:05.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4809
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:22:05.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 19 17:22:08.701: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4809 --namespace=crd-publish-openapi-4809 create -f -'
Oct 19 17:22:09.062: INFO: stderr: ""
Oct 19 17:22:09.062: INFO: stdout: "e2e-test-crd-publish-openapi-8929-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 19 17:22:09.062: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4809 --namespace=crd-publish-openapi-4809 delete e2e-test-crd-publish-openapi-8929-crds test-cr'
Oct 19 17:22:09.136: INFO: stderr: ""
Oct 19 17:22:09.136: INFO: stdout: "e2e-test-crd-publish-openapi-8929-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Oct 19 17:22:09.136: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4809 --namespace=crd-publish-openapi-4809 apply -f -'
Oct 19 17:22:09.261: INFO: stderr: ""
Oct 19 17:22:09.261: INFO: stdout: "e2e-test-crd-publish-openapi-8929-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 19 17:22:09.261: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4809 --namespace=crd-publish-openapi-4809 delete e2e-test-crd-publish-openapi-8929-crds test-cr'
Oct 19 17:22:09.311: INFO: stderr: ""
Oct 19 17:22:09.311: INFO: stdout: "e2e-test-crd-publish-openapi-8929-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Oct 19 17:22:09.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4809 explain e2e-test-crd-publish-openapi-8929-crds'
Oct 19 17:22:09.439: INFO: stderr: ""
Oct 19 17:22:09.439: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8929-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:12.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4809" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":331,"skipped":5891,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:12.812: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5499
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:22:12.960: INFO: Found 0 stateful pods, waiting for 1
Oct 19 17:22:22.965: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Oct 19 17:22:22.983: INFO: Found 1 stateful pods, waiting for 2
Oct 19 17:22:32.987: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 19 17:22:32.987: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Oct 19 17:22:33.005: INFO: Deleting all statefulset in ns statefulset-5499
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:33.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5499" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":332,"skipped":5891,"failed":0}

------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:33.023: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 19 17:22:33.656: INFO: starting watch
STEP: patching
STEP: updating
Oct 19 17:22:33.666: INFO: waiting for watch events with expected annotations
Oct 19 17:22:33.666: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9266" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":333,"skipped":5891,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:33.729: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 19 17:22:33.861: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:37.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9445" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":334,"skipped":5907,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:37.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:41.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9686" for this suite.
•{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":335,"skipped":5911,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:41.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 19 17:22:41.501: INFO: Waiting up to 5m0s for pod "pod-8fb11fee-21b7-4e51-8452-81ec301e921c" in namespace "emptydir-4604" to be "Succeeded or Failed"
Oct 19 17:22:41.505: INFO: Pod "pod-8fb11fee-21b7-4e51-8452-81ec301e921c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884127ms
Oct 19 17:22:43.509: INFO: Pod "pod-8fb11fee-21b7-4e51-8452-81ec301e921c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008271838s
STEP: Saw pod success
Oct 19 17:22:43.509: INFO: Pod "pod-8fb11fee-21b7-4e51-8452-81ec301e921c" satisfied condition "Succeeded or Failed"
Oct 19 17:22:43.512: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod pod-8fb11fee-21b7-4e51-8452-81ec301e921c container test-container: <nil>
STEP: delete the pod
Oct 19 17:22:43.525: INFO: Waiting for pod pod-8fb11fee-21b7-4e51-8452-81ec301e921c to disappear
Oct 19 17:22:43.528: INFO: Pod pod-8fb11fee-21b7-4e51-8452-81ec301e921c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:43.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4604" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":5922,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:43.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:43.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8195" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":337,"skipped":5926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:43.688: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:43.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3049" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":338,"skipped":5963,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:43.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:22:44.014: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2425 version'
Oct 19 17:22:44.069: INFO: stderr: ""
Oct 19 17:22:44.069: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\", GitCommit:\"8b5a19147530eaac9476b0ab82980b4088bbc1b2\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:38:50Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\", GitCommit:\"8b5a19147530eaac9476b0ab82980b4088bbc1b2\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:32:41Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:44.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2425" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":339,"skipped":5988,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:44.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:22:44.211: INFO: Creating deployment "test-recreate-deployment"
Oct 19 17:22:44.215: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Oct 19 17:22:44.220: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Oct 19 17:22:46.227: INFO: Waiting deployment "test-recreate-deployment" to complete
Oct 19 17:22:46.230: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Oct 19 17:22:46.237: INFO: Updating deployment test-recreate-deployment
Oct 19 17:22:46.237: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Oct 19 17:22:46.274: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2005  c7c4075e-1a13-495b-91fb-d51ba267f4b8 42056 2 2021-10-19 17:22:44 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006444408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-10-19 17:22:46 +0000 UTC,LastTransitionTime:2021-10-19 17:22:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-10-19 17:22:46 +0000 UTC,LastTransitionTime:2021-10-19 17:22:44 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Oct 19 17:22:46.278: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-2005  44da6ed0-5b4f-4bd1-a401-af1c7952b70e 42055 1 2021-10-19 17:22:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c7c4075e-1a13-495b-91fb-d51ba267f4b8 0xc006444990 0xc006444991}] []  [{kube-controller-manager Update apps/v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7c4075e-1a13-495b-91fb-d51ba267f4b8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006444a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 19 17:22:46.278: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Oct 19 17:22:46.278: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-2005  0a8f57a0-cfbf-4ff9-babb-e319285e7ace 42048 2 2021-10-19 17:22:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c7c4075e-1a13-495b-91fb-d51ba267f4b8 0xc006444837 0xc006444838}] []  [{kube-controller-manager Update apps/v1 2021-10-19 17:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7c4075e-1a13-495b-91fb-d51ba267f4b8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006444908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 19 17:22:46.281: INFO: Pod "test-recreate-deployment-85d47dcb4-lccgl" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-lccgl test-recreate-deployment-85d47dcb4- deployment-2005  4e8d3599-d7ab-4bc8-b72d-f1943a23a632 42057 0 2021-10-19 17:22:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 44da6ed0-5b4f-4bd1-a401-af1c7952b70e 0xc006444f20 0xc006444f21}] []  [{kube-controller-manager Update v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44da6ed0-5b4f-4bd1-a401-af1c7952b70e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-10-19 17:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rd6lq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmhay-ddd.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rd6lq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-19 17:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.1.123,PodIP:,StartTime:2021-10-19 17:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:22:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2005" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":340,"skipped":5996,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:22:46.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1301.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1301.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 17:22:48.512: INFO: DNS probes using dns-test-953344d7-30d9-4edc-bab8-5eb6c94cdd3b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1301.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1301.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 17:22:50.599: INFO: File wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:22:50.605: INFO: File jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:22:50.605: INFO: Lookups using dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 failed for: [wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local]

Oct 19 17:22:55.655: INFO: File wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:22:55.660: INFO: File jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:22:55.660: INFO: Lookups using dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 failed for: [wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local]

Oct 19 17:23:00.613: INFO: File wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:00.618: INFO: File jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:00.618: INFO: Lookups using dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 failed for: [wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local]

Oct 19 17:23:05.612: INFO: File wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:05.655: INFO: File jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:05.655: INFO: Lookups using dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 failed for: [wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local]

Oct 19 17:23:10.611: INFO: File wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:10.655: INFO: File jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:10.655: INFO: Lookups using dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 failed for: [wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local]

Oct 19 17:23:15.611: INFO: File wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:15.616: INFO: File jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local from pod  dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct 19 17:23:15.616: INFO: Lookups using dns-1301/dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 failed for: [wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local]

Oct 19 17:23:20.618: INFO: DNS probes using dns-test-3728493d-122a-46c0-83e0-c89ab8e36356 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1301.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1301.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1301.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1301.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 19 17:23:22.726: INFO: DNS probes using dns-test-e15d938b-de03-4bbe-abbc-9bdf2da49359 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:23:22.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1301" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":341,"skipped":6009,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:23:22.757: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Oct 19 17:23:22.898: INFO: Waiting up to 5m0s for pod "var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc" in namespace "var-expansion-9326" to be "Succeeded or Failed"
Oct 19 17:23:22.901: INFO: Pod "var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.89429ms
Oct 19 17:23:24.906: INFO: Pod "var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007301904s
STEP: Saw pod success
Oct 19 17:23:24.906: INFO: Pod "var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc" satisfied condition "Succeeded or Failed"
Oct 19 17:23:24.909: INFO: Trying to get logs from node shoot--it--tmhay-ddd-worker-1-z1-67558-hxdg9 pod var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc container dapi-container: <nil>
STEP: delete the pod
Oct 19 17:23:24.925: INFO: Waiting for pod var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc to disappear
Oct 19 17:23:24.930: INFO: Pod var-expansion-64cb4e42-b3a4-4c71-9064-9f03264647bc no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:23:24.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9326" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":6022,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:23:24.941: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 19 17:23:25.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5485 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Oct 19 17:23:25.147: INFO: stderr: ""
Oct 19 17:23:25.147: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Oct 19 17:23:25.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmhay-ddd.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5485 delete pods e2e-test-httpd-pod'
Oct 19 17:23:27.383: INFO: stderr: ""
Oct 19 17:23:27.383: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:23:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5485" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":343,"skipped":6031,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:23:27.392: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:25:01.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-199" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":344,"skipped":6038,"failed":0}

------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:25:01.552: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 19 17:25:01.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6399
I1019 17:25:01.710450    4339 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6399, replica count: 1
I1019 17:25:02.761429    4339 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 19 17:25:02.870: INFO: Created: latency-svc-b4t4v
Oct 19 17:25:02.874: INFO: Got endpoints: latency-svc-b4t4v [11.640031ms]
Oct 19 17:25:02.880: INFO: Created: latency-svc-xn2mb
Oct 19 17:25:02.884: INFO: Created: latency-svc-dtwh4
Oct 19 17:25:02.884: INFO: Got endpoints: latency-svc-xn2mb [10.448636ms]
Oct 19 17:25:02.886: INFO: Got endpoints: latency-svc-dtwh4 [11.871314ms]
Oct 19 17:25:02.887: INFO: Created: latency-svc-rrb7w
Oct 19 17:25:02.890: INFO: Got endpoints: latency-svc-rrb7w [16.36573ms]
Oct 19 17:25:02.890: INFO: Created: latency-svc-mxqjv
Oct 19 17:25:02.893: INFO: Got endpoints: latency-svc-mxqjv [19.462812ms]
Oct 19 17:25:02.894: INFO: Created: latency-svc-87r25
Oct 19 17:25:02.897: INFO: Got endpoints: latency-svc-87r25 [22.83169ms]
Oct 19 17:25:02.897: INFO: Created: latency-svc-qxl74
Oct 19 17:25:02.901: INFO: Got endpoints: latency-svc-qxl74 [26.555485ms]
Oct 19 17:25:02.901: INFO: Created: latency-svc-mqqlk
Oct 19 17:25:02.902: INFO: Got endpoints: latency-svc-mqqlk [28.280871ms]
Oct 19 17:25:02.905: INFO: Created: latency-svc-vnpmx
Oct 19 17:25:02.909: INFO: Got endpoints: latency-svc-vnpmx [35.067489ms]
Oct 19 17:25:02.909: INFO: Created: latency-svc-lr2t6
Oct 19 17:25:02.914: INFO: Got endpoints: latency-svc-lr2t6 [40.119422ms]
Oct 19 17:25:02.916: INFO: Created: latency-svc-sbqpb
Oct 19 17:25:02.917: INFO: Got endpoints: latency-svc-sbqpb [43.164306ms]
Oct 19 17:25:02.920: INFO: Created: latency-svc-6l8l5
Oct 19 17:25:02.921: INFO: Got endpoints: latency-svc-6l8l5 [46.834262ms]
Oct 19 17:25:02.924: INFO: Created: latency-svc-rncbm
Oct 19 17:25:02.926: INFO: Got endpoints: latency-svc-rncbm [51.877023ms]
Oct 19 17:25:02.928: INFO: Created: latency-svc-8r7cq
Oct 19 17:25:02.930: INFO: Got endpoints: latency-svc-8r7cq [55.731874ms]
Oct 19 17:25:02.934: INFO: Created: latency-svc-cr27l
Oct 19 17:25:02.937: INFO: Created: latency-svc-7nl2n
Oct 19 17:25:02.937: INFO: Got endpoints: latency-svc-cr27l [62.844706ms]
Oct 19 17:25:02.972: INFO: Got endpoints: latency-svc-7nl2n [98.425234ms]
Oct 19 17:25:02.976: INFO: Created: latency-svc-965pg
Oct 19 17:25:02.977: INFO: Got endpoints: latency-svc-965pg [93.062815ms]
Oct 19 17:25:02.979: INFO: Created: latency-svc-jg2ps
Oct 19 17:25:02.981: INFO: Got endpoints: latency-svc-jg2ps [94.699783ms]
Oct 19 17:25:02.983: INFO: Created: latency-svc-9l4vn
Oct 19 17:25:02.985: INFO: Got endpoints: latency-svc-9l4vn [95.077884ms]
Oct 19 17:25:02.986: INFO: Created: latency-svc-r6s4k
Oct 19 17:25:02.988: INFO: Got endpoints: latency-svc-r6s4k [94.545172ms]
Oct 19 17:25:02.990: INFO: Created: latency-svc-plwr9
Oct 19 17:25:02.991: INFO: Got endpoints: latency-svc-plwr9 [94.391747ms]
Oct 19 17:25:02.994: INFO: Created: latency-svc-6z6cf
Oct 19 17:25:02.995: INFO: Got endpoints: latency-svc-6z6cf [94.730808ms]
Oct 19 17:25:02.997: INFO: Created: latency-svc-jzcb9
Oct 19 17:25:02.998: INFO: Got endpoints: latency-svc-jzcb9 [96.113162ms]
Oct 19 17:25:03.002: INFO: Created: latency-svc-vqbtl
Oct 19 17:25:03.004: INFO: Got endpoints: latency-svc-vqbtl [94.788322ms]
Oct 19 17:25:03.006: INFO: Created: latency-svc-tvtvm
Oct 19 17:25:03.009: INFO: Got endpoints: latency-svc-tvtvm [94.981816ms]
Oct 19 17:25:03.011: INFO: Created: latency-svc-jv8lc
Oct 19 17:25:03.012: INFO: Got endpoints: latency-svc-jv8lc [94.47051ms]
Oct 19 17:25:03.014: INFO: Created: latency-svc-9k6js
Oct 19 17:25:03.017: INFO: Got endpoints: latency-svc-9k6js [95.911806ms]
Oct 19 17:25:03.018: INFO: Created: latency-svc-vwnbr
Oct 19 17:25:03.020: INFO: Got endpoints: latency-svc-vwnbr [93.969897ms]
Oct 19 17:25:03.022: INFO: Created: latency-svc-qswnk
Oct 19 17:25:03.025: INFO: Got endpoints: latency-svc-qswnk [95.321076ms]
Oct 19 17:25:03.026: INFO: Created: latency-svc-c2gls
Oct 19 17:25:03.027: INFO: Got endpoints: latency-svc-c2gls [90.004134ms]
Oct 19 17:25:03.079: INFO: Created: latency-svc-lxsd8
Oct 19 17:25:03.083: INFO: Created: latency-svc-rxfpg
Oct 19 17:25:03.083: INFO: Got endpoints: latency-svc-lxsd8 [110.671397ms]
Oct 19 17:25:03.085: INFO: Got endpoints: latency-svc-rxfpg [107.854455ms]
Oct 19 17:25:03.087: INFO: Created: latency-svc-jcc9t
Oct 19 17:25:03.089: INFO: Got endpoints: latency-svc-jcc9t [108.094906ms]
Oct 19 17:25:03.091: INFO: Created: latency-svc-lztkq
Oct 19 17:25:03.092: INFO: Got endpoints: latency-svc-lztkq [106.740036ms]
Oct 19 17:25:03.095: INFO: Created: latency-svc-ppw48
Oct 19 17:25:03.099: INFO: Created: latency-svc-ggf7k
Oct 19 17:25:03.102: INFO: Created: latency-svc-dgdtc
Oct 19 17:25:03.105: INFO: Created: latency-svc-p72qq
Oct 19 17:25:03.109: INFO: Created: latency-svc-bmqks
Oct 19 17:25:03.115: INFO: Created: latency-svc-gmpsd
Oct 19 17:25:03.119: INFO: Created: latency-svc-nk5jf
Oct 19 17:25:03.123: INFO: Created: latency-svc-97vd8
Oct 19 17:25:03.123: INFO: Got endpoints: latency-svc-ppw48 [134.940567ms]
Oct 19 17:25:03.126: INFO: Created: latency-svc-zgpfw
Oct 19 17:25:03.129: INFO: Created: latency-svc-fr62p
Oct 19 17:25:03.132: INFO: Created: latency-svc-57c49
Oct 19 17:25:03.135: INFO: Created: latency-svc-tgd4b
Oct 19 17:25:03.139: INFO: Created: latency-svc-fpw8x
Oct 19 17:25:03.142: INFO: Created: latency-svc-rfmlv
Oct 19 17:25:03.145: INFO: Created: latency-svc-hnf7d
Oct 19 17:25:03.148: INFO: Created: latency-svc-z65vm
Oct 19 17:25:03.172: INFO: Got endpoints: latency-svc-ggf7k [180.818885ms]
Oct 19 17:25:03.179: INFO: Created: latency-svc-sx6qk
Oct 19 17:25:03.223: INFO: Got endpoints: latency-svc-dgdtc [227.66322ms]
Oct 19 17:25:03.230: INFO: Created: latency-svc-bwjl6
Oct 19 17:25:03.273: INFO: Got endpoints: latency-svc-p72qq [274.809832ms]
Oct 19 17:25:03.280: INFO: Created: latency-svc-bwb4v
Oct 19 17:25:03.323: INFO: Got endpoints: latency-svc-bmqks [318.91037ms]
Oct 19 17:25:03.329: INFO: Created: latency-svc-fcng8
Oct 19 17:25:03.376: INFO: Got endpoints: latency-svc-gmpsd [366.369768ms]
Oct 19 17:25:03.382: INFO: Created: latency-svc-bdlhx
Oct 19 17:25:03.423: INFO: Got endpoints: latency-svc-nk5jf [410.907088ms]
Oct 19 17:25:03.429: INFO: Created: latency-svc-w4hvq
Oct 19 17:25:03.472: INFO: Got endpoints: latency-svc-97vd8 [455.119057ms]
Oct 19 17:25:03.482: INFO: Created: latency-svc-w4zzg
Oct 19 17:25:03.522: INFO: Got endpoints: latency-svc-zgpfw [502.247664ms]
Oct 19 17:25:03.529: INFO: Created: latency-svc-8j72f
Oct 19 17:25:03.573: INFO: Got endpoints: latency-svc-fr62p [545.853008ms]
Oct 19 17:25:03.582: INFO: Created: latency-svc-ftvml
Oct 19 17:25:03.623: INFO: Got endpoints: latency-svc-57c49 [597.708389ms]
Oct 19 17:25:03.630: INFO: Created: latency-svc-tp5wx
Oct 19 17:25:03.673: INFO: Got endpoints: latency-svc-tgd4b [589.638788ms]
Oct 19 17:25:03.679: INFO: Created: latency-svc-kxxdf
Oct 19 17:25:03.722: INFO: Got endpoints: latency-svc-fpw8x [637.253162ms]
Oct 19 17:25:03.729: INFO: Created: latency-svc-wf9hb
Oct 19 17:25:03.772: INFO: Got endpoints: latency-svc-rfmlv [683.375396ms]
Oct 19 17:25:03.779: INFO: Created: latency-svc-tn9nh
Oct 19 17:25:03.822: INFO: Got endpoints: latency-svc-hnf7d [730.151697ms]
Oct 19 17:25:03.830: INFO: Created: latency-svc-b6b4d
Oct 19 17:25:03.872: INFO: Got endpoints: latency-svc-z65vm [749.094956ms]
Oct 19 17:25:03.878: INFO: Created: latency-svc-7nplq
Oct 19 17:25:03.930: INFO: Got endpoints: latency-svc-sx6qk [758.386697ms]
Oct 19 17:25:03.937: INFO: Created: latency-svc-7mw9b
Oct 19 17:25:03.973: INFO: Got endpoints: latency-svc-bwjl6 [749.552037ms]
Oct 19 17:25:03.979: INFO: Created: latency-svc-8r7fz
Oct 19 17:25:04.023: INFO: Got endpoints: latency-svc-bwb4v [749.751234ms]
Oct 19 17:25:04.030: INFO: Created: latency-svc-dqw6v
Oct 19 17:25:04.073: INFO: Got endpoints: latency-svc-fcng8 [749.903029ms]
Oct 19 17:25:04.080: INFO: Created: latency-svc-hgphl
Oct 19 17:25:04.123: INFO: Got endpoints: latency-svc-bdlhx [747.504499ms]
Oct 19 17:25:04.129: INFO: Created: latency-svc-mq7m6
Oct 19 17:25:04.172: INFO: Got endpoints: latency-svc-w4hvq [749.547801ms]
Oct 19 17:25:04.186: INFO: Created: latency-svc-dvrzz
Oct 19 17:25:04.222: INFO: Got endpoints: latency-svc-w4zzg [750.078136ms]
Oct 19 17:25:04.230: INFO: Created: latency-svc-4g6k4
Oct 19 17:25:04.272: INFO: Got endpoints: latency-svc-8j72f [749.559754ms]
Oct 19 17:25:04.278: INFO: Created: latency-svc-pdhxs
Oct 19 17:25:04.323: INFO: Got endpoints: latency-svc-ftvml [750.186181ms]
Oct 19 17:25:04.330: INFO: Created: latency-svc-2bqgm
Oct 19 17:25:04.372: INFO: Got endpoints: latency-svc-tp5wx [749.44943ms]
Oct 19 17:25:04.379: INFO: Created: latency-svc-7lnx6
Oct 19 17:25:04.423: INFO: Got endpoints: latency-svc-kxxdf [750.43456ms]
Oct 19 17:25:04.430: INFO: Created: latency-svc-7rf2r
Oct 19 17:25:04.473: INFO: Got endpoints: latency-svc-wf9hb [750.442532ms]
Oct 19 17:25:04.479: INFO: Created: latency-svc-m89gm
Oct 19 17:25:04.522: INFO: Got endpoints: latency-svc-tn9nh [749.8106ms]
Oct 19 17:25:04.528: INFO: Created: latency-svc-25fb5
Oct 19 17:25:04.573: INFO: Got endpoints: latency-svc-b6b4d [750.672225ms]
Oct 19 17:25:04.580: INFO: Created: latency-svc-wbwhz
Oct 19 17:25:04.623: INFO: Got endpoints: latency-svc-7nplq [750.766888ms]
Oct 19 17:25:04.629: INFO: Created: latency-svc-9gggg
Oct 19 17:25:04.673: INFO: Got endpoints: latency-svc-7mw9b [742.968295ms]
Oct 19 17:25:04.680: INFO: Created: latency-svc-lkwkf
Oct 19 17:25:04.721: INFO: Got endpoints: latency-svc-8r7fz [748.897268ms]
Oct 19 17:25:04.728: INFO: Created: latency-svc-5jwjs
Oct 19 17:25:04.772: INFO: Got endpoints: latency-svc-dqw6v [748.679545ms]
Oct 19 17:25:04.784: INFO: Created: latency-svc-blw5d
Oct 19 17:25:04.874: INFO: Got endpoints: latency-svc-hgphl [801.358074ms]
Oct 19 17:25:04.874: INFO: Got endpoints: latency-svc-mq7m6 [751.145673ms]
Oct 19 17:25:04.881: INFO: Created: latency-svc-znjng
Oct 19 17:25:04.884: INFO: Created: latency-svc-zlmkt
Oct 19 17:25:04.974: INFO: Got endpoints: latency-svc-dvrzz [802.127815ms]
Oct 19 17:25:04.975: INFO: Got endpoints: latency-svc-4g6k4 [752.429722ms]
Oct 19 17:25:04.981: INFO: Created: latency-svc-47nf9
Oct 19 17:25:04.985: INFO: Created: latency-svc-lkpsx
Oct 19 17:25:05.022: INFO: Got endpoints: latency-svc-pdhxs [750.412473ms]
Oct 19 17:25:05.029: INFO: Created: latency-svc-vcwwz
Oct 19 17:25:05.072: INFO: Got endpoints: latency-svc-2bqgm [748.777402ms]
Oct 19 17:25:05.079: INFO: Created: latency-svc-mcx7l
Oct 19 17:25:05.121: INFO: Got endpoints: latency-svc-7lnx6 [748.911737ms]
Oct 19 17:25:05.134: INFO: Created: latency-svc-2mjcc
Oct 19 17:25:05.173: INFO: Got endpoints: latency-svc-7rf2r [749.231346ms]
Oct 19 17:25:05.179: INFO: Created: latency-svc-hw7l4
Oct 19 17:25:05.221: INFO: Got endpoints: latency-svc-m89gm [748.507675ms]
Oct 19 17:25:05.228: INFO: Created: latency-svc-r42gh
Oct 19 17:25:05.273: INFO: Got endpoints: latency-svc-25fb5 [751.071486ms]
Oct 19 17:25:05.279: INFO: Created: latency-svc-ct6d8
Oct 19 17:25:05.323: INFO: Got endpoints: latency-svc-wbwhz [750.303704ms]
Oct 19 17:25:05.329: INFO: Created: latency-svc-fnz6x
Oct 19 17:25:05.373: INFO: Got endpoints: latency-svc-9gggg [750.171022ms]
Oct 19 17:25:05.379: INFO: Created: latency-svc-wm2kq
Oct 19 17:25:05.423: INFO: Got endpoints: latency-svc-lkwkf [749.050607ms]
Oct 19 17:25:05.430: INFO: Created: latency-svc-qzszt
Oct 19 17:25:05.474: INFO: Got endpoints: latency-svc-5jwjs [752.7571ms]
Oct 19 17:25:05.483: INFO: Created: latency-svc-wdswn
Oct 19 17:25:05.523: INFO: Got endpoints: latency-svc-blw5d [751.214718ms]
Oct 19 17:25:05.530: INFO: Created: latency-svc-xkkpn
Oct 19 17:25:05.572: INFO: Got endpoints: latency-svc-znjng [698.112163ms]
Oct 19 17:25:05.578: INFO: Created: latency-svc-5rm5d
Oct 19 17:25:05.623: INFO: Got endpoints: latency-svc-zlmkt [748.467237ms]
Oct 19 17:25:05.629: INFO: Created: latency-svc-fgmjv
Oct 19 17:25:05.672: INFO: Got endpoints: latency-svc-47nf9 [697.756593ms]
Oct 19 17:25:05.678: INFO: Created: latency-svc-ns596
Oct 19 17:25:05.723: INFO: Got endpoints: latency-svc-lkpsx [748.725444ms]
Oct 19 17:25:05.729: INFO: Created: latency-svc-hf7zh
Oct 19 17:25:05.772: INFO: Got endpoints: latency-svc-vcwwz [749.296064ms]
Oct 19 17:25:05.779: INFO: Created: latency-svc-qnsjl
Oct 19 17:25:05.823: INFO: Got endpoints: latency-svc-mcx7l [751.216204ms]
Oct 19 17:25:05.829: INFO: Created: latency-svc-2b26q
Oct 19 17:25:05.873: INFO: Got endpoints: latency-svc-2mjcc [751.359292ms]
Oct 19 17:25:05.879: INFO: Created: latency-svc-f2sxb
Oct 19 17:25:05.922: INFO: Got endpoints: latency-svc-hw7l4 [749.351151ms]
Oct 19 17:25:05.929: INFO: Created: latency-svc-hg8gv
Oct 19 17:25:05.979: INFO: Got endpoints: latency-svc-r42gh [757.669228ms]
Oct 19 17:25:05.985: INFO: Created: latency-svc-kqs28
Oct 19 17:25:06.022: INFO: Got endpoints: latency-svc-ct6d8 [748.995168ms]
Oct 19 17:25:06.028: INFO: Created: latency-svc-wsrb7
Oct 19 17:25:06.073: INFO: Got endpoints: latency-svc-fnz6x [749.363484ms]
Oct 19 17:25:06.079: INFO: Created: latency-svc-xmw89
Oct 19 17:25:06.122: INFO: Got endpoints: latency-svc-wm2kq [749.022416ms]
Oct 19 17:25:06.128: INFO: Created: latency-svc-qwpwb
Oct 19 17:25:06.172: INFO: Got endpoints: latency-svc-qzszt [749.816327ms]
Oct 19 17:25:06.179: INFO: Created: latency-svc-6zc64
Oct 19 17:25:06.222: INFO: Got endpoints: latency-svc-wdswn [747.891077ms]
Oct 19 17:25:06.229: INFO: Created: latency-svc-kbpzr
Oct 19 17:25:06.272: INFO: Got endpoints: latency-svc-xkkpn [749.361577ms]
Oct 19 17:25:06.279: INFO: Created: latency-svc-9vxlm
Oct 19 17:25:06.323: INFO: Got endpoints: latency-svc-5rm5d [750.571291ms]
Oct 19 17:25:06.329: INFO: Created: latency-svc-2l4px
Oct 19 17:25:06.372: INFO: Got endpoints: latency-svc-fgmjv [749.690841ms]
Oct 19 17:25:06.379: INFO: Created: latency-svc-hmhgg
Oct 19 17:25:06.423: INFO: Got endpoints: latency-svc-ns596 [751.15321ms]
Oct 19 17:25:06.430: INFO: Created: latency-svc-lc4fg
Oct 19 17:25:06.472: INFO: Got endpoints: latency-svc-hf7zh [749.076337ms]
Oct 19 17:25:06.479: INFO: Created: latency-svc-xfzdb
Oct 19 17:25:06.523: INFO: Got endpoints: latency-svc-qnsjl [751.326954ms]
Oct 19 17:25:06.531: INFO: Created: latency-svc-wntd4
Oct 19 17:25:06.571: INFO: Got endpoints: latency-svc-2b26q [748.308316ms]
Oct 19 17:25:06.577: INFO: Created: latency-svc-qrxv6
Oct 19 17:25:06.623: INFO: Got endpoints: latency-svc-f2sxb [750.267419ms]
Oct 19 17:25:06.629: INFO: Created: latency-svc-lrdsq
Oct 19 17:25:06.672: INFO: Got endpoints: latency-svc-hg8gv [749.627295ms]
Oct 19 17:25:06.678: INFO: Created: latency-svc-vvhsf
Oct 19 17:25:06.722: INFO: Got endpoints: latency-svc-kqs28 [742.925259ms]
Oct 19 17:25:06.730: INFO: Created: latency-svc-qc895
Oct 19 17:25:06.772: INFO: Got endpoints: latency-svc-wsrb7 [749.823799ms]
Oct 19 17:25:06.778: INFO: Created: latency-svc-96hpf
Oct 19 17:25:06.822: INFO: Got endpoints: latency-svc-xmw89 [749.418823ms]
Oct 19 17:25:06.828: INFO: Created: latency-svc-bpslr
Oct 19 17:25:06.873: INFO: Got endpoints: latency-svc-qwpwb [750.489145ms]
Oct 19 17:25:06.879: INFO: Created: latency-svc-vl77s
Oct 19 17:25:06.922: INFO: Got endpoints: latency-svc-6zc64 [749.562849ms]
Oct 19 17:25:06.928: INFO: Created: latency-svc-fk4tq
Oct 19 17:25:06.976: INFO: Got endpoints: latency-svc-kbpzr [753.31356ms]
Oct 19 17:25:06.982: INFO: Created: latency-svc-rxn8s
Oct 19 17:25:07.022: INFO: Got endpoints: latency-svc-9vxlm [749.277355ms]
Oct 19 17:25:07.028: INFO: Created: latency-svc-qckfh
Oct 19 17:25:07.072: INFO: Got endpoints: latency-svc-2l4px [749.265318ms]
Oct 19 17:25:07.079: INFO: Created: latency-svc-6h5hs
Oct 19 17:25:07.122: INFO: Got endpoints: latency-svc-hmhgg [749.529758ms]
Oct 19 17:25:07.128: INFO: Created: latency-svc-pvldd
Oct 19 17:25:07.172: INFO: Got endpoints: latency-svc-lc4fg [748.79989ms]
Oct 19 17:25:07.179: INFO: Created: latency-svc-55mcf
Oct 19 17:25:07.222: INFO: Got endpoints: latency-svc-xfzdb [749.586258ms]
Oct 19 17:25:07.229: INFO: Created: latency-svc-xkpq9
Oct 19 17:25:07.274: INFO: Got endpoints: latency-svc-wntd4 [750.80553ms]
Oct 19 17:25:07.282: INFO: Created: latency-svc-wxcj6
Oct 19 17:25:07.323: INFO: Got endpoints: latency-svc-qrxv6 [751.786556ms]
Oct 19 17:25:07.330: INFO: Created: latency-svc-wnc8s
Oct 19 17:25:07.373: INFO: Got endpoints: latency-svc-lrdsq [749.887811ms]
Oct 19 17:25:07.380: INFO: Created: latency-svc-9flhd
Oct 19 17:25:07.422: INFO: Got endpoints: latency-svc-vvhsf [750.364156ms]
Oct 19 17:25:07.429: INFO: Created: latency-svc-qwlbm
Oct 19 17:25:07.473: INFO: Got endpoints: latency-svc-qc895 [750.687716ms]
Oct 19 17:25:07.480: INFO: Created: latency-svc-dvtx7
Oct 19 17:25:07.522: INFO: Got endpoints: latency-svc-96hpf [750.092769ms]
Oct 19 17:25:07.528: INFO: Created: latency-svc-jkmcg
Oct 19 17:25:07.573: INFO: Got endpoints: latency-svc-bpslr [750.487153ms]
Oct 19 17:25:07.581: INFO: Created: latency-svc-sxswz
Oct 19 17:25:07.623: INFO: Got endpoints: latency-svc-vl77s [750.068884ms]
Oct 19 17:25:07.629: INFO: Created: latency-svc-v2gkk
Oct 19 17:25:07.672: INFO: Got endpoints: latency-svc-fk4tq [750.028763ms]
Oct 19 17:25:07.678: INFO: Created: latency-svc-fd8w2
Oct 19 17:25:07.734: INFO: Got endpoints: latency-svc-rxn8s [758.164854ms]
Oct 19 17:25:07.750: INFO: Created: latency-svc-wzd9d
Oct 19 17:25:07.775: INFO: Got endpoints: latency-svc-qckfh [753.382144ms]
Oct 19 17:25:07.782: INFO: Created: latency-svc-cv265
Oct 19 17:25:07.830: INFO: Got endpoints: latency-svc-6h5hs [757.818164ms]
Oct 19 17:25:07.840: INFO: Created: latency-svc-lhcnc
Oct 19 17:25:07.873: INFO: Got endpoints: latency-svc-pvldd [751.286178ms]
Oct 19 17:25:07.879: INFO: Created: latency-svc-9xdww
Oct 19 17:25:07.924: INFO: Got endpoints: latency-svc-55mcf [751.97999ms]
Oct 19 17:25:07.931: INFO: Created: latency-svc-ll94c
Oct 19 17:25:07.995: INFO: Got endpoints: latency-svc-xkpq9 [773.314429ms]
Oct 19 17:25:08.012: INFO: Created: latency-svc-4gn62
Oct 19 17:25:08.025: INFO: Got endpoints: latency-svc-wxcj6 [751.282499ms]
Oct 19 17:25:08.033: INFO: Created: latency-svc-s2bg9
Oct 19 17:25:08.072: INFO: Got endpoints: latency-svc-wnc8s [749.099263ms]
Oct 19 17:25:08.079: INFO: Created: latency-svc-8pk8t
Oct 19 17:25:08.122: INFO: Got endpoints: latency-svc-9flhd [749.211757ms]
Oct 19 17:25:08.129: INFO: Created: latency-svc-cq262
Oct 19 17:25:08.172: INFO: Got endpoints: latency-svc-qwlbm [749.875963ms]
Oct 19 17:25:08.179: INFO: Created: latency-svc-cgw5m
Oct 19 17:25:08.222: INFO: Got endpoints: latency-svc-dvtx7 [749.404633ms]
Oct 19 17:25:08.230: INFO: Created: latency-svc-xhgbs
Oct 19 17:25:08.272: INFO: Got endpoints: latency-svc-jkmcg [749.900815ms]
Oct 19 17:25:08.279: INFO: Created: latency-svc-wbblg
Oct 19 17:25:08.322: INFO: Got endpoints: latency-svc-sxswz [749.747344ms]
Oct 19 17:25:08.329: INFO: Created: latency-svc-k5xm6
Oct 19 17:25:08.372: INFO: Got endpoints: latency-svc-v2gkk [749.187664ms]
Oct 19 17:25:08.378: INFO: Created: latency-svc-x2h7m
Oct 19 17:25:08.423: INFO: Got endpoints: latency-svc-fd8w2 [750.435059ms]
Oct 19 17:25:08.429: INFO: Created: latency-svc-mbbcc
Oct 19 17:25:08.472: INFO: Got endpoints: latency-svc-wzd9d [737.861379ms]
Oct 19 17:25:08.478: INFO: Created: latency-svc-ccnmk
Oct 19 17:25:08.523: INFO: Got endpoints: latency-svc-cv265 [747.352824ms]
Oct 19 17:25:08.529: INFO: Created: latency-svc-j6bbl
Oct 19 17:25:08.572: INFO: Got endpoints: latency-svc-lhcnc [741.933054ms]
Oct 19 17:25:08.579: INFO: Created: latency-svc-rbswv
Oct 19 17:25:08.622: INFO: Got endpoints: latency-svc-9xdww [748.57553ms]
Oct 19 17:25:08.635: INFO: Created: latency-svc-bwcwm
Oct 19 17:25:08.672: INFO: Got endpoints: latency-svc-ll94c [747.734155ms]
Oct 19 17:25:08.678: INFO: Created: latency-svc-tst2v
Oct 19 17:25:08.723: INFO: Got endpoints: latency-svc-4gn62 [727.167349ms]
Oct 19 17:25:08.729: INFO: Created: latency-svc-2tvjg
Oct 19 17:25:08.773: INFO: Got endpoints: latency-svc-s2bg9 [748.189631ms]
Oct 19 17:25:08.782: INFO: Created: latency-svc-lwrng
Oct 19 17:25:08.823: INFO: Got endpoints: latency-svc-8pk8t [750.897562ms]
Oct 19 17:25:08.831: INFO: Created: latency-svc-pv9x2
Oct 19 17:25:08.873: INFO: Got endpoints: latency-svc-cq262 [750.422846ms]
Oct 19 17:25:08.879: INFO: Created: latency-svc-jlvt6
Oct 19 17:25:08.922: INFO: Got endpoints: latency-svc-cgw5m [750.105349ms]
Oct 19 17:25:08.929: INFO: Created: latency-svc-qgctg
Oct 19 17:25:08.973: INFO: Got endpoints: latency-svc-xhgbs [750.416511ms]
Oct 19 17:25:08.979: INFO: Created: latency-svc-gvhxl
Oct 19 17:25:09.022: INFO: Got endpoints: latency-svc-wbblg [749.814586ms]
Oct 19 17:25:09.029: INFO: Created: latency-svc-54hgd
Oct 19 17:25:09.073: INFO: Got endpoints: latency-svc-k5xm6 [750.390965ms]
Oct 19 17:25:09.080: INFO: Created: latency-svc-tgpqm
Oct 19 17:25:09.599: INFO: Got endpoints: latency-svc-x2h7m [1.227544092s]
Oct 19 17:25:09.600: INFO: Got endpoints: latency-svc-mbbcc [1.177200594s]
Oct 19 17:25:09.601: INFO: Got endpoints: latency-svc-rbswv [1.02862036s]
Oct 19 17:25:09.601: INFO: Got endpoints: latency-svc-ccnmk [1.129069142s]
Oct 19 17:25:09.601: INFO: Got endpoints: latency-svc-j6bbl [1.078308595s]
Oct 19 17:25:09.602: INFO: Got endpoints: latency-svc-bwcwm [979.728304ms]
Oct 19 17:25:09.602: INFO: Got endpoints: latency-svc-2tvjg [879.158669ms]
Oct 19 17:25:09.602: INFO: Got endpoints: latency-svc-lwrng [828.389464ms]
Oct 19 17:25:09.602: INFO: Got endpoints: latency-svc-tst2v [929.757999ms]
Oct 19 17:25:09.602: INFO: Got endpoints: latency-svc-pv9x2 [778.72098ms]
Oct 19 17:25:09.608: INFO: Created: latency-svc-492bj
Oct 19 17:25:09.614: INFO: Created: latency-svc-jwqtc
Oct 19 17:25:09.617: INFO: Created: latency-svc-wwwq4
Oct 19 17:25:09.620: INFO: Created: latency-svc-vd66r
Oct 19 17:25:09.622: INFO: Got endpoints: latency-svc-jlvt6 [749.3355ms]
Oct 19 17:25:09.624: INFO: Created: latency-svc-rzqc8
Oct 19 17:25:09.628: INFO: Created: latency-svc-tplgn
Oct 19 17:25:09.673: INFO: Created: latency-svc-hm5nk
Oct 19 17:25:09.673: INFO: Got endpoints: latency-svc-qgctg [751.333516ms]
Oct 19 17:25:09.677: INFO: Created: latency-svc-s4kdj
Oct 19 17:25:09.679: INFO: Created: latency-svc-mk7pt
Oct 19 17:25:09.684: INFO: Created: latency-svc-brbqp
Oct 19 17:25:09.688: INFO: Created: latency-svc-vp5f8
Oct 19 17:25:09.690: INFO: Created: latency-svc-6j962
Oct 19 17:25:09.723: INFO: Got endpoints: latency-svc-gvhxl [750.53311ms]
Oct 19 17:25:09.732: INFO: Created: latency-svc-6tmwh
Oct 19 17:25:09.773: INFO: Got endpoints: latency-svc-54hgd [751.32821ms]
Oct 19 17:25:09.780: INFO: Created: latency-svc-w4s6z
Oct 19 17:25:09.822: INFO: Got endpoints: latency-svc-tgpqm [748.734867ms]
Oct 19 17:25:09.830: INFO: Created: latency-svc-s448c
Oct 19 17:25:09.874: INFO: Got endpoints: latency-svc-492bj [274.964381ms]
Oct 19 17:25:09.882: INFO: Created: latency-svc-s8fzl
Oct 19 17:25:09.922: INFO: Got endpoints: latency-svc-jwqtc [322.599997ms]
Oct 19 17:25:09.929: INFO: Created: latency-svc-b9wgj
Oct 19 17:25:09.973: INFO: Got endpoints: latency-svc-wwwq4 [371.931489ms]
Oct 19 17:25:09.980: INFO: Created: latency-svc-r65nt
Oct 19 17:25:10.022: INFO: Got endpoints: latency-svc-vd66r [421.417904ms]
Oct 19 17:25:10.029: INFO: Created: latency-svc-sz47l
Oct 19 17:25:10.072: INFO: Got endpoints: latency-svc-rzqc8 [470.88395ms]
Oct 19 17:25:10.079: INFO: Created: latency-svc-lr6ht
Oct 19 17:25:10.122: INFO: Got endpoints: latency-svc-tplgn [520.408388ms]
Oct 19 17:25:10.129: INFO: Created: latency-svc-wzdsq
Oct 19 17:25:10.172: INFO: Got endpoints: latency-svc-hm5nk [569.94196ms]
Oct 19 17:25:10.178: INFO: Created: latency-svc-npcj2
Oct 19 17:25:10.223: INFO: Got endpoints: latency-svc-s4kdj [621.011786ms]
Oct 19 17:25:10.229: INFO: Created: latency-svc-wjfks
Oct 19 17:25:10.272: INFO: Got endpoints: latency-svc-mk7pt [670.380563ms]
Oct 19 17:25:10.281: INFO: Created: latency-svc-bbtn2
Oct 19 17:25:10.323: INFO: Got endpoints: latency-svc-brbqp [721.001086ms]
Oct 19 17:25:10.375: INFO: Got endpoints: latency-svc-vp5f8 [753.441742ms]
Oct 19 17:25:10.376: INFO: Created: latency-svc-6m6tf
Oct 19 17:25:10.382: INFO: Created: latency-svc-wwpmb
Oct 19 17:25:10.473: INFO: Got endpoints: latency-svc-6tmwh [750.186165ms]
Oct 19 17:25:10.475: INFO: Got endpoints: latency-svc-6j962 [801.248718ms]
Oct 19 17:25:10.484: INFO: Created: latency-svc-lsc4s
Oct 19 17:25:10.488: INFO: Created: latency-svc-kfx8m
Oct 19 17:25:10.524: INFO: Got endpoints: latency-svc-w4s6z [751.13341ms]
Oct 19 17:25:10.542: INFO: Created: latency-svc-b5npc
Oct 19 17:25:10.572: INFO: Got endpoints: latency-svc-s448c [750.366573ms]
Oct 19 17:25:10.584: INFO: Created: latency-svc-5k2md
Oct 19 17:25:10.624: INFO: Got endpoints: latency-svc-s8fzl [749.431455ms]
Oct 19 17:25:10.636: INFO: Created: latency-svc-pnv8z
Oct 19 17:25:10.672: INFO: Got endpoints: latency-svc-b9wgj [749.445101ms]
Oct 19 17:25:10.678: INFO: Created: latency-svc-pm8b5
Oct 19 17:25:10.723: INFO: Got endpoints: latency-svc-r65nt [750.324298ms]
Oct 19 17:25:10.773: INFO: Got endpoints: latency-svc-sz47l [750.709197ms]
Oct 19 17:25:10.821: INFO: Got endpoints: latency-svc-lr6ht [749.604003ms]
Oct 19 17:25:10.872: INFO: Got endpoints: latency-svc-wzdsq [749.928723ms]
Oct 19 17:25:10.922: INFO: Got endpoints: latency-svc-npcj2 [749.723347ms]
Oct 19 17:25:10.972: INFO: Got endpoints: latency-svc-wjfks [749.139654ms]
Oct 19 17:25:11.022: INFO: Got endpoints: latency-svc-bbtn2 [749.864011ms]
Oct 19 17:25:11.072: INFO: Got endpoints: latency-svc-6m6tf [749.313925ms]
Oct 19 17:25:11.124: INFO: Got endpoints: latency-svc-wwpmb [748.608621ms]
Oct 19 17:25:11.172: INFO: Got endpoints: latency-svc-lsc4s [698.735227ms]
Oct 19 17:25:11.223: INFO: Got endpoints: latency-svc-kfx8m [748.620714ms]
Oct 19 17:25:11.272: INFO: Got endpoints: latency-svc-b5npc [747.69497ms]
Oct 19 17:25:11.323: INFO: Got endpoints: latency-svc-5k2md [750.754097ms]
Oct 19 17:25:11.372: INFO: Got endpoints: latency-svc-pnv8z [747.992486ms]
Oct 19 17:25:11.422: INFO: Got endpoints: latency-svc-pm8b5 [750.254006ms]
Oct 19 17:25:11.422: INFO: Latencies: [10.448636ms 11.871314ms 16.36573ms 19.462812ms 22.83169ms 26.555485ms 28.280871ms 35.067489ms 40.119422ms 43.164306ms 46.834262ms 51.877023ms 55.731874ms 62.844706ms 90.004134ms 93.062815ms 93.969897ms 94.391747ms 94.47051ms 94.545172ms 94.699783ms 94.730808ms 94.788322ms 94.981816ms 95.077884ms 95.321076ms 95.911806ms 96.113162ms 98.425234ms 106.740036ms 107.854455ms 108.094906ms 110.671397ms 134.940567ms 180.818885ms 227.66322ms 274.809832ms 274.964381ms 318.91037ms 322.599997ms 366.369768ms 371.931489ms 410.907088ms 421.417904ms 455.119057ms 470.88395ms 502.247664ms 520.408388ms 545.853008ms 569.94196ms 589.638788ms 597.708389ms 621.011786ms 637.253162ms 670.380563ms 683.375396ms 697.756593ms 698.112163ms 698.735227ms 721.001086ms 727.167349ms 730.151697ms 737.861379ms 741.933054ms 742.925259ms 742.968295ms 747.352824ms 747.504499ms 747.69497ms 747.734155ms 747.891077ms 747.992486ms 748.189631ms 748.308316ms 748.467237ms 748.507675ms 748.57553ms 748.608621ms 748.620714ms 748.679545ms 748.725444ms 748.734867ms 748.777402ms 748.79989ms 748.897268ms 748.911737ms 748.995168ms 749.022416ms 749.050607ms 749.076337ms 749.094956ms 749.099263ms 749.139654ms 749.187664ms 749.211757ms 749.231346ms 749.265318ms 749.277355ms 749.296064ms 749.313925ms 749.3355ms 749.351151ms 749.361577ms 749.363484ms 749.404633ms 749.418823ms 749.431455ms 749.445101ms 749.44943ms 749.529758ms 749.547801ms 749.552037ms 749.559754ms 749.562849ms 749.586258ms 749.604003ms 749.627295ms 749.690841ms 749.723347ms 749.747344ms 749.751234ms 749.8106ms 749.814586ms 749.816327ms 749.823799ms 749.864011ms 749.875963ms 749.887811ms 749.900815ms 749.903029ms 749.928723ms 750.028763ms 750.068884ms 750.078136ms 750.092769ms 750.105349ms 750.171022ms 750.186165ms 750.186181ms 750.254006ms 750.267419ms 750.303704ms 750.324298ms 750.364156ms 750.366573ms 750.390965ms 750.412473ms 750.416511ms 750.422846ms 750.43456ms 750.435059ms 750.442532ms 750.487153ms 750.489145ms 750.53311ms 750.571291ms 750.672225ms 750.687716ms 750.709197ms 750.754097ms 750.766888ms 750.80553ms 750.897562ms 751.071486ms 751.13341ms 751.145673ms 751.15321ms 751.214718ms 751.216204ms 751.282499ms 751.286178ms 751.326954ms 751.32821ms 751.333516ms 751.359292ms 751.786556ms 751.97999ms 752.429722ms 752.7571ms 753.31356ms 753.382144ms 753.441742ms 757.669228ms 757.818164ms 758.164854ms 758.386697ms 773.314429ms 778.72098ms 801.248718ms 801.358074ms 802.127815ms 828.389464ms 879.158669ms 929.757999ms 979.728304ms 1.02862036s 1.078308595s 1.129069142s 1.177200594s 1.227544092s]
Oct 19 17:25:11.422: INFO: 50 %ile: 749.3355ms
Oct 19 17:25:11.422: INFO: 90 %ile: 753.382144ms
Oct 19 17:25:11.422: INFO: 99 %ile: 1.177200594s
Oct 19 17:25:11.422: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:25:11.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6399" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":345,"skipped":6038,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 19 17:25:11.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Oct 19 17:25:12.109: INFO: created pod pod-service-account-defaultsa
Oct 19 17:25:12.109: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct 19 17:25:12.117: INFO: created pod pod-service-account-mountsa
Oct 19 17:25:12.117: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct 19 17:25:12.123: INFO: created pod pod-service-account-nomountsa
Oct 19 17:25:12.123: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct 19 17:25:12.130: INFO: created pod pod-service-account-defaultsa-mountspec
Oct 19 17:25:12.130: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct 19 17:25:12.136: INFO: created pod pod-service-account-mountsa-mountspec
Oct 19 17:25:12.136: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct 19 17:25:12.142: INFO: created pod pod-service-account-nomountsa-mountspec
Oct 19 17:25:12.142: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct 19 17:25:12.148: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct 19 17:25:12.148: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct 19 17:25:12.171: INFO: created pod pod-service-account-mountsa-nomountspec
Oct 19 17:25:12.171: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct 19 17:25:12.178: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct 19 17:25:12.178: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 19 17:25:12.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6491" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":346,"skipped":6046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSOct 19 17:25:12.185: INFO: Running AfterSuite actions on all nodes
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Oct 19 17:25:12.185: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Oct 19 17:25:12.185: INFO: Running AfterSuite actions on node 1
Oct 19 17:25:12.185: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/e2e/artifacts/1634658811/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6086,"failed":0}

Ran 346 of 6432 Specs in 5498.692 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Flaked | 0 Pending | 6086 Skipped
PASS

Ginkgo ran 1 suite in 1h31m40.483530472s
Test Suite Passed
