I1126 07:20:41.134734      22 e2e.go:129] Starting e2e run "ac993761-5996-4499-8441-e454f3302656" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1637911241 - Will randomize all specs
Will run 346 of 6432 specs

Nov 26 07:20:43.010: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:20:43.013: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 26 07:20:43.029: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 26 07:20:43.054: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 26 07:20:43.054: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Nov 26 07:20:43.054: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 26 07:20:43.061: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 26 07:20:43.061: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 26 07:20:43.061: INFO: e2e test version: v1.22.2
Nov 26 07:20:43.062: INFO: kube-apiserver version: v1.22.2
Nov 26 07:20:43.062: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:20:43.065: INFO: Cluster IP family: ipv4
SS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:20:43.065: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename containers
W1126 07:20:43.104842      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Nov 26 07:20:43.104: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Nov 26 07:20:43.110: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Nov 26 07:20:43.117: INFO: Waiting up to 5m0s for pod "client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695" in namespace "containers-9450" to be "Succeeded or Failed"
Nov 26 07:20:43.147: INFO: Pod "client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695": Phase="Pending", Reason="", readiness=false. Elapsed: 30.214473ms
Nov 26 07:20:45.150: INFO: Pod "client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033572132s
Nov 26 07:20:47.154: INFO: Pod "client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037207094s
STEP: Saw pod success
Nov 26 07:20:47.154: INFO: Pod "client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695" satisfied condition "Succeeded or Failed"
Nov 26 07:20:47.156: INFO: Trying to get logs from node cncf-node3 pod client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:20:47.191: INFO: Waiting for pod client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695 to disappear
Nov 26 07:20:47.196: INFO: Pod client-containers-37390b4d-9a22-48cf-88b5-fd9779ee8695 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:20:47.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9450" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":2,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:20:47.202: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-dd04263b-7d74-444e-b86f-20815d2d6a29
STEP: Creating a pod to test consume secrets
Nov 26 07:20:47.331: INFO: Waiting up to 5m0s for pod "pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07" in namespace "secrets-6842" to be "Succeeded or Failed"
Nov 26 07:20:47.341: INFO: Pod "pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097304ms
Nov 26 07:20:49.345: INFO: Pod "pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014356607s
Nov 26 07:20:51.350: INFO: Pod "pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018591892s
STEP: Saw pod success
Nov 26 07:20:51.350: INFO: Pod "pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07" satisfied condition "Succeeded or Failed"
Nov 26 07:20:51.351: INFO: Trying to get logs from node cncf-node2 pod pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07 container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 07:20:51.385: INFO: Waiting for pod pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07 to disappear
Nov 26 07:20:51.391: INFO: Pod pod-secrets-1623958b-de1f-4128-8478-24d5dc73ff07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:20:51.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6842" for this suite.
STEP: Destroying namespace "secret-namespace-2329" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":33,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:20:51.397: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-740f5b76-49db-4a56-a1e7-e82df6499a0a in namespace container-probe-9329
Nov 26 07:20:53.482: INFO: Started pod liveness-740f5b76-49db-4a56-a1e7-e82df6499a0a in namespace container-probe-9329
STEP: checking the pod's current state and verifying that restartCount is present
Nov 26 07:20:53.483: INFO: Initial restart count of pod liveness-740f5b76-49db-4a56-a1e7-e82df6499a0a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:24:53.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9329" for this suite.

• [SLOW TEST:242.611 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":3,"skipped":37,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:24:54.008: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Nov 26 07:24:54.085: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Nov 26 07:24:55.306: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Nov 26 07:24:57.387: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773508295, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773508295, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773508295, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773508295, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 07:25:00.609: INFO: Waited 1.215585338s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Nov 26 07:25:01.035: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:01.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3008" for this suite.

• [SLOW TEST:7.924 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":4,"skipped":51,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:01.932: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:25:01.986: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:25:03.989: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:05.990: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:07.989: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:10.002: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:11.989: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:13.990: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:15.990: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:17.989: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:19.989: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:21.989: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = false)
Nov 26 07:25:23.991: INFO: The status of Pod test-webserver-f15f98e1-3fd3-4108-9494-68cc32dc2d4e is Running (Ready = true)
Nov 26 07:25:23.992: INFO: Container started at 2021-11-26 07:25:03 +0000 UTC, pod became ready at 2021-11-26 07:25:22 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:23.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9895" for this suite.

• [SLOW TEST:22.065 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":82,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:23.998: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:25:24.065: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a339991d-8573-46f1-9723-0db9fcea1721" in namespace "security-context-test-947" to be "Succeeded or Failed"
Nov 26 07:25:24.075: INFO: Pod "busybox-user-65534-a339991d-8573-46f1-9723-0db9fcea1721": Phase="Pending", Reason="", readiness=false. Elapsed: 9.900417ms
Nov 26 07:25:26.077: INFO: Pod "busybox-user-65534-a339991d-8573-46f1-9723-0db9fcea1721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01236701s
Nov 26 07:25:26.077: INFO: Pod "busybox-user-65534-a339991d-8573-46f1-9723-0db9fcea1721" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:26.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-947" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":143,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:26.082: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:25:26.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-8959 version'
Nov 26 07:25:26.212: INFO: stderr: ""
Nov 26 07:25:26.212: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\", GitCommit:\"8b5a19147530eaac9476b0ab82980b4088bbc1b2\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:38:50Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.2\", GitCommit:\"8b5a19147530eaac9476b0ab82980b4088bbc1b2\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:32:41Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:26.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8959" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":7,"skipped":157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:26.218: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:25:26.281: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 26 07:25:28.327: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:29.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7157" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":8,"skipped":189,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:29.350: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 26 07:25:29.428: INFO: The status of Pod labelsupdate557637ce-5146-44b3-8607-d6c8b5e3f112 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:25:31.430: INFO: The status of Pod labelsupdate557637ce-5146-44b3-8607-d6c8b5e3f112 is Running (Ready = true)
Nov 26 07:25:31.975: INFO: Successfully updated pod "labelsupdate557637ce-5146-44b3-8607-d6c8b5e3f112"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:35.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6598" for this suite.

• [SLOW TEST:6.650 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:36.001: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:36.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2704" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":10,"skipped":315,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6605
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6605
I1126 07:25:36.241385      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6605, replica count: 2
Nov 26 07:25:39.292: INFO: Creating new exec pod
I1126 07:25:39.292915      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 07:25:42.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 26 07:25:43.312: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:43.312: INFO: stdout: ""
Nov 26 07:25:44.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 26 07:25:44.479: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:44.479: INFO: stdout: "externalname-service-p9ffp"
Nov 26 07:25:44.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:44.653: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:44.653: INFO: stdout: ""
Nov 26 07:25:45.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:45.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:45.809: INFO: stdout: ""
Nov 26 07:25:46.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:46.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:46.813: INFO: stdout: ""
Nov 26 07:25:47.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:47.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:47.814: INFO: stdout: ""
Nov 26 07:25:48.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:48.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:48.808: INFO: stdout: ""
Nov 26 07:25:49.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:49.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:49.818: INFO: stdout: ""
Nov 26 07:25:50.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:50.787: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:50.787: INFO: stdout: ""
Nov 26 07:25:51.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.176.246 80'
Nov 26 07:25:51.807: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.176.246 80\nConnection to 10.96.176.246 80 port [tcp/http] succeeded!\n"
Nov 26 07:25:51.807: INFO: stdout: "externalname-service-p9ffp"
Nov 26 07:25:51.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.8 31514'
Nov 26 07:25:51.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.8 31514\nConnection to 172.21.7.8 31514 port [tcp/*] succeeded!\n"
Nov 26 07:25:51.960: INFO: stdout: "externalname-service-7mjr6"
Nov 26 07:25:51.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.12 31514'
Nov 26 07:25:52.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.12 31514\nConnection to 172.21.7.12 31514 port [tcp/*] succeeded!\n"
Nov 26 07:25:52.119: INFO: stdout: ""
Nov 26 07:25:53.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6605 exec execpodcp9d4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.12 31514'
Nov 26 07:25:53.271: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.12 31514\nConnection to 172.21.7.12 31514 port [tcp/*] succeeded!\n"
Nov 26 07:25:53.271: INFO: stdout: "externalname-service-p9ffp"
Nov 26 07:25:53.271: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:53.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6605" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:17.282 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":11,"skipped":328,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:53.405: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 26 07:25:53.451: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5071  f6a0331d-9c81-42ce-97a3-743dc42d512c 88312 0 2021-11-26 07:25:53 +0000 UTC <nil> <nil> map[] map[createdTime:2021-11-26T16:25:54.624045826+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:25:54.624045826+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:25:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jzlfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jzlfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 07:25:53.460: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:25:55.463: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 26 07:25:55.463: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5071 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:25:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Verifying customized DNS server is configured on pod...
Nov 26 07:25:55.577: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5071 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:25:55.577: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:25:55.675: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:25:55.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5071" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":12,"skipped":347,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:25:55.705: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 26 07:25:55.799: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 07:25:55.802: INFO: Number of nodes with available pods: 0
Nov 26 07:25:55.802: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 07:25:56.814: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 07:25:56.816: INFO: Number of nodes with available pods: 0
Nov 26 07:25:56.816: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 07:25:57.806: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 07:25:57.808: INFO: Number of nodes with available pods: 2
Nov 26 07:25:57.808: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Getting /status
Nov 26 07:25:57.812: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Nov 26 07:25:57.842: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Nov 26 07:25:57.843: INFO: Observed &DaemonSet event: ADDED
Nov 26 07:25:57.843: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.843: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.843: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.843: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.843: INFO: Found daemon set daemon-set in namespace daemonsets-820 with labels: map[daemonset-name:daemon-set] annotations: map[createdTime:2021-11-26T16:25:56.962980143+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2021-11-26T16:25:56.962980143+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 26 07:25:57.843: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Nov 26 07:25:57.847: INFO: Observed &DaemonSet event: ADDED
Nov 26 07:25:57.847: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.848: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.848: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.848: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.848: INFO: Observed daemon set daemon-set in namespace daemonsets-820 with annotations: map[createdTime:2021-11-26T16:25:56.962980143+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2021-11-26T16:25:56.962980143+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 26 07:25:57.848: INFO: Observed &DaemonSet event: MODIFIED
Nov 26 07:25:57.848: INFO: Found daemon set daemon-set in namespace daemonsets-820 with labels: map[daemonset-name:daemon-set] annotations: map[createdTime:2021-11-26T16:25:56.962980143+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deprecated.daemonset.template.generation:1 updatedTime:2021-11-26T16:25:56.962980143+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 26 07:25:57.848: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-820, will wait for the garbage collector to delete the pods
Nov 26 07:25:57.919: INFO: Deleting DaemonSet.extensions daemon-set took: 2.275671ms
Nov 26 07:25:58.020: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.875906ms
Nov 26 07:26:00.522: INFO: Number of nodes with available pods: 0
Nov 26 07:26:00.522: INFO: Number of running nodes: 0, number of available pods: 0
Nov 26 07:26:00.523: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"88467"},"items":null}

Nov 26 07:26:00.544: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"88467"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:26:00.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-820" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":13,"skipped":348,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:26:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Nov 26 07:26:40.753: INFO: The status of Pod kube-controller-manager-cncf-node1 is Running (Ready = true)
Nov 26 07:26:40.962: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 26 07:26:40.962: INFO: Deleting pod "simpletest.rc-4fl2v" in namespace "gc-2854"
Nov 26 07:26:40.984: INFO: Deleting pod "simpletest.rc-4spcm" in namespace "gc-2854"
Nov 26 07:26:41.007: INFO: Deleting pod "simpletest.rc-dh6lv" in namespace "gc-2854"
Nov 26 07:26:41.071: INFO: Deleting pod "simpletest.rc-g6clj" in namespace "gc-2854"
Nov 26 07:26:41.087: INFO: Deleting pod "simpletest.rc-j4rt9" in namespace "gc-2854"
Nov 26 07:26:41.127: INFO: Deleting pod "simpletest.rc-jjtwn" in namespace "gc-2854"
Nov 26 07:26:41.150: INFO: Deleting pod "simpletest.rc-llnct" in namespace "gc-2854"
Nov 26 07:26:41.196: INFO: Deleting pod "simpletest.rc-w9fhj" in namespace "gc-2854"
Nov 26 07:26:41.219: INFO: Deleting pod "simpletest.rc-xwfqj" in namespace "gc-2854"
Nov 26 07:26:41.242: INFO: Deleting pod "simpletest.rc-zc4t4" in namespace "gc-2854"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:26:41.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2854" for this suite.

• [SLOW TEST:40.724 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":14,"skipped":361,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:26:41.278: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Nov 26 07:26:41.347: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-9232 proxy --unix-socket=/tmp/kubectl-proxy-unix737900183/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:26:41.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9232" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":15,"skipped":363,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:26:41.389: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:26:41.796: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:26:44.820: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:26:44.823: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:26:48.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2916" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.672 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":16,"skipped":376,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:26:48.061: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Nov 26 07:26:48.151: INFO: created test-pod-1
Nov 26 07:26:48.158: INFO: created test-pod-2
Nov 26 07:26:48.168: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Nov 26 07:26:48.282: INFO: Pod quantity 3 is different from expected quantity 0
Nov 26 07:26:49.285: INFO: Pod quantity 3 is different from expected quantity 0
Nov 26 07:26:50.285: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:26:51.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7301" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":17,"skipped":390,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:26:51.293: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2467
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2467
I1126 07:26:51.391389      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2467, replica count: 2
Nov 26 07:26:54.442: INFO: Creating new exec pod
I1126 07:26:54.442774      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 07:26:57.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2467 exec execpodmptnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 26 07:26:57.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 26 07:26:57.672: INFO: stdout: "externalname-service-8rrpw"
Nov 26 07:26:57.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2467 exec execpodmptnv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.198.41 80'
Nov 26 07:26:57.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.198.41 80\nConnection to 10.96.198.41 80 port [tcp/http] succeeded!\n"
Nov 26 07:26:57.818: INFO: stdout: "externalname-service-8rrpw"
Nov 26 07:26:57.818: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:26:57.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2467" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:6.564 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":18,"skipped":390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:26:57.857: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5235 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5235;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5235 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5235;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5235.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5235.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5235.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5235.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5235.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 211.104.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.104.211_udp@PTR;check="$$(dig +tcp +noall +answer +search 211.104.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.104.211_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5235 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5235;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5235 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5235;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5235.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5235.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5235.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5235.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5235.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5235.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5235.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 211.104.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.104.211_udp@PTR;check="$$(dig +tcp +noall +answer +search 211.104.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.104.211_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 07:26:59.953: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.956: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.958: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.962: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.963: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.965: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.967: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.981: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.983: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.984: INFO: Unable to read jessie_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.986: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.988: INFO: Unable to read jessie_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.990: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.992: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:26:59.995: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:00.006: INFO: Lookups using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5235 wheezy_tcp@dns-test-service.dns-5235 wheezy_udp@dns-test-service.dns-5235.svc wheezy_tcp@dns-test-service.dns-5235.svc wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5235 jessie_tcp@dns-test-service.dns-5235 jessie_udp@dns-test-service.dns-5235.svc jessie_tcp@dns-test-service.dns-5235.svc jessie_udp@_http._tcp.dns-test-service.dns-5235.svc jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc]

Nov 26 07:27:05.010: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.012: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.021: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.035: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.044: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.046: INFO: Unable to read jessie_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.048: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.050: INFO: Unable to read jessie_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.052: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.054: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.056: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:05.082: INFO: Lookups using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5235 wheezy_tcp@dns-test-service.dns-5235 wheezy_udp@dns-test-service.dns-5235.svc wheezy_tcp@dns-test-service.dns-5235.svc wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5235 jessie_tcp@dns-test-service.dns-5235 jessie_udp@dns-test-service.dns-5235.svc jessie_tcp@dns-test-service.dns-5235.svc jessie_udp@_http._tcp.dns-test-service.dns-5235.svc jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc]

Nov 26 07:27:10.010: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.013: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.021: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.023: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.025: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.040: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.042: INFO: Unable to read jessie_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.045: INFO: Unable to read jessie_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.047: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.051: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:10.063: INFO: Lookups using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5235 wheezy_tcp@dns-test-service.dns-5235 wheezy_udp@dns-test-service.dns-5235.svc wheezy_tcp@dns-test-service.dns-5235.svc wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5235 jessie_tcp@dns-test-service.dns-5235 jessie_udp@dns-test-service.dns-5235.svc jessie_tcp@dns-test-service.dns-5235.svc jessie_udp@_http._tcp.dns-test-service.dns-5235.svc jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc]

Nov 26 07:27:15.011: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.013: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.022: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.024: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.040: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.042: INFO: Unable to read jessie_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.045: INFO: Unable to read jessie_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.047: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.050: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:15.061: INFO: Lookups using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5235 wheezy_tcp@dns-test-service.dns-5235 wheezy_udp@dns-test-service.dns-5235.svc wheezy_tcp@dns-test-service.dns-5235.svc wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5235 jessie_tcp@dns-test-service.dns-5235 jessie_udp@dns-test-service.dns-5235.svc jessie_tcp@dns-test-service.dns-5235.svc jessie_udp@_http._tcp.dns-test-service.dns-5235.svc jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc]

Nov 26 07:27:20.012: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.014: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.020: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.023: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.024: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.027: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.040: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.042: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.044: INFO: Unable to read jessie_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.046: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.048: INFO: Unable to read jessie_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.050: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.052: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.053: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:20.064: INFO: Lookups using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5235 wheezy_tcp@dns-test-service.dns-5235 wheezy_udp@dns-test-service.dns-5235.svc wheezy_tcp@dns-test-service.dns-5235.svc wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5235 jessie_tcp@dns-test-service.dns-5235 jessie_udp@dns-test-service.dns-5235.svc jessie_tcp@dns-test-service.dns-5235.svc jessie_udp@_http._tcp.dns-test-service.dns-5235.svc jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc]

Nov 26 07:27:25.012: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.014: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.022: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.024: INFO: Unable to read wheezy_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.028: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.030: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.068: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.071: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.073: INFO: Unable to read jessie_udp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.074: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235 from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.076: INFO: Unable to read jessie_udp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.078: INFO: Unable to read jessie_tcp@dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.080: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc from pod dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5: the server could not find the requested resource (get pods dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5)
Nov 26 07:27:25.094: INFO: Lookups using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5235 wheezy_tcp@dns-test-service.dns-5235 wheezy_udp@dns-test-service.dns-5235.svc wheezy_tcp@dns-test-service.dns-5235.svc wheezy_udp@_http._tcp.dns-test-service.dns-5235.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5235.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5235 jessie_tcp@dns-test-service.dns-5235 jessie_udp@dns-test-service.dns-5235.svc jessie_tcp@dns-test-service.dns-5235.svc jessie_udp@_http._tcp.dns-test-service.dns-5235.svc jessie_tcp@_http._tcp.dns-test-service.dns-5235.svc]

Nov 26 07:27:30.061: INFO: DNS probes using dns-5235/dns-test-61fd0828-cc9a-4636-9e7d-2ef005809bb5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:27:30.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5235" for this suite.

• [SLOW TEST:32.339 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":19,"skipped":431,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:27:30.197: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 26 07:27:32.281: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:27:32.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9404" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:27:32.304: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 26 07:27:32.380: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 26 07:27:32.382: INFO: starting watch
STEP: patching
STEP: updating
Nov 26 07:27:32.393: INFO: waiting for watch events with expected annotations
Nov 26 07:27:32.393: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:27:32.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4481" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":21,"skipped":487,"failed":0}
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:27:32.440: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 26 07:27:32.467: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:27:35.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3892" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":22,"skipped":493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:27:35.495: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 26 07:27:35.569: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 26 07:28:35.618: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Nov 26 07:28:35.641: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 26 07:28:35.648: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 26 07:28:35.680: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 26 07:28:35.694: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:28:47.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4715" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.325 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":23,"skipped":518,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:28:47.820: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 26 07:28:47.892: INFO: Waiting up to 5m0s for pod "pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6" in namespace "emptydir-6910" to be "Succeeded or Failed"
Nov 26 07:28:47.899: INFO: Pod "pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.86982ms
Nov 26 07:28:49.902: INFO: Pod "pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009865798s
STEP: Saw pod success
Nov 26 07:28:49.902: INFO: Pod "pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6" satisfied condition "Succeeded or Failed"
Nov 26 07:28:49.903: INFO: Trying to get logs from node cncf-node2 pod pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6 container test-container: <nil>
STEP: delete the pod
Nov 26 07:28:49.933: INFO: Waiting for pod pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6 to disappear
Nov 26 07:28:49.938: INFO: Pod pod-a51a98cb-dab4-45e1-a6c1-9662c8a4fad6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:28:49.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6910" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":536,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:28:49.943: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:28:50.006: INFO: created pod
Nov 26 07:28:50.006: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8010" to be "Succeeded or Failed"
Nov 26 07:28:50.035: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 28.647935ms
Nov 26 07:28:52.038: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.032008304s
Nov 26 07:28:54.044: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 4.037654114s
Nov 26 07:28:56.047: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 6.040788346s
Nov 26 07:28:58.050: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.043938324s
STEP: Saw pod success
Nov 26 07:28:58.050: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 26 07:29:28.051: INFO: polling logs
Nov 26 07:29:28.055: INFO: Pod logs: 
2021/11/26 07:28:52 OK: Got token
2021/11/26 07:28:52 validating with in-cluster discovery
2021/11/26 07:28:52 OK: got issuer https://kubernetes.default.svc.cluster.local
2021/11/26 07:28:52 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8010:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1637912330, NotBefore:1637911730, IssuedAt:1637911730, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8010", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2cf47a59-e335-4f84-9ff7-efd88484a5d6"}}}
2021/11/26 07:28:57 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2021/11/26 07:28:57 OK: Validated signature on JWT
2021/11/26 07:28:57 OK: Got valid claims from token!
2021/11/26 07:28:57 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8010:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1637912330, NotBefore:1637911730, IssuedAt:1637911730, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8010", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2cf47a59-e335-4f84-9ff7-efd88484a5d6"}}}

Nov 26 07:29:28.055: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:28.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8010" for this suite.

• [SLOW TEST:38.147 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":25,"skipped":538,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:28.090: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:32.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2903" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":548,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:32.162: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 26 07:29:32.261: INFO: The status of Pod annotationupdateee15159d-10d0-47d9-abd5-c3b1d84b729c is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:29:34.264: INFO: The status of Pod annotationupdateee15159d-10d0-47d9-abd5-c3b1d84b729c is Running (Ready = true)
Nov 26 07:29:34.778: INFO: Successfully updated pod "annotationupdateee15159d-10d0-47d9-abd5-c3b1d84b729c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:36.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9102" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":549,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:36.803: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Nov 26 07:29:38.904: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:40.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5320" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":28,"skipped":551,"failed":0}
S
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:40.913: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Nov 26 07:29:41.000: INFO: The status of Pod pod-hostip-11ba934e-0d82-4c5a-955d-202b957712c8 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:29:43.005: INFO: The status of Pod pod-hostip-11ba934e-0d82-4c5a-955d-202b957712c8 is Running (Ready = true)
Nov 26 07:29:43.007: INFO: Pod pod-hostip-11ba934e-0d82-4c5a-955d-202b957712c8 has hostIP: 172.21.7.8
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:43.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4510" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":552,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:43.012: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-875" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":30,"skipped":561,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:43.071: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:43.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7967" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":31,"skipped":582,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:43.208: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 26 07:29:43.283: INFO: Waiting up to 5m0s for pod "pod-78baa394-f947-49e8-9e76-873d229187bc" in namespace "emptydir-8053" to be "Succeeded or Failed"
Nov 26 07:29:43.292: INFO: Pod "pod-78baa394-f947-49e8-9e76-873d229187bc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.350728ms
Nov 26 07:29:45.294: INFO: Pod "pod-78baa394-f947-49e8-9e76-873d229187bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011831009s
STEP: Saw pod success
Nov 26 07:29:45.295: INFO: Pod "pod-78baa394-f947-49e8-9e76-873d229187bc" satisfied condition "Succeeded or Failed"
Nov 26 07:29:45.296: INFO: Trying to get logs from node cncf-node2 pod pod-78baa394-f947-49e8-9e76-873d229187bc container test-container: <nil>
STEP: delete the pod
Nov 26 07:29:45.315: INFO: Waiting for pod pod-78baa394-f947-49e8-9e76-873d229187bc to disappear
Nov 26 07:29:45.320: INFO: Pod pod-78baa394-f947-49e8-9e76-873d229187bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8053" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":601,"failed":0}

------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:45.324: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-6369
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6369
STEP: Deleting pre-stop pod
Nov 26 07:29:54.434: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:29:54.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6369" for this suite.

• [SLOW TEST:9.137 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":33,"skipped":601,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:29:54.462: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:07.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-365" for this suite.

• [SLOW TEST:13.134 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":34,"skipped":606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:07.596: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Nov 26 07:30:08.688: INFO: The status of Pod kube-controller-manager-cncf-node1 is Running (Ready = true)
Nov 26 07:30:08.922: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:08.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6666" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":35,"skipped":649,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:08.929: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-fffd3295-d92a-4dbb-b2a4-fceb36cde8a1
STEP: Creating a pod to test consume secrets
Nov 26 07:30:08.983: INFO: Waiting up to 5m0s for pod "pod-secrets-348efe90-9942-4e91-b583-346c461d6af1" in namespace "secrets-7384" to be "Succeeded or Failed"
Nov 26 07:30:08.985: INFO: Pod "pod-secrets-348efe90-9942-4e91-b583-346c461d6af1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.517705ms
Nov 26 07:30:10.988: INFO: Pod "pod-secrets-348efe90-9942-4e91-b583-346c461d6af1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004841978s
STEP: Saw pod success
Nov 26 07:30:10.988: INFO: Pod "pod-secrets-348efe90-9942-4e91-b583-346c461d6af1" satisfied condition "Succeeded or Failed"
Nov 26 07:30:10.989: INFO: Trying to get logs from node cncf-node2 pod pod-secrets-348efe90-9942-4e91-b583-346c461d6af1 container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 07:30:11.009: INFO: Waiting for pod pod-secrets-348efe90-9942-4e91-b583-346c461d6af1 to disappear
Nov 26 07:30:11.014: INFO: Pod pod-secrets-348efe90-9942-4e91-b583-346c461d6af1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:11.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7384" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:11.019: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Nov 26 07:30:11.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 create -f -'
Nov 26 07:30:11.416: INFO: stderr: ""
Nov 26 07:30:11.416: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 26 07:30:11.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 07:30:11.527: INFO: stderr: ""
Nov 26 07:30:11.527: INFO: stdout: "update-demo-nautilus-5cgxn update-demo-nautilus-dc2d4 "
Nov 26 07:30:11.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:11.581: INFO: stderr: ""
Nov 26 07:30:11.581: INFO: stdout: ""
Nov 26 07:30:11.581: INFO: update-demo-nautilus-5cgxn is created but not running
Nov 26 07:30:16.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 07:30:16.633: INFO: stderr: ""
Nov 26 07:30:16.633: INFO: stdout: "update-demo-nautilus-5cgxn update-demo-nautilus-dc2d4 "
Nov 26 07:30:16.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:16.683: INFO: stderr: ""
Nov 26 07:30:16.683: INFO: stdout: "true"
Nov 26 07:30:16.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 07:30:16.731: INFO: stderr: ""
Nov 26 07:30:16.731: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 07:30:16.731: INFO: validating pod update-demo-nautilus-5cgxn
Nov 26 07:30:16.734: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 07:30:16.734: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 07:30:16.734: INFO: update-demo-nautilus-5cgxn is verified up and running
Nov 26 07:30:16.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-dc2d4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:16.787: INFO: stderr: ""
Nov 26 07:30:16.787: INFO: stdout: "true"
Nov 26 07:30:16.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-dc2d4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 07:30:16.848: INFO: stderr: ""
Nov 26 07:30:16.848: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 07:30:16.848: INFO: validating pod update-demo-nautilus-dc2d4
Nov 26 07:30:16.851: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 07:30:16.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 07:30:16.851: INFO: update-demo-nautilus-dc2d4 is verified up and running
STEP: scaling down the replication controller
Nov 26 07:30:16.853: INFO: scanned /root for discovery docs: <nil>
Nov 26 07:30:16.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 26 07:30:17.927: INFO: stderr: ""
Nov 26 07:30:17.927: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 26 07:30:17.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 07:30:17.980: INFO: stderr: ""
Nov 26 07:30:17.980: INFO: stdout: "update-demo-nautilus-5cgxn "
Nov 26 07:30:17.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:18.029: INFO: stderr: ""
Nov 26 07:30:18.029: INFO: stdout: "true"
Nov 26 07:30:18.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 07:30:18.077: INFO: stderr: ""
Nov 26 07:30:18.077: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 07:30:18.077: INFO: validating pod update-demo-nautilus-5cgxn
Nov 26 07:30:18.080: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 07:30:18.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 07:30:18.080: INFO: update-demo-nautilus-5cgxn is verified up and running
STEP: scaling up the replication controller
Nov 26 07:30:18.081: INFO: scanned /root for discovery docs: <nil>
Nov 26 07:30:18.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 26 07:30:19.148: INFO: stderr: ""
Nov 26 07:30:19.148: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 26 07:30:19.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 07:30:19.201: INFO: stderr: ""
Nov 26 07:30:19.202: INFO: stdout: "update-demo-nautilus-5cgxn update-demo-nautilus-8jwkw "
Nov 26 07:30:19.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:19.250: INFO: stderr: ""
Nov 26 07:30:19.250: INFO: stdout: "true"
Nov 26 07:30:19.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 07:30:19.301: INFO: stderr: ""
Nov 26 07:30:19.301: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 07:30:19.301: INFO: validating pod update-demo-nautilus-5cgxn
Nov 26 07:30:19.303: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 07:30:19.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 07:30:19.303: INFO: update-demo-nautilus-5cgxn is verified up and running
Nov 26 07:30:19.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-8jwkw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:19.356: INFO: stderr: ""
Nov 26 07:30:19.356: INFO: stdout: ""
Nov 26 07:30:19.357: INFO: update-demo-nautilus-8jwkw is created but not running
Nov 26 07:30:24.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 07:30:24.417: INFO: stderr: ""
Nov 26 07:30:24.417: INFO: stdout: "update-demo-nautilus-5cgxn update-demo-nautilus-8jwkw "
Nov 26 07:30:24.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:24.467: INFO: stderr: ""
Nov 26 07:30:24.467: INFO: stdout: "true"
Nov 26 07:30:24.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-5cgxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 07:30:24.517: INFO: stderr: ""
Nov 26 07:30:24.517: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 07:30:24.517: INFO: validating pod update-demo-nautilus-5cgxn
Nov 26 07:30:24.520: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 07:30:24.520: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 07:30:24.520: INFO: update-demo-nautilus-5cgxn is verified up and running
Nov 26 07:30:24.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-8jwkw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 07:30:24.570: INFO: stderr: ""
Nov 26 07:30:24.570: INFO: stdout: "true"
Nov 26 07:30:24.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods update-demo-nautilus-8jwkw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 07:30:24.625: INFO: stderr: ""
Nov 26 07:30:24.625: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 07:30:24.625: INFO: validating pod update-demo-nautilus-8jwkw
Nov 26 07:30:24.629: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 07:30:24.629: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 07:30:24.629: INFO: update-demo-nautilus-8jwkw is verified up and running
STEP: using delete to clean up resources
Nov 26 07:30:24.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 delete --grace-period=0 --force -f -'
Nov 26 07:30:24.680: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 07:30:24.680: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 26 07:30:24.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get rc,svc -l name=update-demo --no-headers'
Nov 26 07:30:24.746: INFO: stderr: "No resources found in kubectl-7516 namespace.\n"
Nov 26 07:30:24.746: INFO: stdout: ""
Nov 26 07:30:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7516 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 26 07:30:24.799: INFO: stderr: ""
Nov 26 07:30:24.799: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:24.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7516" for this suite.

• [SLOW TEST:13.786 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":37,"skipped":717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:24.804: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:30:24.884: INFO: The status of Pod busybox-scheduling-a297e023-8f5c-4e61-bca7-01a88c8486ff is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:30:26.887: INFO: The status of Pod busybox-scheduling-a297e023-8f5c-4e61-bca7-01a88c8486ff is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:26.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8859" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:26.897: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:30:26.968: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-d9f4e433-34d2-412c-ade1-33fef93bf507" in namespace "security-context-test-9772" to be "Succeeded or Failed"
Nov 26 07:30:26.971: INFO: Pod "busybox-privileged-false-d9f4e433-34d2-412c-ade1-33fef93bf507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.797876ms
Nov 26 07:30:28.976: INFO: Pod "busybox-privileged-false-d9f4e433-34d2-412c-ade1-33fef93bf507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007817161s
Nov 26 07:30:28.976: INFO: Pod "busybox-privileged-false-d9f4e433-34d2-412c-ade1-33fef93bf507" satisfied condition "Succeeded or Failed"
Nov 26 07:30:28.987: INFO: Got logs for pod "busybox-privileged-false-d9f4e433-34d2-412c-ade1-33fef93bf507": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:28.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9772" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":782,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:29.017: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Nov 26 07:30:31.119: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1334" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":40,"skipped":799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:33.164: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Nov 26 07:30:33.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6773 api-versions'
Nov 26 07:30:33.298: INFO: stderr: ""
Nov 26 07:30:33.298: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nceph.rook.io/v1\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1\nclaim.tmax.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhypercloud.tmaxcloud.com/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nobjectbucket.io/v1alpha1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrook.io/v1alpha2\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntmax.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:33.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6773" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":41,"skipped":856,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:33.315: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-p44q
STEP: Creating a pod to test atomic-volume-subpath
Nov 26 07:30:33.415: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p44q" in namespace "subpath-4008" to be "Succeeded or Failed"
Nov 26 07:30:33.423: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Pending", Reason="", readiness=false. Elapsed: 7.251914ms
Nov 26 07:30:35.425: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 2.009815824s
Nov 26 07:30:37.428: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 4.012322223s
Nov 26 07:30:39.430: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 6.01426898s
Nov 26 07:30:41.432: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 8.016748236s
Nov 26 07:30:43.434: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 10.019166961s
Nov 26 07:30:45.437: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 12.021634221s
Nov 26 07:30:47.441: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 14.025534299s
Nov 26 07:30:49.463: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 16.047293815s
Nov 26 07:30:51.465: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 18.049919887s
Nov 26 07:30:53.467: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Running", Reason="", readiness=true. Elapsed: 20.052226775s
Nov 26 07:30:55.471: INFO: Pod "pod-subpath-test-configmap-p44q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.055562642s
STEP: Saw pod success
Nov 26 07:30:55.471: INFO: Pod "pod-subpath-test-configmap-p44q" satisfied condition "Succeeded or Failed"
Nov 26 07:30:55.472: INFO: Trying to get logs from node cncf-node3 pod pod-subpath-test-configmap-p44q container test-container-subpath-configmap-p44q: <nil>
STEP: delete the pod
Nov 26 07:30:55.493: INFO: Waiting for pod pod-subpath-test-configmap-p44q to disappear
Nov 26 07:30:55.499: INFO: Pod pod-subpath-test-configmap-p44q no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p44q
Nov 26 07:30:55.499: INFO: Deleting pod "pod-subpath-test-configmap-p44q" in namespace "subpath-4008"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:55.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4008" for this suite.

• [SLOW TEST:22.190 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":42,"skipped":862,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:55.504: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-43077a90-1aa4-4af1-b925-943742c75aea
STEP: Creating a pod to test consume configMaps
Nov 26 07:30:55.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c" in namespace "configmap-6078" to be "Succeeded or Failed"
Nov 26 07:30:55.596: INFO: Pod "pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.454349ms
Nov 26 07:30:57.601: INFO: Pod "pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01840822s
STEP: Saw pod success
Nov 26 07:30:57.601: INFO: Pod "pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c" satisfied condition "Succeeded or Failed"
Nov 26 07:30:57.602: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:30:57.625: INFO: Waiting for pod pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c to disappear
Nov 26 07:30:57.630: INFO: Pod pod-configmaps-d6b6472b-6538-4df1-baa3-41205ac13c5c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:30:57.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6078" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":871,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:30:57.660: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-2352
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2352 to expose endpoints map[]
Nov 26 07:30:57.722: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 26 07:30:58.740: INFO: successfully validated that service endpoint-test2 in namespace services-2352 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2352
Nov 26 07:30:58.751: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:31:00.754: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2352 to expose endpoints map[pod1:[80]]
Nov 26 07:31:00.760: INFO: successfully validated that service endpoint-test2 in namespace services-2352 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Nov 26 07:31:00.760: INFO: Creating new exec pod
Nov 26 07:31:03.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2352 exec execpodglzsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 26 07:31:03.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:03.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 07:31:03.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2352 exec execpodglzsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.80.53 80'
Nov 26 07:31:04.122: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.80.53 80\nConnection to 10.96.80.53 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:04.122: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2352
Nov 26 07:31:04.134: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:31:06.139: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2352 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 26 07:31:06.148: INFO: successfully validated that service endpoint-test2 in namespace services-2352 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Nov 26 07:31:07.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2352 exec execpodglzsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 26 07:31:07.313: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:07.313: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 07:31:07.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2352 exec execpodglzsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.80.53 80'
Nov 26 07:31:07.458: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.80.53 80\nConnection to 10.96.80.53 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:07.458: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2352
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2352 to expose endpoints map[pod2:[80]]
Nov 26 07:31:07.525: INFO: successfully validated that service endpoint-test2 in namespace services-2352 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Nov 26 07:31:08.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2352 exec execpodglzsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 26 07:31:08.684: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:08.684: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 07:31:08.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-2352 exec execpodglzsz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.80.53 80'
Nov 26 07:31:08.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.80.53 80\nConnection to 10.96.80.53 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:08.854: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2352
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2352 to expose endpoints map[]
Nov 26 07:31:08.899: INFO: successfully validated that service endpoint-test2 in namespace services-2352 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:31:08.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2352" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.304 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":44,"skipped":882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:31:08.964: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-e7281c55-58d7-47c1-9a23-515345f17ca1
STEP: Creating a pod to test consume configMaps
Nov 26 07:31:09.024: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508" in namespace "projected-6066" to be "Succeeded or Failed"
Nov 26 07:31:09.030: INFO: Pod "pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284852ms
Nov 26 07:31:11.033: INFO: Pod "pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00868327s
STEP: Saw pod success
Nov 26 07:31:11.033: INFO: Pod "pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508" satisfied condition "Succeeded or Failed"
Nov 26 07:31:11.034: INFO: Trying to get logs from node cncf-node2 pod pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:31:11.050: INFO: Waiting for pod pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508 to disappear
Nov 26 07:31:11.059: INFO: Pod pod-projected-configmaps-512cd4f0-a10a-4649-b7bf-d4cea1029508 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:31:11.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6066" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":909,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:31:11.064: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:31:11.619: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 26 07:31:11.620: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 26 07:31:11.620: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 26 07:31:11.620: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 26 07:31:11.620: INFO: Checking APIGroup: apps
Nov 26 07:31:11.621: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 26 07:31:11.621: INFO: Versions found [{apps/v1 v1}]
Nov 26 07:31:11.621: INFO: apps/v1 matches apps/v1
Nov 26 07:31:11.621: INFO: Checking APIGroup: events.k8s.io
Nov 26 07:31:11.621: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 26 07:31:11.621: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 26 07:31:11.621: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 26 07:31:11.621: INFO: Checking APIGroup: authentication.k8s.io
Nov 26 07:31:11.622: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 26 07:31:11.622: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 26 07:31:11.622: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 26 07:31:11.622: INFO: Checking APIGroup: authorization.k8s.io
Nov 26 07:31:11.622: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 26 07:31:11.622: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 26 07:31:11.623: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 26 07:31:11.623: INFO: Checking APIGroup: autoscaling
Nov 26 07:31:11.623: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov 26 07:31:11.623: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 26 07:31:11.623: INFO: autoscaling/v1 matches autoscaling/v1
Nov 26 07:31:11.623: INFO: Checking APIGroup: batch
Nov 26 07:31:11.624: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 26 07:31:11.624: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 26 07:31:11.624: INFO: batch/v1 matches batch/v1
Nov 26 07:31:11.624: INFO: Checking APIGroup: certificates.k8s.io
Nov 26 07:31:11.624: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 26 07:31:11.624: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 26 07:31:11.624: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 26 07:31:11.624: INFO: Checking APIGroup: networking.k8s.io
Nov 26 07:31:11.625: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 26 07:31:11.625: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 26 07:31:11.625: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 26 07:31:11.625: INFO: Checking APIGroup: policy
Nov 26 07:31:11.625: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 26 07:31:11.625: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Nov 26 07:31:11.625: INFO: policy/v1 matches policy/v1
Nov 26 07:31:11.625: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 26 07:31:11.626: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 26 07:31:11.626: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 26 07:31:11.626: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 26 07:31:11.626: INFO: Checking APIGroup: storage.k8s.io
Nov 26 07:31:11.627: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 26 07:31:11.627: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 26 07:31:11.627: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 26 07:31:11.627: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 26 07:31:11.627: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 26 07:31:11.627: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 26 07:31:11.627: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 26 07:31:11.627: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 26 07:31:11.628: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 26 07:31:11.628: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 26 07:31:11.628: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 26 07:31:11.628: INFO: Checking APIGroup: scheduling.k8s.io
Nov 26 07:31:11.628: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 26 07:31:11.628: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 26 07:31:11.628: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 26 07:31:11.628: INFO: Checking APIGroup: coordination.k8s.io
Nov 26 07:31:11.629: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 26 07:31:11.629: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 26 07:31:11.629: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 26 07:31:11.629: INFO: Checking APIGroup: node.k8s.io
Nov 26 07:31:11.630: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 26 07:31:11.630: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Nov 26 07:31:11.630: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 26 07:31:11.630: INFO: Checking APIGroup: discovery.k8s.io
Nov 26 07:31:11.630: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 26 07:31:11.630: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Nov 26 07:31:11.630: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 26 07:31:11.630: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 26 07:31:11.631: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Nov 26 07:31:11.631: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Nov 26 07:31:11.631: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Nov 26 07:31:11.631: INFO: Checking APIGroup: acme.cert-manager.io
Nov 26 07:31:11.632: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Nov 26 07:31:11.632: INFO: Versions found [{acme.cert-manager.io/v1 v1} {acme.cert-manager.io/v1beta1 v1beta1} {acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Nov 26 07:31:11.632: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Nov 26 07:31:11.632: INFO: Checking APIGroup: ceph.rook.io
Nov 26 07:31:11.632: INFO: PreferredVersion.GroupVersion: ceph.rook.io/v1
Nov 26 07:31:11.632: INFO: Versions found [{ceph.rook.io/v1 v1}]
Nov 26 07:31:11.632: INFO: ceph.rook.io/v1 matches ceph.rook.io/v1
Nov 26 07:31:11.632: INFO: Checking APIGroup: cert-manager.io
Nov 26 07:31:11.633: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Nov 26 07:31:11.633: INFO: Versions found [{cert-manager.io/v1 v1} {cert-manager.io/v1beta1 v1beta1} {cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Nov 26 07:31:11.633: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Nov 26 07:31:11.633: INFO: Checking APIGroup: crd.projectcalico.org
Nov 26 07:31:11.633: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 26 07:31:11.633: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 26 07:31:11.633: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 26 07:31:11.633: INFO: Checking APIGroup: hypercloud.tmaxcloud.com
Nov 26 07:31:11.634: INFO: PreferredVersion.GroupVersion: hypercloud.tmaxcloud.com/v1
Nov 26 07:31:11.634: INFO: Versions found [{hypercloud.tmaxcloud.com/v1 v1}]
Nov 26 07:31:11.634: INFO: hypercloud.tmaxcloud.com/v1 matches hypercloud.tmaxcloud.com/v1
Nov 26 07:31:11.634: INFO: Checking APIGroup: snapshot.storage.k8s.io
Nov 26 07:31:11.635: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Nov 26 07:31:11.635: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Nov 26 07:31:11.635: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Nov 26 07:31:11.635: INFO: Checking APIGroup: tmax.io
Nov 26 07:31:11.635: INFO: PreferredVersion.GroupVersion: tmax.io/v1
Nov 26 07:31:11.635: INFO: Versions found [{tmax.io/v1 v1}]
Nov 26 07:31:11.635: INFO: tmax.io/v1 matches tmax.io/v1
Nov 26 07:31:11.635: INFO: Checking APIGroup: claim.tmax.io
Nov 26 07:31:11.636: INFO: PreferredVersion.GroupVersion: claim.tmax.io/v1alpha1
Nov 26 07:31:11.636: INFO: Versions found [{claim.tmax.io/v1alpha1 v1alpha1}]
Nov 26 07:31:11.636: INFO: claim.tmax.io/v1alpha1 matches claim.tmax.io/v1alpha1
Nov 26 07:31:11.636: INFO: Checking APIGroup: objectbucket.io
Nov 26 07:31:11.637: INFO: PreferredVersion.GroupVersion: objectbucket.io/v1alpha1
Nov 26 07:31:11.637: INFO: Versions found [{objectbucket.io/v1alpha1 v1alpha1}]
Nov 26 07:31:11.637: INFO: objectbucket.io/v1alpha1 matches objectbucket.io/v1alpha1
Nov 26 07:31:11.637: INFO: Checking APIGroup: rook.io
Nov 26 07:31:11.637: INFO: PreferredVersion.GroupVersion: rook.io/v1alpha2
Nov 26 07:31:11.637: INFO: Versions found [{rook.io/v1alpha2 v1alpha2}]
Nov 26 07:31:11.637: INFO: rook.io/v1alpha2 matches rook.io/v1alpha2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:31:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5254" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":46,"skipped":927,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:31:11.642: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-439.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-439.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-439.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 07:31:13.744: INFO: DNS probes using dns-test-a557f247-cddb-45e2-be1e-f8ec6739e73e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-439.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-439.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-439.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 07:31:15.811: INFO: File wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:15.813: INFO: File jessie_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:15.813: INFO: Lookups using dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 failed for: [wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local jessie_udp@dns-test-service-3.dns-439.svc.cluster.local]

Nov 26 07:31:20.817: INFO: File wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:20.820: INFO: File jessie_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:20.820: INFO: Lookups using dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 failed for: [wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local jessie_udp@dns-test-service-3.dns-439.svc.cluster.local]

Nov 26 07:31:25.818: INFO: File wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:25.820: INFO: File jessie_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:25.820: INFO: Lookups using dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 failed for: [wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local jessie_udp@dns-test-service-3.dns-439.svc.cluster.local]

Nov 26 07:31:30.818: INFO: File wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:30.820: INFO: File jessie_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:30.820: INFO: Lookups using dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 failed for: [wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local jessie_udp@dns-test-service-3.dns-439.svc.cluster.local]

Nov 26 07:31:35.816: INFO: File wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:35.819: INFO: File jessie_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:35.819: INFO: Lookups using dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 failed for: [wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local jessie_udp@dns-test-service-3.dns-439.svc.cluster.local]

Nov 26 07:31:40.817: INFO: File wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:40.820: INFO: File jessie_udp@dns-test-service-3.dns-439.svc.cluster.local from pod  dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 26 07:31:40.820: INFO: Lookups using dns-439/dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 failed for: [wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local jessie_udp@dns-test-service-3.dns-439.svc.cluster.local]

Nov 26 07:31:45.819: INFO: DNS probes using dns-test-eb330e6e-8c35-44c2-82be-870f75f9ab94 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-439.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-439.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-439.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-439.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 07:31:47.887: INFO: DNS probes using dns-test-a2c89b87-9ed3-47e5-9aa8-0337f3104842 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:31:47.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-439" for this suite.

• [SLOW TEST:36.313 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":47,"skipped":927,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:31:47.956: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-4583
STEP: creating replication controller nodeport-test in namespace services-4583
I1126 07:31:48.064342      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4583, replica count: 2
Nov 26 07:31:51.115: INFO: Creating new exec pod
I1126 07:31:51.115432      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 07:31:54.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4583 exec execpod4chxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 26 07:31:54.304: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:54.304: INFO: stdout: ""
Nov 26 07:31:55.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4583 exec execpod4chxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 26 07:31:55.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:55.447: INFO: stdout: "nodeport-test-f2pjh"
Nov 26 07:31:55.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4583 exec execpod4chxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.108.30 80'
Nov 26 07:31:55.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.108.30 80\nConnection to 10.96.108.30 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:55.605: INFO: stdout: ""
Nov 26 07:31:56.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4583 exec execpod4chxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.108.30 80'
Nov 26 07:31:56.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.108.30 80\nConnection to 10.96.108.30 80 port [tcp/http] succeeded!\n"
Nov 26 07:31:56.785: INFO: stdout: "nodeport-test-slfd2"
Nov 26 07:31:56.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4583 exec execpod4chxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.8 32411'
Nov 26 07:31:56.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.8 32411\nConnection to 172.21.7.8 32411 port [tcp/*] succeeded!\n"
Nov 26 07:31:56.942: INFO: stdout: "nodeport-test-slfd2"
Nov 26 07:31:56.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4583 exec execpod4chxx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.12 32411'
Nov 26 07:31:57.093: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.12 32411\nConnection to 172.21.7.12 32411 port [tcp/*] succeeded!\n"
Nov 26 07:31:57.093: INFO: stdout: "nodeport-test-slfd2"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:31:57.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4583" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.144 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":48,"skipped":935,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:31:57.100: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:32:25.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3739" for this suite.

• [SLOW TEST:28.098 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":49,"skipped":944,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:32:25.198: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Nov 26 07:32:25.248: INFO: Waiting up to 5m0s for pod "security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49" in namespace "security-context-6356" to be "Succeeded or Failed"
Nov 26 07:32:25.258: INFO: Pod "security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.879184ms
Nov 26 07:32:27.261: INFO: Pod "security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013330175s
STEP: Saw pod success
Nov 26 07:32:27.261: INFO: Pod "security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49" satisfied condition "Succeeded or Failed"
Nov 26 07:32:27.263: INFO: Trying to get logs from node cncf-node2 pod security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49 container test-container: <nil>
STEP: delete the pod
Nov 26 07:32:27.278: INFO: Waiting for pod security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49 to disappear
Nov 26 07:32:27.283: INFO: Pod security-context-1357f51d-7aba-4b47-90ec-e4f835a0ea49 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:32:27.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6356" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":50,"skipped":961,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:32:27.288: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-3dd97ea0-d219-42aa-93e1-5b9b9d0048fb
STEP: Creating secret with name s-test-opt-upd-e160f384-4a3d-4878-8dcc-37e8893feee9
STEP: Creating the pod
Nov 26 07:32:27.372: INFO: The status of Pod pod-secrets-cbf5920e-9da4-4308-a6a2-2f8f1ec61f4b is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:32:29.378: INFO: The status of Pod pod-secrets-cbf5920e-9da4-4308-a6a2-2f8f1ec61f4b is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:32:31.375: INFO: The status of Pod pod-secrets-cbf5920e-9da4-4308-a6a2-2f8f1ec61f4b is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-3dd97ea0-d219-42aa-93e1-5b9b9d0048fb
STEP: Updating secret s-test-opt-upd-e160f384-4a3d-4878-8dcc-37e8893feee9
STEP: Creating secret with name s-test-opt-create-fef8f160-0156-4cb4-9222-bc8198b8df35
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:33:57.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-689" for this suite.

• [SLOW TEST:90.388 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":51,"skipped":969,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:33:57.677: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 26 07:33:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 26 07:34:16.959: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:34:21.606: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:34:41.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7203" for this suite.

• [SLOW TEST:43.553 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":52,"skipped":974,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:34:41.230: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 26 07:34:41.312: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5524  f52d9574-c2a5-4a87-b1e4-befc691f054c 93298 0 2021-11-26 07:34:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2021-11-26T16:34:42.476534451+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:34:42.476534451+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:34:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 07:34:41.312: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5524  f52d9574-c2a5-4a87-b1e4-befc691f054c 93299 0 2021-11-26 07:34:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2021-11-26T16:34:42.476534451+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:34:42.476534451+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:34:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 07:34:41.312: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5524  f52d9574-c2a5-4a87-b1e4-befc691f054c 93300 0 2021-11-26 07:34:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2021-11-26T16:34:42.476534451+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:34:42.476534451+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:34:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 26 07:34:51.340: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5524  f52d9574-c2a5-4a87-b1e4-befc691f054c 93358 0 2021-11-26 07:34:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2021-11-26T16:34:42.476534451+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:34:42.476534451+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:34:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 07:34:51.340: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5524  f52d9574-c2a5-4a87-b1e4-befc691f054c 93359 0 2021-11-26 07:34:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2021-11-26T16:34:42.476534451+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:34:42.476534451+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:34:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 07:34:51.340: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5524  f52d9574-c2a5-4a87-b1e4-befc691f054c 93360 0 2021-11-26 07:34:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[createdTime:2021-11-26T16:34:42.476534451+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T16:34:42.476534451+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 07:34:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:34:51.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5524" for this suite.

• [SLOW TEST:10.123 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":53,"skipped":982,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:34:51.353: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 26 07:34:51.393: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 26 07:34:51.404: INFO: Waiting for terminating namespaces to be deleted...
Nov 26 07:34:51.406: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Nov 26 07:34:51.416: INFO: cert-manager-cainjector-668d9c86df-mjdsd from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:34:51.416: INFO: console-https-secret-create--1-74vfc from console-system started at 2021-11-26 04:05:29 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container create ready: false, restart count 0
Nov 26 07:34:51.416: INFO: hyperauth-765f5c6d78-qhm62 from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 07:34:51.416: INFO: hyperauth-765f5c6d78-tqrpp from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 07:34:51.416: INFO: hypercloud5-api-server-78fdd97c4b-ct44f from hypercloud5-system started at 2021-11-26 04:05:03 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container hypercloud5-api-server ready: true, restart count 0
Nov 26 07:34:51.416: INFO: postgres-d85758466-7zvpj from hypercloud5-system started at 2021-11-26 04:05:04 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container postgres ready: true, restart count 0
Nov 26 07:34:51.416: INFO: calico-node-5c7nc from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 07:34:51.416: INFO: kube-proxy-8lrhw from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 07:34:51.416: INFO: snapshot-controller-bb7675d55-ccb2z from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 07:34:51.416: INFO: csi-cephfsplugin-provisioner-fb98b8789-h5xws from rook-ceph started at 2021-11-26 03:53:16 +0000 UTC (6 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.416: INFO: csi-cephfsplugin-w6qlg from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.416: INFO: csi-rbdplugin-9jp7k from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (3 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.416: INFO: csi-rbdplugin-provisioner-7dffb8c5c-7z6g2 from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.416: INFO: rook-ceph-mon-a-6844dffc48-h4rtd from rook-ceph started at 2021-11-26 03:53:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container mon ready: true, restart count 0
Nov 26 07:34:51.416: INFO: rook-ceph-operator-cdf9dfd9c-c6rpm from rook-ceph started at 2021-11-26 03:52:55 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Nov 26 07:34:51.416: INFO: rook-ceph-osd-0-f55d4f54c-lx2lb from rook-ceph started at 2021-11-26 03:53:54 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container osd ready: true, restart count 0
Nov 26 07:34:51.416: INFO: rook-ceph-osd-prepare-cncf-node2--1-lxkz4 from rook-ceph started at 2021-11-26 04:53:13 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container provision ready: false, restart count 0
Nov 26 07:34:51.416: INFO: rook-ceph-tools-6967fc698d-rtdgw from rook-ceph started at 2021-11-26 03:53:44 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Nov 26 07:34:51.416: INFO: rook-discover-vpgq5 from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 07:34:51.416: INFO: sonobuoy from sonobuoy started at 2021-11-26 07:20:40 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 26 07:34:51.416: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-7b26m from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 07:34:51.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 26 07:34:51.416: INFO: 
Logging pods the apiserver thinks is on node cncf-node3 before test
Nov 26 07:34:51.451: INFO: cert-manager-7c6f78c46d-rlvcs from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.451: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:34:51.451: INFO: cert-manager-webhook-764b556954-7dqpb from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.451: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:34:51.451: INFO: console-58964cf476-bm4wx from console-system started at 2021-11-26 04:05:30 +0000 UTC (2 container statuses recorded)
Nov 26 07:34:51.451: INFO: 	Container console ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container manager ready: true, restart count 0
Nov 26 07:34:51.452: INFO: postgresql-7fd9d45fc-s8kfv from hyperauth started at 2021-11-26 04:03:34 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container postgresql ready: true, restart count 0
Nov 26 07:34:51.452: INFO: calico-node-qwtck from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 07:34:51.452: INFO: kube-proxy-77n76 from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 07:34:51.452: INFO: snapshot-controller-bb7675d55-ld7k5 from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 07:34:51.452: INFO: csi-cephfsplugin-provisioner-fb98b8789-6tqgt from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (6 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.452: INFO: csi-cephfsplugin-qkp2d from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.452: INFO: csi-rbdplugin-8ztcb from rook-ceph started at 2021-11-26 03:53:13 +0000 UTC (3 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.452: INFO: csi-rbdplugin-provisioner-7dffb8c5c-bpsbv from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:34:51.452: INFO: rook-ceph-mds-myfs-a-7745758cff-g6xnp from rook-ceph started at 2021-11-26 03:53:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container mds ready: true, restart count 0
Nov 26 07:34:51.452: INFO: rook-ceph-mds-myfs-b-84457dc995-4r7kk from rook-ceph started at 2021-11-26 03:53:40 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container mds ready: true, restart count 0
Nov 26 07:34:51.452: INFO: rook-ceph-mgr-a-5dc4b44f94-7kd2x from rook-ceph started at 2021-11-26 03:53:20 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container mgr ready: true, restart count 0
Nov 26 07:34:51.452: INFO: rook-ceph-osd-1-558d464b49-kf8wj from rook-ceph started at 2021-11-26 03:53:55 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container osd ready: true, restart count 0
Nov 26 07:34:51.452: INFO: rook-ceph-osd-prepare-cncf-node3--1-dkjrx from rook-ceph started at 2021-11-26 04:53:15 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container provision ready: false, restart count 0
Nov 26 07:34:51.452: INFO: rook-discover-jct2w from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 07:34:51.452: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-gm2tc from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 07:34:51.452: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 07:34:51.452: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16bb085e81ac0515], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:34:52.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6461" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":54,"skipped":998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:34:52.482: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-fbbb3678-541d-430d-a5c4-0e6ee52ef6e4
STEP: Creating a pod to test consume configMaps
Nov 26 07:34:52.548: INFO: Waiting up to 5m0s for pod "pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7" in namespace "configmap-8905" to be "Succeeded or Failed"
Nov 26 07:34:52.555: INFO: Pod "pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.149613ms
Nov 26 07:34:54.558: INFO: Pod "pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009809972s
STEP: Saw pod success
Nov 26 07:34:54.558: INFO: Pod "pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7" satisfied condition "Succeeded or Failed"
Nov 26 07:34:54.560: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:34:54.592: INFO: Waiting for pod pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7 to disappear
Nov 26 07:34:54.597: INFO: Pod pod-configmaps-30089791-c894-4b59-88cc-c08eec3417e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:34:54.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8905" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":1035,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:34:54.602: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-aee6d7ef-fda8-4001-872e-e64dec68c8de
STEP: Creating a pod to test consume configMaps
Nov 26 07:34:54.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6" in namespace "projected-3305" to be "Succeeded or Failed"
Nov 26 07:34:54.706: INFO: Pod "pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6": Phase="Pending", Reason="", readiness=false. Elapsed: 43.415744ms
Nov 26 07:34:56.716: INFO: Pod "pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052490007s
STEP: Saw pod success
Nov 26 07:34:56.716: INFO: Pod "pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6" satisfied condition "Succeeded or Failed"
Nov 26 07:34:56.717: INFO: Trying to get logs from node cncf-node3 pod pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:34:56.752: INFO: Waiting for pod pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6 to disappear
Nov 26 07:34:56.757: INFO: Pod pod-projected-configmaps-ccbeaa8b-2b6b-4eac-9d57-a1904e2006e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:34:56.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3305" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":1052,"failed":0}

------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:34:56.762: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-f8891376-2ae9-4d1d-a84c-6c04b9570c05
STEP: Creating secret with name secret-projected-all-test-volume-81933a16-4c82-4e69-963b-2152e2b04c81
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 26 07:34:56.846: INFO: Waiting up to 5m0s for pod "projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5" in namespace "projected-9338" to be "Succeeded or Failed"
Nov 26 07:34:56.853: INFO: Pod "projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642101ms
Nov 26 07:34:58.870: INFO: Pod "projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024234353s
STEP: Saw pod success
Nov 26 07:34:58.870: INFO: Pod "projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5" satisfied condition "Succeeded or Failed"
Nov 26 07:34:58.878: INFO: Trying to get logs from node cncf-node3 pod projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 26 07:34:58.893: INFO: Waiting for pod projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5 to disappear
Nov 26 07:34:58.899: INFO: Pod projected-volume-0fb57157-fdd4-441c-a09f-a31bd10b82a5 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:34:58.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9338" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":57,"skipped":1052,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:34:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:34:58.982: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0" in namespace "projected-9644" to be "Succeeded or Failed"
Nov 26 07:34:58.987: INFO: Pod "downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.102714ms
Nov 26 07:35:00.990: INFO: Pod "downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008162318s
STEP: Saw pod success
Nov 26 07:35:00.990: INFO: Pod "downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0" satisfied condition "Succeeded or Failed"
Nov 26 07:35:00.991: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0 container client-container: <nil>
STEP: delete the pod
Nov 26 07:35:01.010: INFO: Waiting for pod downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0 to disappear
Nov 26 07:35:01.015: INFO: Pod downwardapi-volume-4594a1e5-8407-4140-91c3-83e26f635ec0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:35:01.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9644" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:35:01.020: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:35:08.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4421" for this suite.

• [SLOW TEST:7.064 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":59,"skipped":1073,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:35:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:35:08.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65" in namespace "projected-9171" to be "Succeeded or Failed"
Nov 26 07:35:08.136: INFO: Pod "downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660196ms
Nov 26 07:35:10.164: INFO: Pod "downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032652218s
STEP: Saw pod success
Nov 26 07:35:10.164: INFO: Pod "downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65" satisfied condition "Succeeded or Failed"
Nov 26 07:35:10.166: INFO: Trying to get logs from node cncf-node2 pod downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65 container client-container: <nil>
STEP: delete the pod
Nov 26 07:35:10.188: INFO: Waiting for pod downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65 to disappear
Nov 26 07:35:10.193: INFO: Pod downwardapi-volume-09a9221c-6a6c-4bc4-a928-f624b1591c65 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:35:10.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9171" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":1080,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:35:10.198: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-8e84b9f2-87d9-49df-87e3-f6b118d683a1
STEP: Creating a pod to test consume secrets
Nov 26 07:35:10.294: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a" in namespace "projected-7648" to be "Succeeded or Failed"
Nov 26 07:35:10.296: INFO: Pod "pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134027ms
Nov 26 07:35:12.300: INFO: Pod "pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005680592s
STEP: Saw pod success
Nov 26 07:35:12.300: INFO: Pod "pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a" satisfied condition "Succeeded or Failed"
Nov 26 07:35:12.301: INFO: Trying to get logs from node cncf-node3 pod pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 26 07:35:12.323: INFO: Waiting for pod pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a to disappear
Nov 26 07:35:12.325: INFO: Pod pod-projected-secrets-df2088eb-98ec-4459-ac4a-28b01ab72b6a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:35:12.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7648" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1133,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:35:12.330: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 26 07:35:12.400: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 26 07:35:12.415: INFO: Waiting for terminating namespaces to be deleted...
Nov 26 07:35:12.417: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Nov 26 07:35:12.432: INFO: cert-manager-cainjector-668d9c86df-mjdsd from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:35:12.432: INFO: console-https-secret-create--1-74vfc from console-system started at 2021-11-26 04:05:29 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container create ready: false, restart count 0
Nov 26 07:35:12.432: INFO: hyperauth-765f5c6d78-qhm62 from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 07:35:12.432: INFO: hyperauth-765f5c6d78-tqrpp from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 07:35:12.432: INFO: hypercloud5-api-server-78fdd97c4b-ct44f from hypercloud5-system started at 2021-11-26 04:05:03 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container hypercloud5-api-server ready: true, restart count 0
Nov 26 07:35:12.432: INFO: postgres-d85758466-7zvpj from hypercloud5-system started at 2021-11-26 04:05:04 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container postgres ready: true, restart count 0
Nov 26 07:35:12.432: INFO: calico-node-5c7nc from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 07:35:12.432: INFO: kube-proxy-8lrhw from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 07:35:12.432: INFO: snapshot-controller-bb7675d55-ccb2z from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 07:35:12.432: INFO: csi-cephfsplugin-provisioner-fb98b8789-h5xws from rook-ceph started at 2021-11-26 03:53:16 +0000 UTC (6 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.432: INFO: csi-cephfsplugin-w6qlg from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.432: INFO: csi-rbdplugin-9jp7k from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (3 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.432: INFO: csi-rbdplugin-provisioner-7dffb8c5c-7z6g2 from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.432: INFO: rook-ceph-mon-a-6844dffc48-h4rtd from rook-ceph started at 2021-11-26 03:53:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container mon ready: true, restart count 0
Nov 26 07:35:12.432: INFO: rook-ceph-operator-cdf9dfd9c-c6rpm from rook-ceph started at 2021-11-26 03:52:55 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Nov 26 07:35:12.432: INFO: rook-ceph-osd-0-f55d4f54c-lx2lb from rook-ceph started at 2021-11-26 03:53:54 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container osd ready: true, restart count 0
Nov 26 07:35:12.432: INFO: rook-ceph-osd-prepare-cncf-node2--1-lxkz4 from rook-ceph started at 2021-11-26 04:53:13 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container provision ready: false, restart count 0
Nov 26 07:35:12.432: INFO: rook-ceph-tools-6967fc698d-rtdgw from rook-ceph started at 2021-11-26 03:53:44 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Nov 26 07:35:12.432: INFO: rook-discover-vpgq5 from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 07:35:12.432: INFO: sonobuoy from sonobuoy started at 2021-11-26 07:20:40 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 26 07:35:12.432: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-7b26m from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 07:35:12.432: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 26 07:35:12.432: INFO: 
Logging pods the apiserver thinks is on node cncf-node3 before test
Nov 26 07:35:12.445: INFO: cert-manager-7c6f78c46d-rlvcs from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:35:12.445: INFO: cert-manager-webhook-764b556954-7dqpb from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:35:12.445: INFO: console-58964cf476-bm4wx from console-system started at 2021-11-26 04:05:30 +0000 UTC (2 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container console ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container manager ready: true, restart count 0
Nov 26 07:35:12.445: INFO: postgresql-7fd9d45fc-s8kfv from hyperauth started at 2021-11-26 04:03:34 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container postgresql ready: true, restart count 0
Nov 26 07:35:12.445: INFO: calico-node-qwtck from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 07:35:12.445: INFO: kube-proxy-77n76 from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 07:35:12.445: INFO: snapshot-controller-bb7675d55-ld7k5 from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 07:35:12.445: INFO: csi-cephfsplugin-provisioner-fb98b8789-6tqgt from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (6 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.445: INFO: csi-cephfsplugin-qkp2d from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.445: INFO: csi-rbdplugin-8ztcb from rook-ceph started at 2021-11-26 03:53:13 +0000 UTC (3 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.445: INFO: csi-rbdplugin-provisioner-7dffb8c5c-bpsbv from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:35:12.445: INFO: rook-ceph-mds-myfs-a-7745758cff-g6xnp from rook-ceph started at 2021-11-26 03:53:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container mds ready: true, restart count 0
Nov 26 07:35:12.445: INFO: rook-ceph-mds-myfs-b-84457dc995-4r7kk from rook-ceph started at 2021-11-26 03:53:40 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container mds ready: true, restart count 0
Nov 26 07:35:12.445: INFO: rook-ceph-mgr-a-5dc4b44f94-7kd2x from rook-ceph started at 2021-11-26 03:53:20 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container mgr ready: true, restart count 0
Nov 26 07:35:12.445: INFO: rook-ceph-osd-1-558d464b49-kf8wj from rook-ceph started at 2021-11-26 03:53:55 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container osd ready: true, restart count 0
Nov 26 07:35:12.445: INFO: rook-ceph-osd-prepare-cncf-node3--1-dkjrx from rook-ceph started at 2021-11-26 04:53:15 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container provision ready: false, restart count 0
Nov 26 07:35:12.445: INFO: rook-discover-jct2w from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 07:35:12.445: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-gm2tc from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 07:35:12.445: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 07:35:12.445: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b3783fab-bac8-4233-98ca-60ffc7d07ab1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.21.7.8 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b3783fab-bac8-4233-98ca-60ffc7d07ab1 off the node cncf-node2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b3783fab-bac8-4233-98ca-60ffc7d07ab1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:40:16.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3698" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.225 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":62,"skipped":1153,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:40:16.555: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 26 07:40:16.619: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 07:40:16.632: INFO: Number of nodes with available pods: 0
Nov 26 07:40:16.632: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 07:40:17.639: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 07:40:17.640: INFO: Number of nodes with available pods: 0
Nov 26 07:40:17.641: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 07:40:18.648: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 07:40:18.649: INFO: Number of nodes with available pods: 2
Nov 26 07:40:18.649: INFO: Number of running nodes: 2, number of available pods: 2
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Nov 26 07:40:18.664: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"95292"},"items":null}

Nov 26 07:40:18.666: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"95292"},"items":[{"metadata":{"name":"daemon-set-2snv7","generateName":"daemon-set-","namespace":"daemonsets-6140","uid":"8fcdf18b-f349-4928-9f39-042835aa9e97","resourceVersion":"95290","creationTimestamp":"2021-11-26T07:40:16Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8ffba8a1178e81985ec686b51518b7f8c4ceb3fcf3ba7fcf01653647d93dd21a","cni.projectcalico.org/podIP":"10.244.35.166/32","cni.projectcalico.org/podIPs":"10.244.35.166/32","createdTime":"2021-11-26T16:40:17.821114438+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2021-11-26T16:40:17.821114438+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"62426fd2-0911-49cd-beeb-e1cb80998e3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-11-26T07:40:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62426fd2-0911-49cd-beeb-e1cb80998e3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-11-26T07:40:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-11-26T07:40:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9m8dw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9m8dw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-node3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-node3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:16Z"}],"hostIP":"172.21.7.12","podIP":"10.244.35.166","podIPs":[{"ip":"10.244.35.166"}],"startTime":"2021-11-26T07:40:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-11-26T07:40:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://2389bd4953a69a91c383602e5149cc8bd2e680e74a5a21f7f9b76ee778132933","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wqpdf","generateName":"daemon-set-","namespace":"daemonsets-6140","uid":"1dd44250-7173-4c3b-bba5-4d09be68a447","resourceVersion":"95284","creationTimestamp":"2021-11-26T07:40:16Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6ea6604aa317a2ee0acd64cab4a5f40cf7567d2b4cc6f1aa0b7207f2bef7fbda","cni.projectcalico.org/podIP":"10.244.89.90/32","cni.projectcalico.org/podIPs":"10.244.89.90/32","createdTime":"2021-11-26T16:40:17.816784605+09:00","creator":"system:serviceaccount:kube-system:daemon-set-controller","updatedTime":"2021-11-26T16:40:17.816784605+09:00","updater":"system:serviceaccount:kube-system:daemon-set-controller"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"62426fd2-0911-49cd-beeb-e1cb80998e3e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-11-26T07:40:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62426fd2-0911-49cd-beeb-e1cb80998e3e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-11-26T07:40:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-11-26T07:40:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xtrrz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xtrrz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cncf-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cncf-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-26T07:40:16Z"}],"hostIP":"172.21.7.8","podIP":"10.244.89.90","podIPs":[{"ip":"10.244.89.90"}],"startTime":"2021-11-26T07:40:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-11-26T07:40:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://0d68ff0cdb40b69d2c2dd2fd3dbcd57c8d6a44cf89dc5cf7f921562e7604ebf4","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:40:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6140" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":63,"skipped":1165,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:40:18.688: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:40:18.731: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Creating first CR 
Nov 26 07:40:21.288: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-26T07:40:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-26T07:40:21Z]] name:name1 resourceVersion:95334 uid:b6de03a9-4162-412b-86de-fcc394608a7a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 26 07:40:31.293: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-26T07:40:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-26T07:40:31Z]] name:name2 resourceVersion:95429 uid:3a9e69be-9784-4d0c-9f6a-37bc4c0684ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 26 07:40:41.299: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-26T07:40:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-26T07:40:41Z]] name:name1 resourceVersion:95480 uid:b6de03a9-4162-412b-86de-fcc394608a7a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 26 07:40:51.303: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-26T07:40:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-26T07:40:51Z]] name:name2 resourceVersion:95530 uid:3a9e69be-9784-4d0c-9f6a-37bc4c0684ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 26 07:41:01.308: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-26T07:40:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-26T07:40:41Z]] name:name1 resourceVersion:95578 uid:b6de03a9-4162-412b-86de-fcc394608a7a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 26 07:41:11.315: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-26T07:40:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-26T07:40:51Z]] name:name2 resourceVersion:95628 uid:3a9e69be-9784-4d0c-9f6a-37bc4c0684ad] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:21.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5461" for this suite.

• [SLOW TEST:63.145 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":64,"skipped":1166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:21.834: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 26 07:41:21.891: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 26 07:41:21.901: INFO: Waiting for terminating namespaces to be deleted...
Nov 26 07:41:21.903: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Nov 26 07:41:21.914: INFO: cert-manager-cainjector-668d9c86df-mjdsd from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:41:21.914: INFO: console-https-secret-create--1-74vfc from console-system started at 2021-11-26 04:05:29 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container create ready: false, restart count 0
Nov 26 07:41:21.914: INFO: hyperauth-765f5c6d78-qhm62 from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 07:41:21.914: INFO: hyperauth-765f5c6d78-tqrpp from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 07:41:21.914: INFO: hypercloud5-api-server-78fdd97c4b-ct44f from hypercloud5-system started at 2021-11-26 04:05:03 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container hypercloud5-api-server ready: true, restart count 0
Nov 26 07:41:21.914: INFO: postgres-d85758466-7zvpj from hypercloud5-system started at 2021-11-26 04:05:04 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container postgres ready: true, restart count 0
Nov 26 07:41:21.914: INFO: calico-node-5c7nc from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 07:41:21.914: INFO: kube-proxy-8lrhw from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 07:41:21.914: INFO: snapshot-controller-bb7675d55-ccb2z from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 07:41:21.914: INFO: csi-cephfsplugin-provisioner-fb98b8789-h5xws from rook-ceph started at 2021-11-26 03:53:16 +0000 UTC (6 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.914: INFO: csi-cephfsplugin-w6qlg from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.914: INFO: csi-rbdplugin-9jp7k from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (3 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.914: INFO: csi-rbdplugin-provisioner-7dffb8c5c-7z6g2 from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.914: INFO: rook-ceph-mon-a-6844dffc48-h4rtd from rook-ceph started at 2021-11-26 03:53:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container mon ready: true, restart count 0
Nov 26 07:41:21.914: INFO: rook-ceph-operator-cdf9dfd9c-c6rpm from rook-ceph started at 2021-11-26 03:52:55 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Nov 26 07:41:21.914: INFO: rook-ceph-osd-0-f55d4f54c-lx2lb from rook-ceph started at 2021-11-26 03:53:54 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container osd ready: true, restart count 0
Nov 26 07:41:21.914: INFO: rook-ceph-osd-prepare-cncf-node2--1-lxkz4 from rook-ceph started at 2021-11-26 04:53:13 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container provision ready: false, restart count 0
Nov 26 07:41:21.914: INFO: rook-ceph-tools-6967fc698d-rtdgw from rook-ceph started at 2021-11-26 03:53:44 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Nov 26 07:41:21.914: INFO: rook-discover-vpgq5 from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 07:41:21.914: INFO: sonobuoy from sonobuoy started at 2021-11-26 07:20:40 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 26 07:41:21.914: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-7b26m from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 07:41:21.914: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 26 07:41:21.914: INFO: 
Logging pods the apiserver thinks is on node cncf-node3 before test
Nov 26 07:41:21.926: INFO: cert-manager-7c6f78c46d-rlvcs from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:41:21.926: INFO: cert-manager-webhook-764b556954-7dqpb from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 07:41:21.926: INFO: console-58964cf476-bm4wx from console-system started at 2021-11-26 04:05:30 +0000 UTC (2 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container console ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container manager ready: true, restart count 0
Nov 26 07:41:21.926: INFO: postgresql-7fd9d45fc-s8kfv from hyperauth started at 2021-11-26 04:03:34 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container postgresql ready: true, restart count 0
Nov 26 07:41:21.926: INFO: calico-node-qwtck from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 07:41:21.926: INFO: kube-proxy-77n76 from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 07:41:21.926: INFO: snapshot-controller-bb7675d55-ld7k5 from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 07:41:21.926: INFO: csi-cephfsplugin-provisioner-fb98b8789-6tqgt from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (6 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.926: INFO: csi-cephfsplugin-qkp2d from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.926: INFO: csi-rbdplugin-8ztcb from rook-ceph started at 2021-11-26 03:53:13 +0000 UTC (3 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.926: INFO: csi-rbdplugin-provisioner-7dffb8c5c-bpsbv from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 07:41:21.926: INFO: rook-ceph-mds-myfs-a-7745758cff-g6xnp from rook-ceph started at 2021-11-26 03:53:39 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container mds ready: true, restart count 0
Nov 26 07:41:21.926: INFO: rook-ceph-mds-myfs-b-84457dc995-4r7kk from rook-ceph started at 2021-11-26 03:53:40 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container mds ready: true, restart count 0
Nov 26 07:41:21.926: INFO: rook-ceph-mgr-a-5dc4b44f94-7kd2x from rook-ceph started at 2021-11-26 03:53:20 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container mgr ready: true, restart count 0
Nov 26 07:41:21.926: INFO: rook-ceph-osd-1-558d464b49-kf8wj from rook-ceph started at 2021-11-26 03:53:55 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container osd ready: true, restart count 0
Nov 26 07:41:21.926: INFO: rook-ceph-osd-prepare-cncf-node3--1-dkjrx from rook-ceph started at 2021-11-26 04:53:15 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container provision ready: false, restart count 0
Nov 26 07:41:21.926: INFO: rook-discover-jct2w from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 07:41:21.926: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-gm2tc from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 07:41:21.926: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 07:41:21.926: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node cncf-node2
STEP: verifying the node has the label node cncf-node3
Nov 26 07:41:22.016: INFO: Pod cert-manager-7c6f78c46d-rlvcs requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod cert-manager-cainjector-668d9c86df-mjdsd requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod cert-manager-webhook-764b556954-7dqpb requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod console-58964cf476-bm4wx requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod hyperauth-765f5c6d78-qhm62 requesting resource cpu=1000m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod hyperauth-765f5c6d78-tqrpp requesting resource cpu=1000m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod postgresql-7fd9d45fc-s8kfv requesting resource cpu=1000m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod hypercloud5-api-server-78fdd97c4b-ct44f requesting resource cpu=300m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod postgres-d85758466-7zvpj requesting resource cpu=300m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod calico-node-5c7nc requesting resource cpu=250m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod calico-node-qwtck requesting resource cpu=250m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod kube-proxy-77n76 requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod kube-proxy-8lrhw requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod snapshot-controller-bb7675d55-ccb2z requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod snapshot-controller-bb7675d55-ld7k5 requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod csi-cephfsplugin-provisioner-fb98b8789-6tqgt requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod csi-cephfsplugin-provisioner-fb98b8789-h5xws requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod csi-cephfsplugin-qkp2d requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod csi-cephfsplugin-w6qlg requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod csi-rbdplugin-8ztcb requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod csi-rbdplugin-9jp7k requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod csi-rbdplugin-provisioner-7dffb8c5c-7z6g2 requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod csi-rbdplugin-provisioner-7dffb8c5c-bpsbv requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod rook-ceph-mds-myfs-a-7745758cff-g6xnp requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod rook-ceph-mds-myfs-b-84457dc995-4r7kk requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod rook-ceph-mgr-a-5dc4b44f94-7kd2x requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod rook-ceph-mon-a-6844dffc48-h4rtd requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod rook-ceph-operator-cdf9dfd9c-c6rpm requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod rook-ceph-osd-0-f55d4f54c-lx2lb requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod rook-ceph-osd-1-558d464b49-kf8wj requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod rook-ceph-tools-6967fc698d-rtdgw requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod rook-discover-jct2w requesting resource cpu=0m on Node cncf-node3
Nov 26 07:41:22.016: INFO: Pod rook-discover-vpgq5 requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod sonobuoy requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-7b26m requesting resource cpu=0m on Node cncf-node2
Nov 26 07:41:22.016: INFO: Pod sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-gm2tc requesting resource cpu=0m on Node cncf-node3
STEP: Starting Pods to consume most of the cluster CPU.
Nov 26 07:41:22.016: INFO: Creating a pod which consumes cpu=9205m on Node cncf-node2
Nov 26 07:41:22.022: INFO: Creating a pod which consumes cpu=10325m on Node cncf-node3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f.16bb08b97071cbbb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5762/filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f to cncf-node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f.16bb08b9dc0c3926], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f.16bb08b9e842b9d0], Reason = [Created], Message = [Created container filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f.16bb08b9eafb76db], Reason = [Started], Message = [Started container filler-pod-9657769d-8555-4954-8a41-39b4a1ead94f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8.16bb08b970e99a01], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5762/filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8 to cncf-node3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8.16bb08b9d1eb96e0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8.16bb08b9de4e3d84], Reason = [Created], Message = [Created container filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8.16bb08b9e0040da3], Reason = [Started], Message = [Started container filler-pod-bf94299c-abdb-4d0f-a7d9-56e1349ba9b8]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16bb08b9eae4f4b7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node cncf-node3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cncf-node2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:25.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5762" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":65,"skipped":1284,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:25.115: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-d616fc64-1e0e-4649-9a8d-2e57f5f33ecd
STEP: Creating a pod to test consume configMaps
Nov 26 07:41:25.185: INFO: Waiting up to 5m0s for pod "pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7" in namespace "configmap-6143" to be "Succeeded or Failed"
Nov 26 07:41:25.190: INFO: Pod "pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.710935ms
Nov 26 07:41:27.193: INFO: Pod "pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00835027s
STEP: Saw pod success
Nov 26 07:41:27.193: INFO: Pod "pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7" satisfied condition "Succeeded or Failed"
Nov 26 07:41:27.195: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:41:27.219: INFO: Waiting for pod pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7 to disappear
Nov 26 07:41:27.222: INFO: Pod pod-configmaps-478ea4bd-6f17-42a0-bcd2-2e18f9f0d5a7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:27.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6143" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1284,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:27.226: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:41:27.787: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:41:30.837: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:30.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9225" for this suite.
STEP: Destroying namespace "webhook-9225-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":67,"skipped":1286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:30.967: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:33.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3267" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1311,"failed":0}
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:41:33.213: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412" in namespace "security-context-test-7581" to be "Succeeded or Failed"
Nov 26 07:41:33.225: INFO: Pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412": Phase="Pending", Reason="", readiness=false. Elapsed: 11.62469ms
Nov 26 07:41:35.229: INFO: Pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015152725s
Nov 26 07:41:37.232: INFO: Pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018243393s
Nov 26 07:41:39.252: INFO: Pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038991777s
Nov 26 07:41:41.257: INFO: Pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.043197092s
Nov 26 07:41:41.257: INFO: Pod "alpine-nnp-false-3641e5c7-19cb-42aa-ba58-a1173df2e412" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:41.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7581" for this suite.

• [SLOW TEST:8.138 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1314,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:41.274: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2601.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2601.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2601.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2601.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2601.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2601.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 07:41:43.398: INFO: DNS probes using dns-2601/dns-test-5c2f1713-989b-4f17-87b6-7257e16ba963 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:43.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2601" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":70,"skipped":1319,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:43.423: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 26 07:41:43.488: INFO: Waiting up to 5m0s for pod "pod-93a408f6-8e1b-45a7-9195-1f0f0018853c" in namespace "emptydir-3870" to be "Succeeded or Failed"
Nov 26 07:41:43.494: INFO: Pod "pod-93a408f6-8e1b-45a7-9195-1f0f0018853c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.258119ms
Nov 26 07:41:45.498: INFO: Pod "pod-93a408f6-8e1b-45a7-9195-1f0f0018853c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010275935s
STEP: Saw pod success
Nov 26 07:41:45.498: INFO: Pod "pod-93a408f6-8e1b-45a7-9195-1f0f0018853c" satisfied condition "Succeeded or Failed"
Nov 26 07:41:45.499: INFO: Trying to get logs from node cncf-node3 pod pod-93a408f6-8e1b-45a7-9195-1f0f0018853c container test-container: <nil>
STEP: delete the pod
Nov 26 07:41:45.518: INFO: Waiting for pod pod-93a408f6-8e1b-45a7-9195-1f0f0018853c to disappear
Nov 26 07:41:45.523: INFO: Pod pod-93a408f6-8e1b-45a7-9195-1f0f0018853c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:45.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3870" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1334,"failed":0}
SS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:45.547: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Nov 26 07:41:45.594: INFO: Waiting up to 5m0s for pod "security-context-599be26e-026a-4c3f-bee7-48db8608d03a" in namespace "security-context-5781" to be "Succeeded or Failed"
Nov 26 07:41:45.603: INFO: Pod "security-context-599be26e-026a-4c3f-bee7-48db8608d03a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.821253ms
Nov 26 07:41:47.606: INFO: Pod "security-context-599be26e-026a-4c3f-bee7-48db8608d03a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012268551s
Nov 26 07:41:49.610: INFO: Pod "security-context-599be26e-026a-4c3f-bee7-48db8608d03a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016196739s
STEP: Saw pod success
Nov 26 07:41:49.610: INFO: Pod "security-context-599be26e-026a-4c3f-bee7-48db8608d03a" satisfied condition "Succeeded or Failed"
Nov 26 07:41:49.612: INFO: Trying to get logs from node cncf-node3 pod security-context-599be26e-026a-4c3f-bee7-48db8608d03a container test-container: <nil>
STEP: delete the pod
Nov 26 07:41:49.638: INFO: Waiting for pod security-context-599be26e-026a-4c3f-bee7-48db8608d03a to disappear
Nov 26 07:41:49.649: INFO: Pod security-context-599be26e-026a-4c3f-bee7-48db8608d03a no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:49.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5781" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":72,"skipped":1336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:49.654: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-616d1fa3-ff10-4834-bb09-8aca8f06f446
STEP: Creating configMap with name cm-test-opt-upd-3422a8f2-6943-454e-aa48-92d3acf333f1
STEP: Creating the pod
Nov 26 07:41:49.758: INFO: The status of Pod pod-projected-configmaps-a1c7d6ea-b400-478c-b9f8-2282a196ca68 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:41:51.763: INFO: The status of Pod pod-projected-configmaps-a1c7d6ea-b400-478c-b9f8-2282a196ca68 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-616d1fa3-ff10-4834-bb09-8aca8f06f446
STEP: Updating configmap cm-test-opt-upd-3422a8f2-6943-454e-aa48-92d3acf333f1
STEP: Creating configMap with name cm-test-opt-create-fbe99d53-1edf-4f7f-8af1-39359f667c86
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:53.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3219" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:53.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-527" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":74,"skipped":1390,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:53.925: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1224110e-46cc-46aa-bfe2-fc72d60e9186
STEP: Creating the pod
Nov 26 07:41:54.033: INFO: The status of Pod pod-projected-configmaps-019edfd7-cde0-4857-bec8-7efe233b3893 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:41:56.035: INFO: The status of Pod pod-projected-configmaps-019edfd7-cde0-4857-bec8-7efe233b3893 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-1224110e-46cc-46aa-bfe2-fc72d60e9186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:41:58.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-231" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1397,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:41:58.065: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Nov 26 07:41:58.116: INFO: Waiting up to 5m0s for pod "client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8" in namespace "containers-765" to be "Succeeded or Failed"
Nov 26 07:41:58.124: INFO: Pod "client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.161973ms
Nov 26 07:42:00.128: INFO: Pod "client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011778519s
STEP: Saw pod success
Nov 26 07:42:00.128: INFO: Pod "client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8" satisfied condition "Succeeded or Failed"
Nov 26 07:42:00.129: INFO: Trying to get logs from node cncf-node3 pod client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:42:00.167: INFO: Waiting for pod client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8 to disappear
Nov 26 07:42:00.176: INFO: Pod client-containers-b6b8843a-95f2-4630-b1df-da441f30fbb8 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:00.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-765" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1405,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:00.180: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:42:00.262: INFO: The status of Pod server-envvars-1fca0ee7-ba6a-4487-96b4-300eae0a3904 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:02.267: INFO: The status of Pod server-envvars-1fca0ee7-ba6a-4487-96b4-300eae0a3904 is Running (Ready = true)
Nov 26 07:42:02.292: INFO: Waiting up to 5m0s for pod "client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367" in namespace "pods-2681" to be "Succeeded or Failed"
Nov 26 07:42:02.302: INFO: Pod "client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367": Phase="Pending", Reason="", readiness=false. Elapsed: 10.410549ms
Nov 26 07:42:04.304: INFO: Pod "client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012471725s
STEP: Saw pod success
Nov 26 07:42:04.304: INFO: Pod "client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367" satisfied condition "Succeeded or Failed"
Nov 26 07:42:04.305: INFO: Trying to get logs from node cncf-node3 pod client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367 container env3cont: <nil>
STEP: delete the pod
Nov 26 07:42:04.350: INFO: Waiting for pod client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367 to disappear
Nov 26 07:42:04.359: INFO: Pod client-envvars-40b3134e-5d04-4885-8f5b-8d249695c367 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:04.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2681" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1409,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:04.364: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:42:04.402: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:05.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6617" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":78,"skipped":1424,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:05.424: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-be135dd8-c26e-45cd-b6b0-ff80133439da
STEP: Creating a pod to test consume configMaps
Nov 26 07:42:05.504: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96" in namespace "projected-7690" to be "Succeeded or Failed"
Nov 26 07:42:05.514: INFO: Pod "pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96": Phase="Pending", Reason="", readiness=false. Elapsed: 9.977898ms
Nov 26 07:42:07.518: INFO: Pod "pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014224954s
STEP: Saw pod success
Nov 26 07:42:07.518: INFO: Pod "pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96" satisfied condition "Succeeded or Failed"
Nov 26 07:42:07.519: INFO: Trying to get logs from node cncf-node3 pod pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:42:07.573: INFO: Waiting for pod pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96 to disappear
Nov 26 07:42:07.578: INFO: Pod pod-projected-configmaps-423968a0-7340-40f2-b8b6-cf750c840e96 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:07.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7690" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1429,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:07.583: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Nov 26 07:42:08.146: INFO: created pod pod-service-account-defaultsa
Nov 26 07:42:08.146: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 26 07:42:08.156: INFO: created pod pod-service-account-mountsa
Nov 26 07:42:08.156: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 26 07:42:08.165: INFO: created pod pod-service-account-nomountsa
Nov 26 07:42:08.165: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 26 07:42:08.177: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 26 07:42:08.177: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 26 07:42:08.188: INFO: created pod pod-service-account-mountsa-mountspec
Nov 26 07:42:08.188: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 26 07:42:08.200: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 26 07:42:08.200: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 26 07:42:08.246: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 26 07:42:08.246: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 26 07:42:08.257: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 26 07:42:08.257: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 26 07:42:08.268: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 26 07:42:08.268: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:08.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4232" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":80,"skipped":1443,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:08.308: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 26 07:42:08.401: INFO: Waiting up to 5m0s for pod "pod-52339109-9080-4285-8332-ae2e62ecedae" in namespace "emptydir-3223" to be "Succeeded or Failed"
Nov 26 07:42:08.417: INFO: Pod "pod-52339109-9080-4285-8332-ae2e62ecedae": Phase="Pending", Reason="", readiness=false. Elapsed: 16.055953ms
Nov 26 07:42:10.422: INFO: Pod "pod-52339109-9080-4285-8332-ae2e62ecedae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021100734s
Nov 26 07:42:12.426: INFO: Pod "pod-52339109-9080-4285-8332-ae2e62ecedae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024942848s
Nov 26 07:42:14.429: INFO: Pod "pod-52339109-9080-4285-8332-ae2e62ecedae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0281666s
STEP: Saw pod success
Nov 26 07:42:14.429: INFO: Pod "pod-52339109-9080-4285-8332-ae2e62ecedae" satisfied condition "Succeeded or Failed"
Nov 26 07:42:14.430: INFO: Trying to get logs from node cncf-node2 pod pod-52339109-9080-4285-8332-ae2e62ecedae container test-container: <nil>
STEP: delete the pod
Nov 26 07:42:14.449: INFO: Waiting for pod pod-52339109-9080-4285-8332-ae2e62ecedae to disappear
Nov 26 07:42:14.454: INFO: Pod pod-52339109-9080-4285-8332-ae2e62ecedae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:14.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3223" for this suite.

• [SLOW TEST:6.153 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1449,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:42:14.527: INFO: The status of Pod busybox-readonly-fsb6b94ee7-d863-40b1-b998-45259132e154 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:16.529: INFO: The status of Pod busybox-readonly-fsb6b94ee7-d863-40b1-b998-45259132e154 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:18.529: INFO: The status of Pod busybox-readonly-fsb6b94ee7-d863-40b1-b998-45259132e154 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:20.532: INFO: The status of Pod busybox-readonly-fsb6b94ee7-d863-40b1-b998-45259132e154 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:20.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8924" for this suite.

• [SLOW TEST:6.082 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1454,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:20.543: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:42:20.699: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"83ee68c8-2fd3-4725-b7fd-5f266f365e9f", Controller:(*bool)(0xc0051bd31a), BlockOwnerDeletion:(*bool)(0xc0051bd31b)}}
Nov 26 07:42:20.705: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6e33c6a6-a767-4e7e-b2e7-8c04328ec6f8", Controller:(*bool)(0xc00566f6aa), BlockOwnerDeletion:(*bool)(0xc00566f6ab)}}
Nov 26 07:42:20.710: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a7d8ebe2-ecaf-47b6-83c9-bb82d0b826b8", Controller:(*bool)(0xc00566f92a), BlockOwnerDeletion:(*bool)(0xc00566f92b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:25.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5682" for this suite.

• [SLOW TEST:5.181 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":83,"skipped":1461,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:25.724: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:42:25.786: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 26 07:42:25.804: INFO: The status of Pod pod-logs-websocket-79619f8d-36e9-4e17-b4dd-493a376cac51 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:27.806: INFO: The status of Pod pod-logs-websocket-79619f8d-36e9-4e17-b4dd-493a376cac51 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:27.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-809" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1467,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:27.820: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8678
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8678
STEP: creating replication controller externalsvc in namespace services-8678
I1126 07:42:27.927984      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8678, replica count: 2
I1126 07:42:30.979576      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 26 07:42:31.002: INFO: Creating new exec pod
Nov 26 07:42:33.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-8678 exec execpod2q988 -- /bin/sh -x -c nslookup clusterip-service.services-8678.svc.cluster.local'
Nov 26 07:42:34.100: INFO: stderr: "+ nslookup clusterip-service.services-8678.svc.cluster.local\n"
Nov 26 07:42:34.100: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8678.svc.cluster.local\tcanonical name = externalsvc.services-8678.svc.cluster.local.\nName:\texternalsvc.services-8678.svc.cluster.local\nAddress: 10.96.15.75\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8678, will wait for the garbage collector to delete the pods
Nov 26 07:42:34.155: INFO: Deleting ReplicationController externalsvc took: 2.718116ms
Nov 26 07:42:34.256: INFO: Terminating ReplicationController externalsvc pods took: 100.967306ms
Nov 26 07:42:36.510: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:36.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8678" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.730 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":85,"skipped":1467,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 26 07:42:36.598: INFO: Waiting up to 5m0s for pod "downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef" in namespace "downward-api-510" to be "Succeeded or Failed"
Nov 26 07:42:36.608: INFO: Pod "downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105103ms
Nov 26 07:42:38.612: INFO: Pod "downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013583112s
STEP: Saw pod success
Nov 26 07:42:38.612: INFO: Pod "downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef" satisfied condition "Succeeded or Failed"
Nov 26 07:42:38.613: INFO: Trying to get logs from node cncf-node3 pod downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef container dapi-container: <nil>
STEP: delete the pod
Nov 26 07:42:38.636: INFO: Waiting for pod downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef to disappear
Nov 26 07:42:38.641: INFO: Pod downward-api-0e4b694c-f319-4f79-8586-04ddec4e66ef no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:38.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-510" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1470,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:38.646: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 26 07:42:38.700: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:40.704: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 26 07:42:40.729: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:42:42.733: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Nov 26 07:42:42.737: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 26 07:42:42.852: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 26 07:42:44.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 26 07:42:44.855: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 26 07:42:46.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 26 07:42:46.856: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:46.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2654" for this suite.

• [SLOW TEST:8.220 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:46.866: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:42:46.947: INFO: Creating deployment "test-recreate-deployment"
Nov 26 07:42:46.958: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 26 07:42:46.975: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 26 07:42:48.996: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 26 07:42:48.997: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 26 07:42:49.001: INFO: Updating deployment test-recreate-deployment
Nov 26 07:42:49.001: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 07:42:49.169: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3394  bccf848f-44fe-47a2-aa27-500bc7656cd6 97287 2 2021-11-26 07:42:46 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[createdTime:2021-11-26T16:42:48.134963544+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T16:42:48.134963544+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0017df7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-11-26 07:42:49 +0000 UTC,LastTransitionTime:2021-11-26 07:42:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-11-26 07:42:49 +0000 UTC,LastTransitionTime:2021-11-26 07:42:46 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 26 07:42:49.171: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-3394  a556836c-9195-46a1-898a-27ac13fd7c8d 97285 1 2021-11-26 07:42:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[createdTime:2021-11-26T16:42:48.134963544+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T16:42:48.134963544+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-recreate-deployment bccf848f-44fe-47a2-aa27-500bc7656cd6 0xc001d1e31e 0xc001d1e31f}] []  [{kube-controller-manager Update apps/v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bccf848f-44fe-47a2-aa27-500bc7656cd6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d1e568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 26 07:42:49.171: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 26 07:42:49.171: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-3394  20c18e6c-82a1-4464-8a49-a01314feb046 97275 2 2021-11-26 07:42:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[createdTime:2021-11-26T16:42:48.134963544+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T16:42:48.134963544+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-recreate-deployment bccf848f-44fe-47a2-aa27-500bc7656cd6 0xc001d1e197 0xc001d1e198}] []  [{kube-controller-manager Update apps/v1 2021-11-26 07:42:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bccf848f-44fe-47a2-aa27-500bc7656cd6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d1e268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 26 07:42:49.172: INFO: Pod "test-recreate-deployment-85d47dcb4-8qcp7" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-8qcp7 test-recreate-deployment-85d47dcb4- deployment-3394  23d60799-5a8e-467e-bd4f-3257927bd916 97286 0 2021-11-26 07:42:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[createdTime:2021-11-26T16:42:50.265206213+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T16:42:50.265206213+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 a556836c-9195-46a1-898a-27ac13fd7c8d 0xc0017dff2e 0xc0017dff2f}] []  [{kube-controller-manager Update v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a556836c-9195-46a1-898a-27ac13fd7c8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 07:42:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4468,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4468,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:42:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:42:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:42:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:42:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 07:42:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:49.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3394" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":88,"skipped":1515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 26 07:42:49.223: INFO: Waiting up to 5m0s for pod "pod-6ba843a1-a598-49ff-a3df-484628890efd" in namespace "emptydir-7355" to be "Succeeded or Failed"
Nov 26 07:42:49.232: INFO: Pod "pod-6ba843a1-a598-49ff-a3df-484628890efd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.151436ms
Nov 26 07:42:51.236: INFO: Pod "pod-6ba843a1-a598-49ff-a3df-484628890efd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013044741s
STEP: Saw pod success
Nov 26 07:42:51.236: INFO: Pod "pod-6ba843a1-a598-49ff-a3df-484628890efd" satisfied condition "Succeeded or Failed"
Nov 26 07:42:51.238: INFO: Trying to get logs from node cncf-node2 pod pod-6ba843a1-a598-49ff-a3df-484628890efd container test-container: <nil>
STEP: delete the pod
Nov 26 07:42:51.272: INFO: Waiting for pod pod-6ba843a1-a598-49ff-a3df-484628890efd to disappear
Nov 26 07:42:51.278: INFO: Pod pod-6ba843a1-a598-49ff-a3df-484628890efd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:42:51.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7355" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1587,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:42:51.282: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1326
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1326
STEP: Waiting until pod test-pod will start running in namespace statefulset-1326
STEP: Creating statefulset with conflicting port in namespace statefulset-1326
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1326
Nov 26 07:42:53.410: INFO: Observed stateful pod in namespace: statefulset-1326, name: ss-0, uid: de636217-029d-4e5a-9feb-66902224f252, status phase: Pending. Waiting for statefulset controller to delete.
Nov 26 07:42:53.461: INFO: Observed stateful pod in namespace: statefulset-1326, name: ss-0, uid: de636217-029d-4e5a-9feb-66902224f252, status phase: Failed. Waiting for statefulset controller to delete.
Nov 26 07:42:53.479: INFO: Observed stateful pod in namespace: statefulset-1326, name: ss-0, uid: de636217-029d-4e5a-9feb-66902224f252, status phase: Failed. Waiting for statefulset controller to delete.
Nov 26 07:42:53.490: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1326
STEP: Removing pod with conflicting port in namespace statefulset-1326
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1326 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 07:42:57.547: INFO: Deleting all statefulset in ns statefulset-1326
Nov 26 07:42:57.548: INFO: Scaling statefulset ss to 0
Nov 26 07:43:07.562: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 07:43:07.564: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:07.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1326" for this suite.

• [SLOW TEST:16.298 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":90,"skipped":1588,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:07.581: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:43:07.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:43:11.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:43:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6856-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:14.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4645" for this suite.
STEP: Destroying namespace "webhook-4645-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.608 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":91,"skipped":1603,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:14.189: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:43:14.250: INFO: Waiting up to 5m0s for pod "downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee" in namespace "downward-api-8715" to be "Succeeded or Failed"
Nov 26 07:43:14.257: INFO: Pod "downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.90187ms
Nov 26 07:43:16.261: INFO: Pod "downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010420256s
STEP: Saw pod success
Nov 26 07:43:16.261: INFO: Pod "downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee" satisfied condition "Succeeded or Failed"
Nov 26 07:43:16.262: INFO: Trying to get logs from node cncf-node2 pod downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee container client-container: <nil>
STEP: delete the pod
Nov 26 07:43:16.304: INFO: Waiting for pod downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee to disappear
Nov 26 07:43:16.310: INFO: Pod downwardapi-volume-649c2e4b-cdf3-4446-aa9b-525720cf58ee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:16.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8715" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:43:16.362: INFO: Got root ca configmap in namespace "svcaccounts-8097"
Nov 26 07:43:16.364: INFO: Deleted root ca configmap in namespace "svcaccounts-8097"
STEP: waiting for a new root ca configmap created
Nov 26 07:43:16.868: INFO: Recreated root ca configmap in namespace "svcaccounts-8097"
Nov 26 07:43:16.871: INFO: Updated root ca configmap in namespace "svcaccounts-8097"
STEP: waiting for the root ca configmap reconciled
Nov 26 07:43:17.375: INFO: Reconciled root ca configmap in namespace "svcaccounts-8097"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:17.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8097" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":93,"skipped":1662,"failed":0}

------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:17.380: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:17.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4548" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":94,"skipped":1662,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:17.458: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:23.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8775" for this suite.
STEP: Destroying namespace "nsdeletetest-7840" for this suite.
Nov 26 07:43:23.716: INFO: Namespace nsdeletetest-7840 was already deleted
STEP: Destroying namespace "nsdeletetest-831" for this suite.

• [SLOW TEST:6.260 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":95,"skipped":1663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:23.719: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Nov 26 07:43:23.788: INFO: Waiting up to 5m0s for pod "var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081" in namespace "var-expansion-843" to be "Succeeded or Failed"
Nov 26 07:43:23.790: INFO: Pod "var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232139ms
Nov 26 07:43:25.793: INFO: Pod "var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005021411s
STEP: Saw pod success
Nov 26 07:43:25.793: INFO: Pod "var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081" satisfied condition "Succeeded or Failed"
Nov 26 07:43:25.795: INFO: Trying to get logs from node cncf-node2 pod var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081 container dapi-container: <nil>
STEP: delete the pod
Nov 26 07:43:25.820: INFO: Waiting for pod var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081 to disappear
Nov 26 07:43:25.825: INFO: Pod var-expansion-f41abcbf-9170-4381-be41-ba5c7cc6d081 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:25.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-843" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1730,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:25.830: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4940.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4940.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 07:43:27.938: INFO: DNS probes using dns-4940/dns-test-89c67421-34fa-41a1-a1f9-693276983c8f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4940" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":97,"skipped":1734,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-e64417c3-f846-426e-9cf4-e9c972895a93
STEP: Creating a pod to test consume secrets
Nov 26 07:43:28.030: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b" in namespace "projected-5475" to be "Succeeded or Failed"
Nov 26 07:43:28.033: INFO: Pod "pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.4691ms
Nov 26 07:43:30.035: INFO: Pod "pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005262962s
STEP: Saw pod success
Nov 26 07:43:30.035: INFO: Pod "pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b" satisfied condition "Succeeded or Failed"
Nov 26 07:43:30.037: INFO: Trying to get logs from node cncf-node3 pod pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 26 07:43:30.081: INFO: Waiting for pod pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b to disappear
Nov 26 07:43:30.084: INFO: Pod pod-projected-secrets-7fe739aa-dbd5-4e05-aa0e-dd16878cc90b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:30.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5475" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1751,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:30.089: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-5a3b8853-9e52-41b7-95a1-932f989843a4
STEP: Creating a pod to test consume secrets
Nov 26 07:43:30.142: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f" in namespace "projected-7688" to be "Succeeded or Failed"
Nov 26 07:43:30.148: INFO: Pod "pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244375ms
Nov 26 07:43:32.150: INFO: Pod "pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007816565s
STEP: Saw pod success
Nov 26 07:43:32.150: INFO: Pod "pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f" satisfied condition "Succeeded or Failed"
Nov 26 07:43:32.152: INFO: Trying to get logs from node cncf-node2 pod pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 26 07:43:32.188: INFO: Waiting for pod pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f to disappear
Nov 26 07:43:32.193: INFO: Pod pod-projected-secrets-9b25f98f-2ded-47fc-83a8-1af14857d54f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:32.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7688" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1759,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:32.199: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:43:32.243: INFO: Waiting up to 5m0s for pod "downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b" in namespace "projected-4116" to be "Succeeded or Failed"
Nov 26 07:43:32.248: INFO: Pod "downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.424275ms
Nov 26 07:43:34.254: INFO: Pod "downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010437817s
STEP: Saw pod success
Nov 26 07:43:34.254: INFO: Pod "downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b" satisfied condition "Succeeded or Failed"
Nov 26 07:43:34.255: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b container client-container: <nil>
STEP: delete the pod
Nov 26 07:43:34.274: INFO: Waiting for pod downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b to disappear
Nov 26 07:43:34.279: INFO: Pod downwardapi-volume-995f6c85-6661-42b7-bac2-64ed70ad127b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4116" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:34.291: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 26 07:43:34.324: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 26 07:43:34.380: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:43:36.382: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:38.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:40.384: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:42.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:44.382: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:46.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:48.384: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:50.385: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:52.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:43:54.393: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 26 07:43:54.399: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 26 07:43:56.416: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Nov 26 07:43:56.416: INFO: Breadth first check of 10.244.89.105 on host 172.21.7.8...
Nov 26 07:43:56.417: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.35.187:9080/dial?request=hostname&protocol=udp&host=10.244.89.105&port=8081&tries=1'] Namespace:pod-network-test-7444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:43:56.417: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:43:56.528: INFO: Waiting for responses: map[]
Nov 26 07:43:56.528: INFO: reached 10.244.89.105 after 0/1 tries
Nov 26 07:43:56.528: INFO: Breadth first check of 10.244.35.185 on host 172.21.7.12...
Nov 26 07:43:56.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.35.187:9080/dial?request=hostname&protocol=udp&host=10.244.35.185&port=8081&tries=1'] Namespace:pod-network-test-7444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:43:56.530: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:43:56.634: INFO: Waiting for responses: map[]
Nov 26 07:43:56.634: INFO: reached 10.244.35.185 after 0/1 tries
Nov 26 07:43:56.634: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:43:56.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7444" for this suite.

• [SLOW TEST:22.348 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1805,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:43:56.639: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:48:56.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2393" for this suite.

• [SLOW TEST:300.069 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":102,"skipped":1825,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:48:56.708: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:48:57.169: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:49:00.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:00.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7113" for this suite.
STEP: Destroying namespace "webhook-7113-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":103,"skipped":1829,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:00.312: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Nov 26 07:49:00.392: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:49:02.403: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:49:04.396: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Nov 26 07:49:04.414: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:49:06.417: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 26 07:49:06.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.418: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:06.527: INFO: Exec stderr: ""
Nov 26 07:49:06.527: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.527: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:06.621: INFO: Exec stderr: ""
Nov 26 07:49:06.621: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.621: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:06.716: INFO: Exec stderr: ""
Nov 26 07:49:06.716: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:06.819: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 26 07:49:06.819: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.819: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:06.913: INFO: Exec stderr: ""
Nov 26 07:49:06.913: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.913: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:06.997: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 26 07:49:06.997: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:06.997: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:07.099: INFO: Exec stderr: ""
Nov 26 07:49:07.099: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:07.099: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:07.200: INFO: Exec stderr: ""
Nov 26 07:49:07.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:07.200: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:07.291: INFO: Exec stderr: ""
Nov 26 07:49:07.291: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-616 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:07.291: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:07.391: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:07.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-616" for this suite.

• [SLOW TEST:7.086 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1840,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:07.399: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Nov 26 07:49:09.474: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-364 PodName:pod-sharedvolume-182758f7-b5ad-4162-842b-a33f2b35f8c6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:49:09.474: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:49:09.552: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:09.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-364" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":105,"skipped":1846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:09.559: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6619
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:49:09.631: INFO: Found 0 stateful pods, waiting for 1
Nov 26 07:49:19.638: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Nov 26 07:49:19.663: INFO: Found 1 stateful pods, waiting for 2
Nov 26 07:49:29.669: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 07:49:29.669: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 07:49:29.687: INFO: Deleting all statefulset in ns statefulset-6619
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:29.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6619" for this suite.

• [SLOW TEST:20.176 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":106,"skipped":1880,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:29.735: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 26 07:49:29.787: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 26 07:49:34.797: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:34.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7333" for this suite.

• [SLOW TEST:5.124 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":107,"skipped":1882,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:34.859: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Nov 26 07:49:34.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-1974 cluster-info'
Nov 26 07:49:34.993: INFO: stderr: ""
Nov 26 07:49:34.993: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:34.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1974" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":108,"skipped":1888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:35.002: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:49:37.099: INFO: Deleting pod "var-expansion-825b4306-4e5d-4f4e-bed9-c89e49897afb" in namespace "var-expansion-8616"
Nov 26 07:49:37.101: INFO: Wait up to 5m0s for pod "var-expansion-825b4306-4e5d-4f4e-bed9-c89e49897afb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:41.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8616" for this suite.

• [SLOW TEST:6.110 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":109,"skipped":1912,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:41.112: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9ff72e2b-e110-418a-9e64-5b7496a4a10e
STEP: Creating a pod to test consume configMaps
Nov 26 07:49:41.186: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4" in namespace "projected-6061" to be "Succeeded or Failed"
Nov 26 07:49:41.191: INFO: Pod "pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481489ms
Nov 26 07:49:43.197: INFO: Pod "pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011631651s
STEP: Saw pod success
Nov 26 07:49:43.197: INFO: Pod "pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4" satisfied condition "Succeeded or Failed"
Nov 26 07:49:43.199: INFO: Trying to get logs from node cncf-node2 pod pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:49:43.243: INFO: Waiting for pod pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4 to disappear
Nov 26 07:49:43.248: INFO: Pod pod-projected-configmaps-d31e8ee3-d1ed-4800-bb20-f8ce019423e4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:49:43.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6061" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":1925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:49:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-b284
STEP: Creating a pod to test atomic-volume-subpath
Nov 26 07:49:43.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b284" in namespace "subpath-9683" to be "Succeeded or Failed"
Nov 26 07:49:43.306: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669089ms
Nov 26 07:49:45.309: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 2.005961444s
Nov 26 07:49:47.317: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 4.01401965s
Nov 26 07:49:49.320: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 6.017113427s
Nov 26 07:49:51.323: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 8.020299858s
Nov 26 07:49:53.326: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 10.023067427s
Nov 26 07:49:55.330: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 12.026661642s
Nov 26 07:49:57.335: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 14.032179868s
Nov 26 07:49:59.338: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 16.035343346s
Nov 26 07:50:01.341: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 18.038019338s
Nov 26 07:50:03.347: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Running", Reason="", readiness=true. Elapsed: 20.043592062s
Nov 26 07:50:05.353: INFO: Pod "pod-subpath-test-configmap-b284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04947328s
STEP: Saw pod success
Nov 26 07:50:05.353: INFO: Pod "pod-subpath-test-configmap-b284" satisfied condition "Succeeded or Failed"
Nov 26 07:50:05.354: INFO: Trying to get logs from node cncf-node2 pod pod-subpath-test-configmap-b284 container test-container-subpath-configmap-b284: <nil>
STEP: delete the pod
Nov 26 07:50:05.373: INFO: Waiting for pod pod-subpath-test-configmap-b284 to disappear
Nov 26 07:50:05.376: INFO: Pod pod-subpath-test-configmap-b284 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-b284
Nov 26 07:50:05.376: INFO: Deleting pod "pod-subpath-test-configmap-b284" in namespace "subpath-9683"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:50:05.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9683" for this suite.

• [SLOW TEST:22.129 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":111,"skipped":1963,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:50:05.383: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Nov 26 07:50:05.418: INFO: created test-event-1
Nov 26 07:50:05.422: INFO: created test-event-2
Nov 26 07:50:05.427: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 26 07:50:05.433: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 26 07:50:05.445: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:50:05.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9655" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":112,"skipped":1976,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:50:05.476: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9929" for this suite.

• [SLOW TEST:60.054 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":1991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:05.530: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:51:05.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb" in namespace "projected-5570" to be "Succeeded or Failed"
Nov 26 07:51:05.627: INFO: Pod "downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.419542ms
Nov 26 07:51:07.637: INFO: Pod "downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01843945s
STEP: Saw pod success
Nov 26 07:51:07.637: INFO: Pod "downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb" satisfied condition "Succeeded or Failed"
Nov 26 07:51:07.638: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb container client-container: <nil>
STEP: delete the pod
Nov 26 07:51:07.680: INFO: Waiting for pod downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb to disappear
Nov 26 07:51:07.686: INFO: Pod downwardapi-volume-3e49c114-6d4d-43c4-b93c-28a40971abfb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:07.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5570" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:07.690: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Nov 26 07:51:07.720: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:33.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3803" for this suite.

• [SLOW TEST:25.495 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":115,"skipped":2041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:33.186: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:51:33.734: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:51:36.765: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:51:36.776: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6308-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:39.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8548" for this suite.
STEP: Destroying namespace "webhook-8548-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.805 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":116,"skipped":2069,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:39.991: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6776
STEP: creating service affinity-clusterip-transition in namespace services-6776
STEP: creating replication controller affinity-clusterip-transition in namespace services-6776
I1126 07:51:40.091785      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6776, replica count: 3
I1126 07:51:43.143118      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 07:51:43.147: INFO: Creating new exec pod
Nov 26 07:51:46.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6776 exec execpod-affinityvbn88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Nov 26 07:51:46.332: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 26 07:51:46.332: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 07:51:46.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6776 exec execpod-affinityvbn88 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.118.103 80'
Nov 26 07:51:46.491: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.118.103 80\nConnection to 10.96.118.103 80 port [tcp/http] succeeded!\n"
Nov 26 07:51:46.491: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 07:51:46.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6776 exec execpod-affinityvbn88 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.118.103:80/ ; done'
Nov 26 07:51:46.704: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n"
Nov 26 07:51:46.704: INFO: stdout: "\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-6rcc2\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-6rcc2\naffinity-clusterip-transition-z7mkj\naffinity-clusterip-transition-6rcc2\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m"
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-6rcc2
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-6rcc2
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-z7mkj
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-6rcc2
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.705: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-6776 exec execpod-affinityvbn88 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.118.103:80/ ; done'
Nov 26 07:51:46.915: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.118.103:80/\n"
Nov 26 07:51:46.915: INFO: stdout: "\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m\naffinity-clusterip-transition-cfd9m"
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Received response from host: affinity-clusterip-transition-cfd9m
Nov 26 07:51:46.915: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6776, will wait for the garbage collector to delete the pods
Nov 26 07:51:47.003: INFO: Deleting ReplicationController affinity-clusterip-transition took: 17.543008ms
Nov 26 07:51:47.104: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.23194ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:49.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6776" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.166 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":117,"skipped":2071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:49.157: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-4291/secret-test-79bfa8ea-3b4e-4af5-bc9f-23a0278622ee
STEP: Creating a pod to test consume secrets
Nov 26 07:51:49.202: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a" in namespace "secrets-4291" to be "Succeeded or Failed"
Nov 26 07:51:49.207: INFO: Pod "pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.307706ms
Nov 26 07:51:51.210: INFO: Pod "pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00812717s
STEP: Saw pod success
Nov 26 07:51:51.210: INFO: Pod "pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a" satisfied condition "Succeeded or Failed"
Nov 26 07:51:51.211: INFO: Trying to get logs from node cncf-node2 pod pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a container env-test: <nil>
STEP: delete the pod
Nov 26 07:51:51.259: INFO: Waiting for pod pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a to disappear
Nov 26 07:51:51.261: INFO: Pod pod-configmaps-cf980f20-868f-4838-a9a2-727e5c201d0a no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4291" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:51:51.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef" in namespace "downward-api-8252" to be "Succeeded or Failed"
Nov 26 07:51:51.310: INFO: Pod "downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.760547ms
Nov 26 07:51:53.313: INFO: Pod "downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008012634s
Nov 26 07:51:55.319: INFO: Pod "downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014050703s
STEP: Saw pod success
Nov 26 07:51:55.319: INFO: Pod "downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef" satisfied condition "Succeeded or Failed"
Nov 26 07:51:55.321: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef container client-container: <nil>
STEP: delete the pod
Nov 26 07:51:55.343: INFO: Waiting for pod downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef to disappear
Nov 26 07:51:55.348: INFO: Pod downwardapi-volume-286e384e-b21f-496b-a0f9-e996035389ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:51:55.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8252" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2146,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:51:55.353: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:51:55.398: INFO: Creating ReplicaSet my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912
Nov 26 07:51:55.405: INFO: Pod name my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912: Found 0 pods out of 1
Nov 26 07:52:00.412: INFO: Pod name my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912: Found 1 pods out of 1
Nov 26 07:52:00.412: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912" is running
Nov 26 07:52:00.414: INFO: Pod "my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912-2nb5t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 07:51:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 07:51:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 07:51:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 07:51:55 +0000 UTC Reason: Message:}])
Nov 26 07:52:00.414: INFO: Trying to dial the pod
Nov 26 07:52:05.424: INFO: Controller my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912: Got expected result from replica 1 [my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912-2nb5t]: "my-hostname-basic-f71f1123-6254-4b76-9f29-8d56e7936912-2nb5t", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:52:05.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1535" for this suite.

• [SLOW TEST:10.079 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":120,"skipped":2231,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:52:05.432: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 26 07:52:05.496: INFO: Waiting up to 5m0s for pod "pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb" in namespace "emptydir-9074" to be "Succeeded or Failed"
Nov 26 07:52:05.505: INFO: Pod "pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.561399ms
Nov 26 07:52:07.508: INFO: Pod "pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012437442s
STEP: Saw pod success
Nov 26 07:52:07.508: INFO: Pod "pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb" satisfied condition "Succeeded or Failed"
Nov 26 07:52:07.509: INFO: Trying to get logs from node cncf-node2 pod pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb container test-container: <nil>
STEP: delete the pod
Nov 26 07:52:07.532: INFO: Waiting for pod pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb to disappear
Nov 26 07:52:07.538: INFO: Pod pod-a6f05d08-9ded-4715-85a0-2aa7b90ac1bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:52:07.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9074" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2240,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:52:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-0fe6d13e-85e1-467c-a548-1c7e84900355
STEP: Creating a pod to test consume secrets
Nov 26 07:52:07.629: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9" in namespace "projected-7205" to be "Succeeded or Failed"
Nov 26 07:52:07.642: INFO: Pod "pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.984198ms
Nov 26 07:52:09.649: INFO: Pod "pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019382148s
Nov 26 07:52:11.652: INFO: Pod "pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022344093s
STEP: Saw pod success
Nov 26 07:52:11.652: INFO: Pod "pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9" satisfied condition "Succeeded or Failed"
Nov 26 07:52:11.653: INFO: Trying to get logs from node cncf-node2 pod pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 26 07:52:11.676: INFO: Waiting for pod pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9 to disappear
Nov 26 07:52:11.681: INFO: Pod pod-projected-secrets-a3b5eb79-1ce2-4237-ab16-8f02b00939f9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:52:11.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7205" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2254,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:52:11.690: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:52:11.752: INFO: Creating simple deployment test-new-deployment
Nov 26 07:52:11.786: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 07:52:13.878: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-392  c5eaf307-4f4b-482c-a6b3-df80be717975 101828 3 2021-11-26 07:52:11 +0000 UTC <nil> <nil> map[name:httpd] map[createdTime:2021-11-26T16:52:12.945192188+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T16:52:12.945192188+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2021-11-26 07:52:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ac7238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-26 07:52:13 +0000 UTC,LastTransitionTime:2021-11-26 07:52:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-11-26 07:52:13 +0000 UTC,LastTransitionTime:2021-11-26 07:52:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 26 07:52:13.890: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-392  4e1727c9-b068-4782-bf12-1a84c0c8677b 101837 3 2021-11-26 07:52:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T16:52:12.945192188+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T16:52:12.945192188+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-new-deployment c5eaf307-4f4b-482c-a6b3-df80be717975 0xc005d7ebe7 0xc005d7ebe8}] []  [{kube-controller-manager Update apps/v1 2021-11-26 07:52:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5eaf307-4f4b-482c-a6b3-df80be717975\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d7ecd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 26 07:52:13.929: INFO: Pod "test-new-deployment-847dcfb7fb-96txc" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-96txc test-new-deployment-847dcfb7fb- deployment-392  c375be7d-d45e-4999-909b-2889a4467ce3 101845 0 2021-11-26 07:52:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T16:52:15.070891181+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T16:52:15.070891181+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4e1727c9-b068-4782-bf12-1a84c0c8677b 0xc005d7f2a7 0xc005d7f2a8}] []  [{kube-controller-manager Update v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e1727c9-b068-4782-bf12-1a84c0c8677b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jt8bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jt8bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 07:52:13.929: INFO: Pod "test-new-deployment-847dcfb7fb-cm7xw" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-cm7xw test-new-deployment-847dcfb7fb- deployment-392  5d6ffbb3-b934-4269-a1df-61905060032a 101842 0 2021-11-26 07:52:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T16:52:15.059614031+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T16:52:15.059614031+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4e1727c9-b068-4782-bf12-1a84c0c8677b 0xc005d7f46e 0xc005d7f46f}] []  [{kube-controller-manager Update v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e1727c9-b068-4782-bf12-1a84c0c8677b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4nk8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4nk8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 07:52:13.929: INFO: Pod "test-new-deployment-847dcfb7fb-fn8sn" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-fn8sn test-new-deployment-847dcfb7fb- deployment-392  26ce86a5-5327-4aa8-ba1a-bfc1c36f6e7b 101839 0 2021-11-26 07:52:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T16:52:15.012613179+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T16:52:15.012613179+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4e1727c9-b068-4782-bf12-1a84c0c8677b 0xc005d7f5fe 0xc005d7f5ff}] []  [{kube-controller-manager Update v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e1727c9-b068-4782-bf12-1a84c0c8677b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnwj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnwj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 07:52:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 07:52:13.929: INFO: Pod "test-new-deployment-847dcfb7fb-mr8qn" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-mr8qn test-new-deployment-847dcfb7fb- deployment-392  9544b80f-b228-4be3-a58a-d6b5f8a033af 101820 0 2021-11-26 07:52:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:0810a1f28437728de1cc99dff879e2e0c7f9356122471a2511d4296b430fe23f cni.projectcalico.org/podIP:10.244.89.122/32 cni.projectcalico.org/podIPs:10.244.89.122/32 createdTime:2021-11-26T16:52:12.97744779+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T16:52:12.97744779+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4e1727c9-b068-4782-bf12-1a84c0c8677b 0xc005d7f877 0xc005d7f878}] []  [{kube-controller-manager Update v1 2021-11-26 07:52:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e1727c9-b068-4782-bf12-1a84c0c8677b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 07:52:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 07:52:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52p7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52p7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:52:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.8,PodIP:10.244.89.122,StartTime:2021-11-26 07:52:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 07:52:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://edaf75be9d8859f696a7b9c6e29b5bb3c1371154245ae521c30d30cb1a86723f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:52:13.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-392" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":123,"skipped":2267,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:52:14.009: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Nov 26 07:52:24.120: INFO: The status of Pod kube-controller-manager-cncf-node1 is Running (Ready = true)
Nov 26 07:52:24.352: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:52:24.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1281" for this suite.

• [SLOW TEST:10.349 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":124,"skipped":2267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:52:24.359: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 26 07:52:26.419: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4257 PodName:var-expansion-df18825e-d16e-493e-86d7-47d5d31a0a03 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:52:26.419: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: test for file in mounted path
Nov 26 07:52:26.530: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4257 PodName:var-expansion-df18825e-d16e-493e-86d7-47d5d31a0a03 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:52:26.530: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: updating the annotation value
Nov 26 07:52:27.126: INFO: Successfully updated pod "var-expansion-df18825e-d16e-493e-86d7-47d5d31a0a03"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 26 07:52:27.135: INFO: Deleting pod "var-expansion-df18825e-d16e-493e-86d7-47d5d31a0a03" in namespace "var-expansion-4257"
Nov 26 07:52:27.138: INFO: Wait up to 5m0s for pod "var-expansion-df18825e-d16e-493e-86d7-47d5d31a0a03" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:53:01.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4257" for this suite.

• [SLOW TEST:36.801 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":125,"skipped":2314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:53:01.160: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9824
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-9824
Nov 26 07:53:01.245: INFO: Found 0 stateful pods, waiting for 1
Nov 26 07:53:11.250: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 07:53:11.286: INFO: Deleting all statefulset in ns statefulset-9824
Nov 26 07:53:11.345: INFO: Scaling statefulset ss to 0
Nov 26 07:53:21.363: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 07:53:21.364: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:53:21.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9824" for this suite.

• [SLOW TEST:20.221 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":126,"skipped":2360,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:53:21.381: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:53:21.426: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 26 07:53:21.435: INFO: The status of Pod pod-exec-websocket-d2c4d6cd-e7a4-47a6-a112-6e7b3628ddfd is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:53:23.437: INFO: The status of Pod pod-exec-websocket-d2c4d6cd-e7a4-47a6-a112-6e7b3628ddfd is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:53:23.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6042" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2380,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:53:23.551: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8789
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 26 07:53:23.609: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 26 07:53:23.641: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:53:25.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:27.659: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:29.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:31.645: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:33.645: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:35.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:37.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:39.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:41.644: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:43.647: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 07:53:45.645: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 26 07:53:45.648: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 26 07:53:47.674: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Nov 26 07:53:47.674: INFO: Going to poll 10.244.89.125 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Nov 26 07:53:47.675: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.89.125:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8789 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:53:47.675: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:53:47.785: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 26 07:53:47.785: INFO: Going to poll 10.244.35.146 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Nov 26 07:53:47.787: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.35.146:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8789 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 07:53:47.787: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 07:53:47.906: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:53:47.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8789" for this suite.

• [SLOW TEST:24.361 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:53:47.912: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-aec9151d-f1fc-4fe1-95f6-b31f99a97696
STEP: Creating a pod to test consume configMaps
Nov 26 07:53:47.989: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d" in namespace "configmap-7035" to be "Succeeded or Failed"
Nov 26 07:53:47.997: INFO: Pod "pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.284287ms
Nov 26 07:53:50.001: INFO: Pod "pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012087904s
Nov 26 07:53:52.004: INFO: Pod "pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015405253s
STEP: Saw pod success
Nov 26 07:53:52.004: INFO: Pod "pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d" satisfied condition "Succeeded or Failed"
Nov 26 07:53:52.006: INFO: Trying to get logs from node cncf-node2 pod pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d container agnhost-container: <nil>
STEP: delete the pod
Nov 26 07:53:52.037: INFO: Waiting for pod pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d to disappear
Nov 26 07:53:52.043: INFO: Pod pod-configmaps-ec16f0e0-ca42-43fe-916e-2b099c2d264d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:53:52.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7035" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2421,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:53:52.048: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:53:58.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4189" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":130,"skipped":2425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:53:58.169: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-6e57851d-3913-40e9-aca2-39ac0295bef3
STEP: Creating configMap with name cm-test-opt-upd-48b6018a-2460-4a17-8c34-d148aa3890ae
STEP: Creating the pod
Nov 26 07:53:58.243: INFO: The status of Pod pod-configmaps-28749d4c-7f12-4297-8eaa-5a16a9e7f244 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:54:00.248: INFO: The status of Pod pod-configmaps-28749d4c-7f12-4297-8eaa-5a16a9e7f244 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:54:02.248: INFO: The status of Pod pod-configmaps-28749d4c-7f12-4297-8eaa-5a16a9e7f244 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-6e57851d-3913-40e9-aca2-39ac0295bef3
STEP: Updating configmap cm-test-opt-upd-48b6018a-2460-4a17-8c34-d148aa3890ae
STEP: Creating configMap with name cm-test-opt-create-49e2f9f8-51ab-47bb-ac23-5cf05bd599a5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:20.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5397" for this suite.

• [SLOW TEST:82.441 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:20.610: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Nov 26 07:55:20.692: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:55:22.696: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 26 07:55:23.722: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:24.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2144" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":132,"skipped":2488,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:24.741: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:55:24.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106" in namespace "downward-api-4241" to be "Succeeded or Failed"
Nov 26 07:55:24.819: INFO: Pod "downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106": Phase="Pending", Reason="", readiness=false. Elapsed: 8.392307ms
Nov 26 07:55:26.822: INFO: Pod "downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011571575s
STEP: Saw pod success
Nov 26 07:55:26.822: INFO: Pod "downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106" satisfied condition "Succeeded or Failed"
Nov 26 07:55:26.823: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106 container client-container: <nil>
STEP: delete the pod
Nov 26 07:55:26.854: INFO: Waiting for pod downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106 to disappear
Nov 26 07:55:26.859: INFO: Pod downwardapi-volume-f39f05fe-24b9-4e36-aea1-2eca75314106 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:26.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4241" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:26.864: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:55:27.459: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:55:30.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 26 07:55:30.509: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:30.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8216" for this suite.
STEP: Destroying namespace "webhook-8216-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":134,"skipped":2547,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:30.638: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 07:55:31.145: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 07:55:34.189: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
Nov 26 07:55:44.371: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4900" for this suite.
STEP: Destroying namespace "webhook-4900-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.052 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":135,"skipped":2552,"failed":0}
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:44.690: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:44.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3259" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":136,"skipped":2553,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:44.773: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 26 07:55:44.819: INFO: Waiting up to 5m0s for pod "pod-ce8fc446-98d8-48d6-8886-a08120a793ff" in namespace "emptydir-2785" to be "Succeeded or Failed"
Nov 26 07:55:44.826: INFO: Pod "pod-ce8fc446-98d8-48d6-8886-a08120a793ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.138493ms
Nov 26 07:55:46.829: INFO: Pod "pod-ce8fc446-98d8-48d6-8886-a08120a793ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010232873s
STEP: Saw pod success
Nov 26 07:55:46.829: INFO: Pod "pod-ce8fc446-98d8-48d6-8886-a08120a793ff" satisfied condition "Succeeded or Failed"
Nov 26 07:55:46.831: INFO: Trying to get logs from node cncf-node3 pod pod-ce8fc446-98d8-48d6-8886-a08120a793ff container test-container: <nil>
STEP: delete the pod
Nov 26 07:55:46.874: INFO: Waiting for pod pod-ce8fc446-98d8-48d6-8886-a08120a793ff to disappear
Nov 26 07:55:46.879: INFO: Pod pod-ce8fc446-98d8-48d6-8886-a08120a793ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:55:46.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2785" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:55:46.884: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:56:03.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9013" for this suite.

• [SLOW TEST:16.322 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":138,"skipped":2642,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:56:03.206: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:56:03.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8390" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":139,"skipped":2663,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:56:03.279: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 26 07:56:03.366: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 26 07:57:03.415: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:57:03.429: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:57:03.505: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Nov 26 07:57:03.507: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:57:03.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2851" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:57:03.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2261" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.328 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":140,"skipped":2668,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:57:03.608: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-96ce04ff-e574-4f4d-8086-12a759d17cc6
STEP: Creating secret with name s-test-opt-upd-6322d3cd-4556-41bd-b5af-f9f46245e574
STEP: Creating the pod
Nov 26 07:57:03.716: INFO: The status of Pod pod-projected-secrets-cfa43ce9-57f7-482a-96fe-2da670e2bb01 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:57:05.719: INFO: The status of Pod pod-projected-secrets-cfa43ce9-57f7-482a-96fe-2da670e2bb01 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 07:57:07.720: INFO: The status of Pod pod-projected-secrets-cfa43ce9-57f7-482a-96fe-2da670e2bb01 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-96ce04ff-e574-4f4d-8086-12a759d17cc6
STEP: Updating secret s-test-opt-upd-6322d3cd-4556-41bd-b5af-f9f46245e574
STEP: Creating secret with name s-test-opt-create-895564e9-251c-4065-8c49-fdee4fb95b24
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:58:26.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8419" for this suite.

• [SLOW TEST:82.428 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2685,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:58:26.036: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 07:58:26.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de" in namespace "downward-api-5706" to be "Succeeded or Failed"
Nov 26 07:58:26.125: INFO: Pod "downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de": Phase="Pending", Reason="", readiness=false. Elapsed: 12.167994ms
Nov 26 07:58:28.127: INFO: Pod "downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014177638s
STEP: Saw pod success
Nov 26 07:58:28.127: INFO: Pod "downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de" satisfied condition "Succeeded or Failed"
Nov 26 07:58:28.128: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de container client-container: <nil>
STEP: delete the pod
Nov 26 07:58:28.165: INFO: Waiting for pod downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de to disappear
Nov 26 07:58:28.176: INFO: Pod downwardapi-volume-2f3151d0-fe22-463e-847e-4c25bbb0a7de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:58:28.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5706" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2687,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:58:28.181: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 26 07:58:28.262: INFO: Waiting up to 5m0s for pod "downward-api-e2992186-5a23-4302-b654-a887b61b7ac0" in namespace "downward-api-7535" to be "Succeeded or Failed"
Nov 26 07:58:28.267: INFO: Pod "downward-api-e2992186-5a23-4302-b654-a887b61b7ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5255ms
Nov 26 07:58:30.271: INFO: Pod "downward-api-e2992186-5a23-4302-b654-a887b61b7ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009512509s
Nov 26 07:58:32.274: INFO: Pod "downward-api-e2992186-5a23-4302-b654-a887b61b7ac0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01209088s
STEP: Saw pod success
Nov 26 07:58:32.274: INFO: Pod "downward-api-e2992186-5a23-4302-b654-a887b61b7ac0" satisfied condition "Succeeded or Failed"
Nov 26 07:58:32.275: INFO: Trying to get logs from node cncf-node3 pod downward-api-e2992186-5a23-4302-b654-a887b61b7ac0 container dapi-container: <nil>
STEP: delete the pod
Nov 26 07:58:32.297: INFO: Waiting for pod downward-api-e2992186-5a23-4302-b654-a887b61b7ac0 to disappear
Nov 26 07:58:32.302: INFO: Pod downward-api-e2992186-5a23-4302-b654-a887b61b7ac0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:58:32.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7535" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2691,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:58:32.307: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8200
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Nov 26 07:58:32.383: INFO: Found 0 stateful pods, waiting for 3
Nov 26 07:58:42.387: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 07:58:42.387: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 07:58:42.387: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Nov 26 07:58:42.433: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 26 07:58:52.473: INFO: Updating stateful set ss2
Nov 26 07:58:52.495: INFO: Waiting for Pod statefulset-8200/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Nov 26 07:59:02.585: INFO: Found 2 stateful pods, waiting for 3
Nov 26 07:59:12.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 07:59:12.589: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 07:59:12.589: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 26 07:59:12.610: INFO: Updating stateful set ss2
Nov 26 07:59:12.621: INFO: Waiting for Pod statefulset-8200/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Nov 26 07:59:22.642: INFO: Updating stateful set ss2
Nov 26 07:59:22.691: INFO: Waiting for StatefulSet statefulset-8200/ss2 to complete update
Nov 26 07:59:22.691: INFO: Waiting for Pod statefulset-8200/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 07:59:32.697: INFO: Deleting all statefulset in ns statefulset-8200
Nov 26 07:59:32.699: INFO: Scaling statefulset ss2 to 0
Nov 26 07:59:42.718: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 07:59:42.720: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:59:42.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8200" for this suite.

• [SLOW TEST:70.434 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":144,"skipped":2706,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:59:42.740: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 26 07:59:42.786: INFO: Waiting up to 5m0s for pod "pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5" in namespace "emptydir-1216" to be "Succeeded or Failed"
Nov 26 07:59:42.827: INFO: Pod "pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.999585ms
Nov 26 07:59:44.832: INFO: Pod "pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045372827s
STEP: Saw pod success
Nov 26 07:59:44.832: INFO: Pod "pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5" satisfied condition "Succeeded or Failed"
Nov 26 07:59:44.833: INFO: Trying to get logs from node cncf-node2 pod pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5 container test-container: <nil>
STEP: delete the pod
Nov 26 07:59:44.896: INFO: Waiting for pod pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5 to disappear
Nov 26 07:59:44.903: INFO: Pod pod-0110f0f4-80dc-4ae5-bdae-9af82b7894c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:59:44.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1216" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2706,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:59:44.908: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 26 07:59:44.995: INFO: Waiting up to 5m0s for pod "pod-ce85b45c-701c-48e8-a486-9137f33039be" in namespace "emptydir-6486" to be "Succeeded or Failed"
Nov 26 07:59:45.000: INFO: Pod "pod-ce85b45c-701c-48e8-a486-9137f33039be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511163ms
Nov 26 07:59:47.010: INFO: Pod "pod-ce85b45c-701c-48e8-a486-9137f33039be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015613325s
STEP: Saw pod success
Nov 26 07:59:47.010: INFO: Pod "pod-ce85b45c-701c-48e8-a486-9137f33039be" satisfied condition "Succeeded or Failed"
Nov 26 07:59:47.022: INFO: Trying to get logs from node cncf-node3 pod pod-ce85b45c-701c-48e8-a486-9137f33039be container test-container: <nil>
STEP: delete the pod
Nov 26 07:59:47.064: INFO: Waiting for pod pod-ce85b45c-701c-48e8-a486-9137f33039be to disappear
Nov 26 07:59:47.069: INFO: Pod pod-ce85b45c-701c-48e8-a486-9137f33039be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 07:59:47.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6486" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2724,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 07:59:47.074: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 07:59:47.144: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 26 07:59:52.146: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 26 07:59:52.146: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 26 07:59:54.149: INFO: Creating deployment "test-rollover-deployment"
Nov 26 07:59:54.156: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 26 07:59:56.193: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 26 07:59:56.205: INFO: Ensure that both replica sets have 1 created replica
Nov 26 07:59:56.208: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 26 07:59:56.212: INFO: Updating deployment test-rollover-deployment
Nov 26 07:59:56.212: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 26 07:59:58.233: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 26 07:59:58.236: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 26 07:59:58.239: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 07:59:58.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:00.244: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:00.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:02.243: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:02.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:04.244: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:04.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:06.245: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:06.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:08.245: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:08.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:10.269: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:10.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:12.243: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:12.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:14.244: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:14.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:16.244: INFO: all replica sets need to contain the pod-template-hash label
Nov 26 08:00:16.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510397, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510394, loc:(*time.Location)(0xa09bc80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 26 08:00:18.244: INFO: 
Nov 26 08:00:18.244: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 08:00:18.249: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6284  5963c3f9-bc24-42c5-83f9-12155851cc7a 105796 2 2021-11-26 07:59:54 +0000 UTC <nil> <nil> map[name:rollover-pod] map[createdTime:2021-11-26T16:59:55.346982216+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T16:59:55.346982216+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 2021-11-26 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:00:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055aa548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-26 07:59:54 +0000 UTC,LastTransitionTime:2021-11-26 07:59:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-11-26 08:00:17 +0000 UTC,LastTransitionTime:2021-11-26 07:59:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 26 08:00:18.251: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-6284  0f932613-c862-4f81-8123-1fc41b00a62f 105786 2 2021-11-26 07:59:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[createdTime:2021-11-26T16:59:55.346982216+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T16:59:55.346982216+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 5963c3f9-bc24-42c5-83f9-12155851cc7a 0xc0055aadfe 0xc0055aadff}] []  [{kube-controller-manager Update apps/v1 2021-11-26 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5963c3f9-bc24-42c5-83f9-12155851cc7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:00:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055aaea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:00:18.251: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 26 08:00:18.251: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6284  9ff699ef-51e5-491c-8cbb-85a04002e813 105795 2 2021-11-26 07:59:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[createdTime:2021-11-26T16:59:48.329623563+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 updatedTime:2021-11-26T16:59:48.329623563+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 5963c3f9-bc24-42c5-83f9-12155851cc7a 0xc0055aa8f7 0xc0055aa8f8}] []  [{e2e.test Update apps/v1 2021-11-26 07:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:00:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5963c3f9-bc24-42c5-83f9-12155851cc7a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:00:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0055aac28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:00:18.251: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6284  7b30d10b-ecde-45eb-99dc-b29f662ee23c 105660 2 2021-11-26 07:59:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[createdTime:2021-11-26T16:59:55.346982216+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T16:59:55.346982216+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rollover-deployment 5963c3f9-bc24-42c5-83f9-12155851cc7a 0xc0055aacc7 0xc0055aacc8}] []  [{kube-controller-manager Update apps/v1 2021-11-26 07:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5963c3f9-bc24-42c5-83f9-12155851cc7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 07:59:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055aad78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:00:18.253: INFO: Pod "test-rollover-deployment-98c5f4599-4mgq7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-4mgq7 test-rollover-deployment-98c5f4599- deployment-6284  a0cc7ef3-d51f-4e5a-908d-1983d9e602eb 105683 0 2021-11-26 07:59:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:7e15f15ad407da8986d0590966a745e216ceaf96c32c300416a2105ffa3e603d cni.projectcalico.org/podIP:10.244.89.91/32 cni.projectcalico.org/podIPs:10.244.89.91/32 createdTime:2021-11-26T16:59:57.471326422+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T16:59:57.471326422+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 0f932613-c862-4f81-8123-1fc41b00a62f 0xc0055ab40e 0xc0055ab40f}] []  [{calico Update v1 2021-11-26 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2021-11-26 07:59:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f932613-c862-4f81-8123-1fc41b00a62f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 07:59:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.89.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w78st,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w78st,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:59:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:59:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:59:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 07:59:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.8,PodIP:10.244.89.91,StartTime:2021-11-26 07:59:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 07:59:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://b882969b83ed9def4bd30311065ee1c909ba53495f23d695e09dc62b75061052,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.89.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:00:18.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6284" for this suite.

• [SLOW TEST:31.184 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":147,"skipped":2740,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:00:18.258: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:00:29.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3771" for this suite.

• [SLOW TEST:11.188 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":148,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:00:29.446: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-6501730a-b957-4201-ba3b-0e4b131a4aa3
STEP: Creating a pod to test consume secrets
Nov 26 08:00:29.486: INFO: Waiting up to 5m0s for pod "pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747" in namespace "secrets-2402" to be "Succeeded or Failed"
Nov 26 08:00:29.493: INFO: Pod "pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747": Phase="Pending", Reason="", readiness=false. Elapsed: 6.629884ms
Nov 26 08:00:31.496: INFO: Pod "pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0102137s
STEP: Saw pod success
Nov 26 08:00:31.496: INFO: Pod "pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747" satisfied condition "Succeeded or Failed"
Nov 26 08:00:31.498: INFO: Trying to get logs from node cncf-node2 pod pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747 container secret-env-test: <nil>
STEP: delete the pod
Nov 26 08:00:31.512: INFO: Waiting for pod pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747 to disappear
Nov 26 08:00:31.518: INFO: Pod pod-secrets-67c091a0-49b7-45af-b2f3-91e10b0b6747 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:00:31.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2402" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":2764,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:00:31.522: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-31cb4464-b7f9-4f88-b065-3b6454d146be in namespace container-probe-246
Nov 26 08:00:33.598: INFO: Started pod liveness-31cb4464-b7f9-4f88-b065-3b6454d146be in namespace container-probe-246
STEP: checking the pod's current state and verifying that restartCount is present
Nov 26 08:00:33.600: INFO: Initial restart count of pod liveness-31cb4464-b7f9-4f88-b065-3b6454d146be is 0
Nov 26 08:00:53.653: INFO: Restart count of pod container-probe-246/liveness-31cb4464-b7f9-4f88-b065-3b6454d146be is now 1 (20.053496435s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:00:53.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-246" for this suite.

• [SLOW TEST:22.154 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2772,"failed":0}
S
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:00:53.676: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 26 08:00:53.738: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:00:58.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6279" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:00:58.161: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 26 08:00:58.195: INFO: PodSpec: initContainers in spec.initContainers
Nov 26 08:01:40.223: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8a9a8c76-0a6a-4584-a582-ce34938e2e34", GenerateName:"", Namespace:"init-container-9788", SelfLink:"", UID:"509b0090-7bbe-4878-a8c8-ecfc68a3fd1f", ResourceVersion:"106410", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63773510458, loc:(*time.Location)(0xa09bc80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"195959374"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"756f13a917082273256310edf38daa5f8dad63b00720d32b79e1406089d143b7", "cni.projectcalico.org/podIP":"10.244.35.166/32", "cni.projectcalico.org/podIPs":"10.244.35.166/32", "createdTime":"2021-11-26T17:00:59.399808786+09:00", "creator":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount", "updatedTime":"2021-11-26T17:00:59.399808786+09:00", "updater":"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002e86d68), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002e86d80), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002e86d98), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002e86db0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002e86dc8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002e86de0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-5q9br", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000f35640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5q9br", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5q9br", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5q9br", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006561870), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cncf-node3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003bc6bd0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006561900)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006561920)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006561928), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00656192c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00580ed80), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510459, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510459, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510459, loc:(*time.Location)(0xa09bc80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773510458, loc:(*time.Location)(0xa09bc80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.21.7.12", PodIP:"10.244.35.166", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.35.166"}}, StartTime:(*v1.Time)(0xc002e86e10), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003bc6cb0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003bc6d20)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:244bdbdf4b8d368b5836e9d2c7808a280a73ad72ae321d644e9f220da503218f", ContainerID:"cri-o://7500f2b7bfb886fe897708d1f7657932f24cd11b5c3ecccda22648effbb3c8a4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f35700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000f356a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0065619af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:01:40.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9788" for this suite.

• [SLOW TEST:42.097 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":152,"skipped":2806,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:01:40.258: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:01:40.574: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:01:43.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:01:43.597: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1847-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:01:46.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6441" for this suite.
STEP: Destroying namespace "webhook-6441-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.529 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":153,"skipped":2813,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:01:46.788: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Nov 26 08:01:46.827: INFO: namespace kubectl-1753
Nov 26 08:01:46.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-1753 create -f -'
Nov 26 08:01:47.941: INFO: stderr: ""
Nov 26 08:01:47.941: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 26 08:01:48.945: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:01:48.945: INFO: Found 0 / 1
Nov 26 08:01:49.963: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:01:49.964: INFO: Found 1 / 1
Nov 26 08:01:49.964: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 26 08:01:49.965: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:01:49.965: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 26 08:01:49.965: INFO: wait on agnhost-primary startup in kubectl-1753 
Nov 26 08:01:49.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-1753 logs agnhost-primary-mmncr agnhost-primary'
Nov 26 08:01:50.024: INFO: stderr: ""
Nov 26 08:01:50.024: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 26 08:01:50.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-1753 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 26 08:01:50.094: INFO: stderr: ""
Nov 26 08:01:50.094: INFO: stdout: "service/rm2 exposed\n"
Nov 26 08:01:50.095: INFO: Service rm2 in namespace kubectl-1753 found.
STEP: exposing service
Nov 26 08:01:52.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-1753 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 26 08:01:52.167: INFO: stderr: ""
Nov 26 08:01:52.167: INFO: stdout: "service/rm3 exposed\n"
Nov 26 08:01:52.169: INFO: Service rm3 in namespace kubectl-1753 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:01:54.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1753" for this suite.

• [SLOW TEST:7.394 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":154,"skipped":2821,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:01:54.182: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Nov 26 08:01:54.220: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:21.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8507" for this suite.

• [SLOW TEST:27.253 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":155,"skipped":2821,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:21.435: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:02:21.469: INFO: Creating pod...
Nov 26 08:02:21.484: INFO: Pod Quantity: 1 Status: Pending
Nov 26 08:02:22.518: INFO: Pod Quantity: 1 Status: Pending
Nov 26 08:02:23.487: INFO: Pod Status: Running
Nov 26 08:02:23.487: INFO: Creating service...
Nov 26 08:02:23.522: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/DELETE
Nov 26 08:02:23.528: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 26 08:02:23.528: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/GET
Nov 26 08:02:23.531: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 26 08:02:23.531: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/HEAD
Nov 26 08:02:23.532: INFO: http.Client request:HEAD | StatusCode:200
Nov 26 08:02:23.532: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 26 08:02:23.534: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 26 08:02:23.534: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/PATCH
Nov 26 08:02:23.535: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 26 08:02:23.535: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/POST
Nov 26 08:02:23.537: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 26 08:02:23.537: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/pods/agnhost/proxy/some/path/with/PUT
Nov 26 08:02:23.539: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 26 08:02:23.539: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/DELETE
Nov 26 08:02:23.541: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 26 08:02:23.541: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/GET
Nov 26 08:02:23.543: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 26 08:02:23.543: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/HEAD
Nov 26 08:02:23.545: INFO: http.Client request:HEAD | StatusCode:200
Nov 26 08:02:23.545: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/OPTIONS
Nov 26 08:02:23.547: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 26 08:02:23.547: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/PATCH
Nov 26 08:02:23.549: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 26 08:02:23.549: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/POST
Nov 26 08:02:23.551: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 26 08:02:23.551: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7962/services/test-service/proxy/some/path/with/PUT
Nov 26 08:02:23.553: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:23.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7962" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":156,"skipped":2825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:23.562: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 26 08:02:23.604: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 08:02:28.737: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:47.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-907" for this suite.

• [SLOW TEST:24.279 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":157,"skipped":2860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:02:47.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5" in namespace "projected-6030" to be "Succeeded or Failed"
Nov 26 08:02:47.883: INFO: Pod "downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.478351ms
Nov 26 08:02:49.886: INFO: Pod "downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008759883s
STEP: Saw pod success
Nov 26 08:02:49.886: INFO: Pod "downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5" satisfied condition "Succeeded or Failed"
Nov 26 08:02:49.888: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5 container client-container: <nil>
STEP: delete the pod
Nov 26 08:02:49.909: INFO: Waiting for pod downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5 to disappear
Nov 26 08:02:49.915: INFO: Pod downwardapi-volume-9c7e842f-42b8-4f0a-b6ff-1526600896b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:49.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6030" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2914,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:49.943: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:02:49.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b" in namespace "downward-api-3523" to be "Succeeded or Failed"
Nov 26 08:02:49.989: INFO: Pod "downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.115422ms
Nov 26 08:02:51.994: INFO: Pod "downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009183462s
STEP: Saw pod success
Nov 26 08:02:51.994: INFO: Pod "downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b" satisfied condition "Succeeded or Failed"
Nov 26 08:02:51.995: INFO: Trying to get logs from node cncf-node2 pod downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b container client-container: <nil>
STEP: delete the pod
Nov 26 08:02:52.013: INFO: Waiting for pod downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b to disappear
Nov 26 08:02:52.018: INFO: Pod downwardapi-volume-b3f133bc-8ef3-4c21-91f1-3d2941ccaf7b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:52.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3523" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":2915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 26 08:02:52.068: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 26 08:02:52.070: INFO: starting watch
STEP: patching
STEP: updating
Nov 26 08:02:52.098: INFO: waiting for watch events with expected annotations
Nov 26 08:02:52.098: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:52.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7257" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":160,"skipped":2948,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:52.137: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 26 08:02:52.171: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 26 08:02:52.175: INFO: Waiting for terminating namespaces to be deleted...
Nov 26 08:02:52.177: INFO: 
Logging pods the apiserver thinks is on node cncf-node2 before test
Nov 26 08:02:52.187: INFO: cert-manager-cainjector-668d9c86df-mjdsd from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 08:02:52.187: INFO: console-https-secret-create--1-74vfc from console-system started at 2021-11-26 04:05:29 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container create ready: false, restart count 0
Nov 26 08:02:52.187: INFO: hyperauth-765f5c6d78-qhm62 from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 08:02:52.187: INFO: hyperauth-765f5c6d78-tqrpp from hyperauth started at 2021-11-26 04:04:39 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container hyperauth ready: true, restart count 0
Nov 26 08:02:52.187: INFO: hypercloud5-api-server-78fdd97c4b-ct44f from hypercloud5-system started at 2021-11-26 04:05:03 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container hypercloud5-api-server ready: true, restart count 0
Nov 26 08:02:52.187: INFO: postgres-d85758466-7zvpj from hypercloud5-system started at 2021-11-26 04:05:04 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container postgres ready: true, restart count 0
Nov 26 08:02:52.187: INFO: calico-node-5c7nc from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 08:02:52.187: INFO: kube-proxy-8lrhw from kube-system started at 2021-11-26 03:46:21 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 08:02:52.187: INFO: snapshot-controller-bb7675d55-ccb2z from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 08:02:52.187: INFO: csi-cephfsplugin-provisioner-fb98b8789-h5xws from rook-ceph started at 2021-11-26 03:53:16 +0000 UTC (6 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.187: INFO: csi-cephfsplugin-w6qlg from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.187: INFO: csi-rbdplugin-9jp7k from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (3 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.187: INFO: csi-rbdplugin-provisioner-7dffb8c5c-7z6g2 from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 08:02:52.187: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.187: INFO: rook-ceph-mon-a-6844dffc48-h4rtd from rook-ceph started at 2021-11-26 03:53:10 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container mon ready: true, restart count 0
Nov 26 08:02:52.187: INFO: rook-ceph-operator-cdf9dfd9c-c6rpm from rook-ceph started at 2021-11-26 03:52:55 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container rook-ceph-operator ready: true, restart count 0
Nov 26 08:02:52.187: INFO: rook-ceph-osd-0-f55d4f54c-lx2lb from rook-ceph started at 2021-11-26 03:53:54 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.187: INFO: 	Container osd ready: true, restart count 0
Nov 26 08:02:52.187: INFO: rook-ceph-osd-prepare-cncf-node2--1-lxkz4 from rook-ceph started at 2021-11-26 04:53:13 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.188: INFO: 	Container provision ready: false, restart count 0
Nov 26 08:02:52.188: INFO: rook-ceph-tools-6967fc698d-rtdgw from rook-ceph started at 2021-11-26 03:53:44 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.188: INFO: 	Container rook-ceph-tools ready: true, restart count 0
Nov 26 08:02:52.188: INFO: rook-discover-vpgq5 from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.188: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 08:02:52.188: INFO: sonobuoy from sonobuoy started at 2021-11-26 07:20:40 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.188: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 26 08:02:52.188: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-7b26m from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 08:02:52.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 08:02:52.188: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 26 08:02:52.188: INFO: 
Logging pods the apiserver thinks is on node cncf-node3 before test
Nov 26 08:02:52.197: INFO: cert-manager-7c6f78c46d-rlvcs from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 08:02:52.197: INFO: cert-manager-webhook-764b556954-7dqpb from cert-manager started at 2021-11-26 04:03:10 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container cert-manager ready: true, restart count 0
Nov 26 08:02:52.197: INFO: console-58964cf476-bm4wx from console-system started at 2021-11-26 04:05:30 +0000 UTC (2 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container console ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container manager ready: true, restart count 0
Nov 26 08:02:52.197: INFO: postgresql-7fd9d45fc-s8kfv from hyperauth started at 2021-11-26 04:03:34 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container postgresql ready: true, restart count 0
Nov 26 08:02:52.197: INFO: calico-node-qwtck from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container calico-node ready: true, restart count 0
Nov 26 08:02:52.197: INFO: kube-proxy-77n76 from kube-system started at 2021-11-26 03:46:12 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 26 08:02:52.197: INFO: snapshot-controller-bb7675d55-ld7k5 from kube-system started at 2021-11-26 03:52:52 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container snapshot-controller ready: true, restart count 0
Nov 26 08:02:52.197: INFO: csi-cephfsplugin-provisioner-fb98b8789-6tqgt from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (6 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.197: INFO: csi-cephfsplugin-qkp2d from rook-ceph started at 2021-11-26 03:53:15 +0000 UTC (3 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.197: INFO: csi-rbdplugin-8ztcb from rook-ceph started at 2021-11-26 03:53:13 +0000 UTC (3 container statuses recorded)
Nov 26 08:02:52.197: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 08:02:52.197: INFO: 	Container driver-registrar ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.198: INFO: csi-rbdplugin-provisioner-7dffb8c5c-bpsbv from rook-ceph started at 2021-11-26 03:53:14 +0000 UTC (6 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container csi-attacher ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container csi-provisioner ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container csi-resizer ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container csi-snapshotter ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container liveness-prometheus ready: true, restart count 0
Nov 26 08:02:52.198: INFO: rook-ceph-mds-myfs-a-7745758cff-g6xnp from rook-ceph started at 2021-11-26 03:53:39 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container mds ready: true, restart count 0
Nov 26 08:02:52.198: INFO: rook-ceph-mds-myfs-b-84457dc995-4r7kk from rook-ceph started at 2021-11-26 03:53:40 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container mds ready: true, restart count 0
Nov 26 08:02:52.198: INFO: rook-ceph-mgr-a-5dc4b44f94-7kd2x from rook-ceph started at 2021-11-26 03:53:20 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container mgr ready: true, restart count 0
Nov 26 08:02:52.198: INFO: rook-ceph-osd-1-558d464b49-kf8wj from rook-ceph started at 2021-11-26 03:53:55 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container osd ready: true, restart count 0
Nov 26 08:02:52.198: INFO: rook-ceph-osd-prepare-cncf-node3--1-dkjrx from rook-ceph started at 2021-11-26 04:53:15 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container provision ready: false, restart count 0
Nov 26 08:02:52.198: INFO: rook-discover-jct2w from rook-ceph started at 2021-11-26 03:52:56 +0000 UTC (1 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container rook-discover ready: true, restart count 0
Nov 26 08:02:52.198: INFO: sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-gm2tc from sonobuoy started at 2021-11-26 07:20:41 +0000 UTC (2 container statuses recorded)
Nov 26 08:02:52.198: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 26 08:02:52.198: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-18f72388-8888-4e3b-8263-9edcfe75a7cc 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-18f72388-8888-4e3b-8263-9edcfe75a7cc off the node cncf-node2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-18f72388-8888-4e3b-8263-9edcfe75a7cc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:02:56.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2213" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":161,"skipped":2954,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:02:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:02:57.140: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:03:00.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:03:10.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3924" for this suite.
STEP: Destroying namespace "webhook-3924-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.103 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":162,"skipped":2956,"failed":0}
SSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:03:10.404: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Nov 26 08:03:10.468: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Nov 26 08:03:10.518: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:03:10.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9606" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":163,"skipped":2960,"failed":0}

------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:03:10.545: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8271" for this suite.

• [SLOW TEST:350.091 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":164,"skipped":2960,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:00.636: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:09:00.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2" in namespace "projected-2002" to be "Succeeded or Failed"
Nov 26 08:09:00.692: INFO: Pod "downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.423813ms
Nov 26 08:09:02.694: INFO: Pod "downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003425594s
STEP: Saw pod success
Nov 26 08:09:02.694: INFO: Pod "downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2" satisfied condition "Succeeded or Failed"
Nov 26 08:09:02.695: INFO: Trying to get logs from node cncf-node2 pod downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2 container client-container: <nil>
STEP: delete the pod
Nov 26 08:09:02.766: INFO: Waiting for pod downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2 to disappear
Nov 26 08:09:02.771: INFO: Pod downwardapi-volume-f2f8e8e5-fef6-467d-9dd0-cb4fefad65e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:02.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2002" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":2961,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:02.776: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 26 08:09:04.826: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:04.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1958" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":2966,"failed":0}
SSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:04.850: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 26 08:09:04.887: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 26 08:09:04.895: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 26 08:09:04.895: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 26 08:09:04.903: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 26 08:09:04.904: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 26 08:09:04.915: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 26 08:09:04.915: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 26 08:09:12.010: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:12.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5760" for this suite.

• [SLOW TEST:7.211 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":167,"skipped":2971,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:12.062: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:09:12.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 create -f -'
Nov 26 08:09:12.430: INFO: stderr: ""
Nov 26 08:09:12.430: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 26 08:09:12.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 create -f -'
Nov 26 08:09:12.703: INFO: stderr: ""
Nov 26 08:09:12.703: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 26 08:09:13.706: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:09:13.706: INFO: Found 0 / 1
Nov 26 08:09:14.706: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:09:14.706: INFO: Found 1 / 1
Nov 26 08:09:14.706: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 26 08:09:14.732: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:09:14.732: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 26 08:09:14.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 describe pod agnhost-primary-qg5dk'
Nov 26 08:09:14.792: INFO: stderr: ""
Nov 26 08:09:14.793: INFO: stdout: "Name:         agnhost-primary-qg5dk\nNamespace:    kubectl-6550\nPriority:     0\nNode:         cncf-node2/172.21.7.8\nStart Time:   Fri, 26 Nov 2021 08:09:13 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: bbcc35ec098ac74ec0fcec274848d27679a654e6e47b0e50c05b629919a54ddd\n              cni.projectcalico.org/podIP: 10.244.89.97/32\n              cni.projectcalico.org/podIPs: 10.244.89.97/32\n              createdTime: 2021-11-26T17:09:13.632480416+09:00\n              creator: system:serviceaccount:kube-system:replication-controller\n              updatedTime: 2021-11-26T17:09:13.632480416+09:00\n              updater: system:serviceaccount:kube-system:replication-controller\nStatus:       Running\nIP:           10.244.89.97\nIPs:\n  IP:           10.244.89.97\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://966518dd59fdb47db6bdec122fcc148fb8e51805f4129084c8ed47cc3c934423\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 26 Nov 2021 08:09:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pf7n7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pf7n7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6550/agnhost-primary-qg5dk to cncf-node2\n  Normal  Pulled     0s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Nov 26 08:09:14.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 describe rc agnhost-primary'
Nov 26 08:09:14.865: INFO: stderr: ""
Nov 26 08:09:14.865: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6550\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  createdTime: 2021-11-26T17:09:13.625884648+09:00\n              creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n              updatedTime: 2021-11-26T17:09:13.625884648+09:00\n              updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-qg5dk\n"
Nov 26 08:09:14.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 describe service agnhost-primary'
Nov 26 08:09:14.922: INFO: stderr: ""
Nov 26 08:09:14.922: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6550\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       createdTime: 2021-11-26T17:09:13.892710626+09:00\n                   creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n                   updatedTime: 2021-11-26T17:09:13.892710626+09:00\n                   updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.130.177\nIPs:               10.96.130.177\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.89.97:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 26 08:09:14.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 describe node cncf-node1'
Nov 26 08:09:15.002: INFO: stderr: ""
Nov 26 08:09:15.002: INFO: stdout: "Name:               cncf-node1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cncf-node1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.7.6/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.232.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 26 Nov 2021 03:43:01 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cncf-node1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 26 Nov 2021 08:09:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 26 Nov 2021 03:43:26 +0000   Fri, 26 Nov 2021 03:43:26 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 26 Nov 2021 08:04:50 +0000   Fri, 26 Nov 2021 03:42:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 26 Nov 2021 08:04:50 +0000   Fri, 26 Nov 2021 03:42:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 26 Nov 2021 08:04:50 +0000   Fri, 26 Nov 2021 03:42:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 26 Nov 2021 08:04:50 +0000   Fri, 26 Nov 2021 03:43:33 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.21.7.7\n  Hostname:    cncf-node1\nCapacity:\n  cpu:                16\n  ephemeral-storage:  51175Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31918588Ki\n  pods:               110\nAllocatable:\n  cpu:                16\n  ephemeral-storage:  48294789041\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             31816188Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9f60f68b5bad45f8875b970b9c164edd\n  System UUID:                b7b6ab24-07b8-11e7-bfae-0894ef3d373e\n  Boot ID:                    b733cc93-7474-4cd3-9efa-15eab893dd9f\n  Kernel Version:             4.18.0-193.el8.x86_64\n  OS Image:                   CentOS Linux 8\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.22.1\n  Kubelet Version:            v1.22.2\n  Kube-Proxy Version:         v1.22.2\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                              ------------  ----------  ---------------  -------------  ---\n  hypercloud5-system          hypercloud-single-operator-controller-manager-5d447c64d6-bvccc    200m (1%)     200m (1%)   40Mi (0%)        80Mi (0%)      4h4m\n  kube-system                 calico-kube-controllers-75f8f6cc59-49qq5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h25m\n  kube-system                 calico-node-4wlr4                                                 250m (1%)     0 (0%)      0 (0%)           0 (0%)         4h25m\n  kube-system                 coredns-78fcd69978-kbplp                                          100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     4h25m\n  kube-system                 coredns-78fcd69978-l5s8q                                          100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     4h25m\n  kube-system                 etcd-cncf-node1                                                   100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         4h26m\n  kube-system                 kube-apiserver-cncf-node1                                         250m (1%)     0 (0%)      0 (0%)           0 (0%)         4h4m\n  kube-system                 kube-controller-manager-cncf-node1                                200m (1%)     0 (0%)      0 (0%)           0 (0%)         4h26m\n  kube-system                 kube-proxy-l4mzq                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h25m\n  kube-system                 kube-scheduler-cncf-node1                                         100m (0%)     0 (0%)      0 (0%)           0 (0%)         4h26m\n  sonobuoy                    sonobuoy-e2e-job-d24da1925e904e25                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-19d667b4eecd4e3c-pbh56           0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1300m (8%)  200m (1%)\n  memory             280Mi (0%)  420Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Nov 26 08:09:15.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6550 describe namespace kubectl-6550'
Nov 26 08:09:15.058: INFO: stderr: ""
Nov 26 08:09:15.058: INFO: stdout: "Name:         kubectl-6550\nLabels:       e2e-framework=kubectl\n              e2e-run=ac993761-5996-4499-8441-e454f3302656\n              kubernetes.io/metadata.name=kubectl-6550\nAnnotations:  createdTime: 2021-11-26T17:09:13.266417402+09:00\n              creator: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\n              updatedTime: 2021-11-26T17:09:13.266417402+09:00\n              updater: system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:15.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6550" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":168,"skipped":2999,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:15.063: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 26 08:09:15.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2728 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Nov 26 08:09:15.188: INFO: stderr: ""
Nov 26 08:09:15.188: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Nov 26 08:09:15.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2728 delete pods e2e-test-httpd-pod'
Nov 26 08:09:17.524: INFO: stderr: ""
Nov 26 08:09:17.524: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:17.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2728" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":169,"skipped":3012,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:17.531: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3d5575b2-d25c-4fd9-b36a-f9c0c8a69461
STEP: Creating a pod to test consume secrets
Nov 26 08:09:17.607: INFO: Waiting up to 5m0s for pod "pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b" in namespace "secrets-3690" to be "Succeeded or Failed"
Nov 26 08:09:17.613: INFO: Pod "pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.138776ms
Nov 26 08:09:19.619: INFO: Pod "pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012045361s
STEP: Saw pod success
Nov 26 08:09:19.619: INFO: Pod "pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b" satisfied condition "Succeeded or Failed"
Nov 26 08:09:19.627: INFO: Trying to get logs from node cncf-node3 pod pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:09:19.654: INFO: Waiting for pod pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b to disappear
Nov 26 08:09:19.659: INFO: Pod pod-secrets-6996a3da-ca27-4de9-ae2e-51d8f405f91b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:19.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3690" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":3032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:19.664: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-a346a9fb-78f4-40e8-a04b-78fc80fc8388
STEP: Creating a pod to test consume configMaps
Nov 26 08:09:19.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203" in namespace "configmap-3064" to be "Succeeded or Failed"
Nov 26 08:09:19.753: INFO: Pod "pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.205011ms
Nov 26 08:09:21.756: INFO: Pod "pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010858207s
STEP: Saw pod success
Nov 26 08:09:21.756: INFO: Pod "pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203" satisfied condition "Succeeded or Failed"
Nov 26 08:09:21.757: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 08:09:21.780: INFO: Waiting for pod pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203 to disappear
Nov 26 08:09:21.793: INFO: Pod pod-configmaps-98a31f5c-c3dd-483f-8398-53e48e3a9203 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:21.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3064" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3055,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:21.798: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Nov 26 08:09:21.834: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7016 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:09:21.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7016" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":172,"skipped":3062,"failed":0}
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:09:21.880: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 26 08:09:21.914: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 26 08:10:21.953: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:10:21.955: INFO: Starting informer...
STEP: Starting pods...
Nov 26 08:10:22.168: INFO: Pod1 is running on cncf-node3. Tainting Node
Nov 26 08:10:24.379: INFO: Pod2 is running on cncf-node3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 26 08:10:35.921: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 26 08:10:50.942: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:10:50.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1149" for this suite.

• [SLOW TEST:89.125 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":173,"skipped":3063,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:10:51.005: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:01.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4203" for this suite.

• [SLOW TEST:70.140 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":174,"skipped":3069,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:01.145: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-ba44277c-b1b0-4aa3-b2fc-082e0911fb78
STEP: Creating a pod to test consume secrets
Nov 26 08:12:01.231: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2" in namespace "projected-2288" to be "Succeeded or Failed"
Nov 26 08:12:01.243: INFO: Pod "pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.063076ms
Nov 26 08:12:03.283: INFO: Pod "pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052585357s
STEP: Saw pod success
Nov 26 08:12:03.283: INFO: Pod "pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2" satisfied condition "Succeeded or Failed"
Nov 26 08:12:03.285: INFO: Trying to get logs from node cncf-node3 pod pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2 container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:12:03.317: INFO: Waiting for pod pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2 to disappear
Nov 26 08:12:03.322: INFO: Pod pod-projected-secrets-2f4726a0-4f60-49a2-8e12-185c1e4700e2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:03.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2288" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3075,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:03.327: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 26 08:12:03.374: INFO: The status of Pod pod-update-32481ade-e7f6-4ca9-aad8-a071274570a0 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:12:05.378: INFO: The status of Pod pod-update-32481ade-e7f6-4ca9-aad8-a071274570a0 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 26 08:12:05.895: INFO: Successfully updated pod "pod-update-32481ade-e7f6-4ca9-aad8-a071274570a0"
STEP: verifying the updated pod is in kubernetes
Nov 26 08:12:05.917: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:05.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9532" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:05.923: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Nov 26 08:12:05.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2237 create -f -'
Nov 26 08:12:07.257: INFO: stderr: ""
Nov 26 08:12:07.257: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 26 08:12:08.260: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:12:08.260: INFO: Found 0 / 1
Nov 26 08:12:09.261: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:12:09.261: INFO: Found 1 / 1
Nov 26 08:12:09.261: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 26 08:12:09.262: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:12:09.262: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 26 08:12:09.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2237 patch pod agnhost-primary-tbnmv -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 26 08:12:09.316: INFO: stderr: ""
Nov 26 08:12:09.316: INFO: stdout: "pod/agnhost-primary-tbnmv patched\n"
STEP: checking annotations
Nov 26 08:12:09.323: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 26 08:12:09.323: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:09.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2237" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":177,"skipped":3128,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:09.330: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Nov 26 08:12:09.394: INFO: Creating simple deployment test-deployment-zmdjz
Nov 26 08:12:09.415: INFO: deployment "test-deployment-zmdjz" doesn't have the required revision set
STEP: Getting /status
Nov 26 08:12:11.423: INFO: Deployment test-deployment-zmdjz has Conditions: [{Available True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmdjz-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Nov 26 08:12:11.428: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773511131, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773511131, loc:(*time.Location)(0xa09bc80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63773511131, loc:(*time.Location)(0xa09bc80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63773511129, loc:(*time.Location)(0xa09bc80)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-zmdjz-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Nov 26 08:12:11.429: INFO: Observed &Deployment event: ADDED
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmdjz-794dd694d8"}
Nov 26 08:12:11.429: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmdjz-794dd694d8"}
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 26 08:12:11.429: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zmdjz-794dd694d8" is progressing.}
Nov 26 08:12:11.429: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 26 08:12:11.429: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmdjz-794dd694d8" has successfully progressed.}
Nov 26 08:12:11.430: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.430: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 26 08:12:11.430: INFO: Observed Deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmdjz-794dd694d8" has successfully progressed.}
Nov 26 08:12:11.430: INFO: Found Deployment test-deployment-zmdjz in namespace deployment-9653 with labels: map[e2e:testing name:httpd] annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 26 08:12:11.430: INFO: Deployment test-deployment-zmdjz has an updated status
STEP: patching the Statefulset Status
Nov 26 08:12:11.430: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 26 08:12:11.438: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Nov 26 08:12:11.439: INFO: Observed &Deployment event: ADDED
Nov 26 08:12:11.439: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmdjz-794dd694d8"}
Nov 26 08:12:11.439: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.439: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-zmdjz-794dd694d8"}
Nov 26 08:12:11.439: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 26 08:12:11.439: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.439: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available False 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 26 08:12:11.439: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:09 +0000 UTC 2021-11-26 08:12:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-zmdjz-794dd694d8" is progressing.}
Nov 26 08:12:11.439: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.440: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 26 08:12:11.440: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmdjz-794dd694d8" has successfully progressed.}
Nov 26 08:12:11.440: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.440: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Available True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 26 08:12:11.440: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {Progressing True 2021-11-26 08:12:11 +0000 UTC 2021-11-26 08:12:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-zmdjz-794dd694d8" has successfully progressed.}
Nov 26 08:12:11.440: INFO: Observed deployment test-deployment-zmdjz in namespace deployment-9653 with annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 26 08:12:11.440: INFO: Observed &Deployment event: MODIFIED
Nov 26 08:12:11.440: INFO: Found deployment test-deployment-zmdjz in namespace deployment-9653 with labels: map[e2e:testing name:httpd] annotations: map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 26 08:12:11.440: INFO: Deployment test-deployment-zmdjz has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 08:12:11.444: INFO: Deployment "test-deployment-zmdjz":
&Deployment{ObjectMeta:{test-deployment-zmdjz  deployment-9653  fed0613e-3a96-4273-82b5-d887c30a945b 111183 1 2021-11-26 08:12:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 2021-11-26 08:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2021-11-26 08:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2021-11-26 08:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055aa488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-zmdjz-794dd694d8",LastUpdateTime:2021-11-26 08:12:11 +0000 UTC,LastTransitionTime:2021-11-26 08:12:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 26 08:12:11.474: INFO: New ReplicaSet "test-deployment-zmdjz-794dd694d8" of Deployment "test-deployment-zmdjz":
&ReplicaSet{ObjectMeta:{test-deployment-zmdjz-794dd694d8  deployment-9653  8ff9a873-fe5f-4f36-b0c8-7329539fc347 111171 1 2021-11-26 08:12:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[createdTime:2021-11-26T17:12:10.599074611+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:12:10.599074611+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment-zmdjz fed0613e-3a96-4273-82b5-d887c30a945b 0xc0055aa887 0xc0055aa888}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fed0613e-3a96-4273-82b5-d887c30a945b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:12:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055aaba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:12:11.476: INFO: Pod "test-deployment-zmdjz-794dd694d8-lsmz9" is available:
&Pod{ObjectMeta:{test-deployment-zmdjz-794dd694d8-lsmz9 test-deployment-zmdjz-794dd694d8- deployment-9653  69cbeb8c-3ca3-4edd-84b4-455c9c9966f0 111169 0 2021-11-26 08:12:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:1ff62aafbdd6bc5376693872ae353b9410cffcde64c7b99db586ed67a2773990 cni.projectcalico.org/podIP:10.244.35.182/32 cni.projectcalico.org/podIPs:10.244.35.182/32 createdTime:2021-11-26T17:12:10.630167555+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:12:10.630167555+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-zmdjz-794dd694d8 8ff9a873-fe5f-4f36-b0c8-7329539fc347 0xc0055aaf8e 0xc0055aaf8f}] []  [{kube-controller-manager Update v1 2021-11-26 08:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ff9a873-fe5f-4f36-b0c8-7329539fc347\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:12:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4lzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4lzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:12:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.182,StartTime:2021-11-26 08:12:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:12:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://7594ad5c519104e9d250b3ecc4f90e7890f24f368443053e8dc075f1c0f2294e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:11.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9653" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":178,"skipped":3132,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:11.513: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:12:13.613: INFO: Deleting pod "var-expansion-57374062-f181-4d70-be3b-7f6bb3fa1612" in namespace "var-expansion-6265"
Nov 26 08:12:13.616: INFO: Wait up to 5m0s for pod "var-expansion-57374062-f181-4d70-be3b-7f6bb3fa1612" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:17.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6265" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":179,"skipped":3135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:17.634: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 26 08:12:17.683: INFO: Waiting up to 5m0s for pod "downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e" in namespace "downward-api-5857" to be "Succeeded or Failed"
Nov 26 08:12:17.690: INFO: Pod "downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.421459ms
Nov 26 08:12:19.696: INFO: Pod "downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013357187s
STEP: Saw pod success
Nov 26 08:12:19.696: INFO: Pod "downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e" satisfied condition "Succeeded or Failed"
Nov 26 08:12:19.697: INFO: Trying to get logs from node cncf-node3 pod downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e container dapi-container: <nil>
STEP: delete the pod
Nov 26 08:12:19.720: INFO: Waiting for pod downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e to disappear
Nov 26 08:12:19.725: INFO: Pod downward-api-0e4360ca-47a0-4f7b-95b1-728c32e7a99e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:12:19.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5857" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:12:19.729: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-b1b1b8a1-381d-452e-8f0a-130af88c4a2b in namespace container-probe-7630
Nov 26 08:12:21.825: INFO: Started pod busybox-b1b1b8a1-381d-452e-8f0a-130af88c4a2b in namespace container-probe-7630
STEP: checking the pod's current state and verifying that restartCount is present
Nov 26 08:12:21.827: INFO: Initial restart count of pod busybox-b1b1b8a1-381d-452e-8f0a-130af88c4a2b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:16:22.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7630" for this suite.

• [SLOW TEST:242.974 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3215,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:16:22.704: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 in namespace container-probe-4469
Nov 26 08:16:24.780: INFO: Started pod liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 in namespace container-probe-4469
STEP: checking the pod's current state and verifying that restartCount is present
Nov 26 08:16:24.781: INFO: Initial restart count of pod liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 is 0
Nov 26 08:16:44.889: INFO: Restart count of pod container-probe-4469/liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 is now 1 (20.107922347s elapsed)
Nov 26 08:17:04.949: INFO: Restart count of pod container-probe-4469/liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 is now 2 (40.167657455s elapsed)
Nov 26 08:17:25.021: INFO: Restart count of pod container-probe-4469/liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 is now 3 (1m0.240048303s elapsed)
Nov 26 08:17:45.096: INFO: Restart count of pod container-probe-4469/liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 is now 4 (1m20.314857956s elapsed)
Nov 26 08:18:47.311: INFO: Restart count of pod container-probe-4469/liveness-4a22cc3a-75f7-4cfb-aa2d-94494dd9c8c5 is now 5 (2m22.530122899s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:18:47.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4469" for this suite.

• [SLOW TEST:144.640 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:18:47.344: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:18:47.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1621" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":183,"skipped":3244,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:18:47.427: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 26 08:20:47.999: INFO: Successfully updated pod "var-expansion-da82ce03-4c69-4b79-a6a4-6d6d3e208c19"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 26 08:20:50.019: INFO: Deleting pod "var-expansion-da82ce03-4c69-4b79-a6a4-6d6d3e208c19" in namespace "var-expansion-2823"
Nov 26 08:20:50.022: INFO: Wait up to 5m0s for pod "var-expansion-da82ce03-4c69-4b79-a6a4-6d6d3e208c19" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:21:22.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2823" for this suite.

• [SLOW TEST:154.644 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":184,"skipped":3248,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:21:22.071: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 26 08:21:22.124: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6519  ae2edde4-d379-4872-8b96-7de0d448211a 114154 0 2021-11-26 08:21:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2021-11-26T17:21:23.325406394+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:21:23.325406394+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:21:22.124: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6519  ae2edde4-d379-4872-8b96-7de0d448211a 114155 0 2021-11-26 08:21:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2021-11-26T17:21:23.325406394+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:21:23.325406394+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:21:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 26 08:21:22.136: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6519  ae2edde4-d379-4872-8b96-7de0d448211a 114156 0 2021-11-26 08:21:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2021-11-26T17:21:23.325406394+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:21:23.325406394+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:21:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:21:22.136: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6519  ae2edde4-d379-4872-8b96-7de0d448211a 114157 0 2021-11-26 08:21:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[createdTime:2021-11-26T17:21:23.325406394+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:21:23.325406394+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:21:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:21:22.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6519" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":185,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:21:22.142: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Nov 26 08:21:23.324: INFO: The status of Pod kube-controller-manager-cncf-node1 is Running (Ready = true)
Nov 26 08:21:23.593: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:21:23.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8116" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":186,"skipped":3291,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:21:23.599: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Nov 26 08:21:23.647: INFO: Waiting up to 5m0s for pod "var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c" in namespace "var-expansion-4261" to be "Succeeded or Failed"
Nov 26 08:21:23.654: INFO: Pod "var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.519195ms
Nov 26 08:21:25.657: INFO: Pod "var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009013827s
STEP: Saw pod success
Nov 26 08:21:25.657: INFO: Pod "var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c" satisfied condition "Succeeded or Failed"
Nov 26 08:21:25.658: INFO: Trying to get logs from node cncf-node3 pod var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c container dapi-container: <nil>
STEP: delete the pod
Nov 26 08:21:25.723: INFO: Waiting for pod var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c to disappear
Nov 26 08:21:25.728: INFO: Pod var-expansion-fc4fa7f2-d5bf-43b0-9251-50f0221ad36c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:21:25.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4261" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":187,"skipped":3297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:21:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 26 08:21:31.801: INFO: The status of Pod kube-controller-manager-cncf-node1 is Running (Ready = true)
Nov 26 08:21:32.061: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:21:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8830" for this suite.

• [SLOW TEST:6.333 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":188,"skipped":3319,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:21:32.066: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-55ae1328-66fc-426a-a5f4-43de549f1b30 in namespace container-probe-542
Nov 26 08:21:40.142: INFO: Started pod test-webserver-55ae1328-66fc-426a-a5f4-43de549f1b30 in namespace container-probe-542
STEP: checking the pod's current state and verifying that restartCount is present
Nov 26 08:21:40.144: INFO: Initial restart count of pod test-webserver-55ae1328-66fc-426a-a5f4-43de549f1b30 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:25:40.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-542" for this suite.

• [SLOW TEST:248.613 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3323,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:25:40.679: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:25:40.760: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 26 08:25:45.766: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Nov 26 08:25:45.771: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Nov 26 08:25:45.800: INFO: observed ReplicaSet test-rs in namespace replicaset-6909 with ReadyReplicas 1, AvailableReplicas 1
Nov 26 08:25:45.845: INFO: observed ReplicaSet test-rs in namespace replicaset-6909 with ReadyReplicas 1, AvailableReplicas 1
Nov 26 08:25:45.862: INFO: observed ReplicaSet test-rs in namespace replicaset-6909 with ReadyReplicas 1, AvailableReplicas 1
Nov 26 08:25:45.873: INFO: observed ReplicaSet test-rs in namespace replicaset-6909 with ReadyReplicas 1, AvailableReplicas 1
Nov 26 08:25:47.182: INFO: observed ReplicaSet test-rs in namespace replicaset-6909 with ReadyReplicas 2, AvailableReplicas 2
Nov 26 08:25:47.222: INFO: observed Replicaset test-rs in namespace replicaset-6909 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:25:47.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6909" for this suite.

• [SLOW TEST:6.557 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":190,"skipped":3338,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:25:47.237: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:25:47.275: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 26 08:25:51.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-4064 --namespace=crd-publish-openapi-4064 create -f -'
Nov 26 08:25:53.331: INFO: stderr: ""
Nov 26 08:25:53.332: INFO: stdout: "e2e-test-crd-publish-openapi-9590-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 26 08:25:53.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-4064 --namespace=crd-publish-openapi-4064 delete e2e-test-crd-publish-openapi-9590-crds test-cr'
Nov 26 08:25:53.391: INFO: stderr: ""
Nov 26 08:25:53.391: INFO: stdout: "e2e-test-crd-publish-openapi-9590-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 26 08:25:53.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-4064 --namespace=crd-publish-openapi-4064 apply -f -'
Nov 26 08:25:53.666: INFO: stderr: ""
Nov 26 08:25:53.666: INFO: stdout: "e2e-test-crd-publish-openapi-9590-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 26 08:25:53.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-4064 --namespace=crd-publish-openapi-4064 delete e2e-test-crd-publish-openapi-9590-crds test-cr'
Nov 26 08:25:53.728: INFO: stderr: ""
Nov 26 08:25:53.728: INFO: stdout: "e2e-test-crd-publish-openapi-9590-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 26 08:25:53.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-4064 explain e2e-test-crd-publish-openapi-9590-crds'
Nov 26 08:25:54.089: INFO: stderr: ""
Nov 26 08:25:54.089: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9590-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:25:59.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4064" for this suite.

• [SLOW TEST:11.967 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":191,"skipped":3359,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:25:59.204: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:26:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3809" for this suite.

• [SLOW TEST:11.151 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":192,"skipped":3371,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:26:10.355: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:26:10.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6" in namespace "projected-7897" to be "Succeeded or Failed"
Nov 26 08:26:10.441: INFO: Pod "downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13763ms
Nov 26 08:26:12.448: INFO: Pod "downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01407145s
STEP: Saw pod success
Nov 26 08:26:12.448: INFO: Pod "downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6" satisfied condition "Succeeded or Failed"
Nov 26 08:26:12.449: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6 container client-container: <nil>
STEP: delete the pod
Nov 26 08:26:12.482: INFO: Waiting for pod downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6 to disappear
Nov 26 08:26:12.487: INFO: Pod downwardapi-volume-258d41ff-909e-4ab4-a7e6-1163a7dd97a6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:26:12.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7897" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3389,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:26:12.492: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3263
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-3263
Nov 26 08:26:12.584: INFO: Found 0 stateful pods, waiting for 1
Nov 26 08:26:22.587: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Nov 26 08:26:22.608: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Nov 26 08:26:22.613: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Nov 26 08:26:22.615: INFO: Observed &StatefulSet event: ADDED
Nov 26 08:26:22.615: INFO: Found Statefulset ss in namespace statefulset-3263 with labels: map[e2e:testing] annotations: map[createdTime:2021-11-26T17:26:13.782404376+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:26:13.782404376+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 26 08:26:22.615: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Nov 26 08:26:22.615: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 26 08:26:22.620: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Nov 26 08:26:22.622: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 08:26:22.622: INFO: Deleting all statefulset in ns statefulset-3263
Nov 26 08:26:22.630: INFO: Scaling statefulset ss to 0
Nov 26 08:26:32.641: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:26:32.643: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:26:32.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3263" for this suite.

• [SLOW TEST:20.189 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":194,"skipped":3403,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:26:32.681: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 26 08:26:32.735: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:26:34.738: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 26 08:26:34.757: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:26:36.760: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 26 08:26:36.769: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 26 08:26:36.780: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 26 08:26:38.781: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 26 08:26:38.783: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 26 08:26:40.781: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 26 08:26:40.784: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:26:40.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9485" for this suite.

• [SLOW TEST:8.108 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3406,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:26:40.790: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:26:40.842: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 26 08:26:40.879: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:40.881: INFO: Number of nodes with available pods: 0
Nov 26 08:26:40.881: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:26:41.884: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:41.886: INFO: Number of nodes with available pods: 0
Nov 26 08:26:41.886: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:26:42.885: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:42.887: INFO: Number of nodes with available pods: 2
Nov 26 08:26:42.887: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 26 08:26:42.912: INFO: Wrong image for pod: daemon-set-n6t2n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 26 08:26:42.912: INFO: Wrong image for pod: daemon-set-xsmxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 26 08:26:42.924: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:43.927: INFO: Wrong image for pod: daemon-set-n6t2n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 26 08:26:43.930: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:44.929: INFO: Wrong image for pod: daemon-set-n6t2n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 26 08:26:44.932: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:45.927: INFO: Pod daemon-set-cvmbw is not available
Nov 26 08:26:45.927: INFO: Wrong image for pod: daemon-set-n6t2n. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 26 08:26:45.932: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:46.930: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:47.926: INFO: Pod daemon-set-fsw9v is not available
Nov 26 08:26:47.928: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 26 08:26:47.931: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:47.932: INFO: Number of nodes with available pods: 1
Nov 26 08:26:47.932: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:26:48.937: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:26:48.939: INFO: Number of nodes with available pods: 2
Nov 26 08:26:48.939: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7194, will wait for the garbage collector to delete the pods
Nov 26 08:26:49.002: INFO: Deleting DaemonSet.extensions daemon-set took: 3.70908ms
Nov 26 08:26:49.102: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.363626ms
Nov 26 08:26:51.505: INFO: Number of nodes with available pods: 0
Nov 26 08:26:51.505: INFO: Number of running nodes: 0, number of available pods: 0
Nov 26 08:26:51.506: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116571"},"items":null}

Nov 26 08:26:51.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116571"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:26:51.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7194" for this suite.

• [SLOW TEST:10.728 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":196,"skipped":3420,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:26:51.518: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:26:51.577: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 26 08:26:51.587: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 26 08:26:56.610: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 26 08:26:56.610: INFO: Creating deployment "test-rolling-update-deployment"
Nov 26 08:26:56.618: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 26 08:26:56.639: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 26 08:26:58.643: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 26 08:26:58.645: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 08:26:58.649: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9986  7e3311ec-727f-4120-9be9-48040fe7479d 116690 1 2021-11-26 08:26:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[createdTime:2021-11-26T17:26:57.824698824+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:3546343826724305833 updatedTime:2021-11-26T17:26:57.824698824+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 2021-11-26 08:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a6dfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-26 08:26:56 +0000 UTC,LastTransitionTime:2021-11-26 08:26:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-11-26 08:26:58 +0000 UTC,LastTransitionTime:2021-11-26 08:26:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 26 08:26:58.651: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-9986  1b1fa1ce-8f3b-426a-8ed3-5fd8c7feea8b 116679 1 2021-11-26 08:26:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[createdTime:2021-11-26T17:26:57.824698824+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833 updatedTime:2021-11-26T17:26:57.824698824+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rolling-update-deployment 7e3311ec-727f-4120-9be9-48040fe7479d 0xc005ab8a07 0xc005ab8a08}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e3311ec-727f-4120-9be9-48040fe7479d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:26:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ab8ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:26:58.651: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 26 08:26:58.651: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9986  59d64568-8141-4ff1-b060-a5f29fde6efe 116689 2 2021-11-26 08:26:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[createdTime:2021-11-26T17:26:52.791114862+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832 updatedTime:2021-11-26T17:26:52.791114862+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-rolling-update-deployment 7e3311ec-727f-4120-9be9-48040fe7479d 0xc005ab8727 0xc005ab8728}] []  [{e2e.test Update apps/v1 2021-11-26 08:26:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e3311ec-727f-4120-9be9-48040fe7479d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:26:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ab8928 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:26:58.653: INFO: Pod "test-rolling-update-deployment-585b757574-l4td7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-l4td7 test-rolling-update-deployment-585b757574- deployment-9986  ff062785-8aae-49fa-a844-082dc743984b 116678 0 2021-11-26 08:26:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:ca8271b1499a225d7fbc38b265b333160a34fdb6caf651ea6716724bbdc9123a cni.projectcalico.org/podIP:10.244.35.167/32 cni.projectcalico.org/podIPs:10.244.35.167/32 createdTime:2021-11-26T17:26:57.852793111+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:26:57.852793111+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 1b1fa1ce-8f3b-426a-8ed3-5fd8c7feea8b 0xc005ab9357 0xc005ab9358}] []  [{kube-controller-manager Update v1 2021-11-26 08:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b1fa1ce-8f3b-426a-8ed3-5fd8c7feea8b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:26:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:26:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ccs2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ccs2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:26:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:26:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:26:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:26:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.167,StartTime:2021-11-26 08:26:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:26:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://74d7f81dc78a55fb209ee71f9be538da828b8c40dedd077838d84825e9dd93cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:26:58.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9986" for this suite.

• [SLOW TEST:7.140 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":197,"skipped":3427,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:26:58.657: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:26:58.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b" in namespace "projected-7066" to be "Succeeded or Failed"
Nov 26 08:26:58.731: INFO: Pod "downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.784696ms
Nov 26 08:27:00.764: INFO: Pod "downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039251052s
STEP: Saw pod success
Nov 26 08:27:00.764: INFO: Pod "downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b" satisfied condition "Succeeded or Failed"
Nov 26 08:27:00.766: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b container client-container: <nil>
STEP: delete the pod
Nov 26 08:27:00.785: INFO: Waiting for pod downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b to disappear
Nov 26 08:27:00.790: INFO: Pod downwardapi-volume-c061ee77-6280-4bd7-b0eb-e99e31269c8b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:27:00.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7066" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":198,"skipped":3443,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:27:00.795: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:27:04.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3298" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":199,"skipped":3443,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:27:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:27:25.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1918" for this suite.

• [SLOW TEST:21.268 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:27:25.718: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:27:25.793: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc" in namespace "downward-api-8046" to be "Succeeded or Failed"
Nov 26 08:27:25.805: INFO: Pod "downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.973103ms
Nov 26 08:27:27.809: INFO: Pod "downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016733078s
STEP: Saw pod success
Nov 26 08:27:27.809: INFO: Pod "downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc" satisfied condition "Succeeded or Failed"
Nov 26 08:27:27.811: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc container client-container: <nil>
STEP: delete the pod
Nov 26 08:27:27.867: INFO: Waiting for pod downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc to disappear
Nov 26 08:27:27.873: INFO: Pod downwardapi-volume-04b7e5fe-2714-44b4-bec7-d2f396f54dfc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:27:27.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8046" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3477,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:27:27.877: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Nov 26 08:27:27.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7420 create -f -'
Nov 26 08:27:28.264: INFO: stderr: ""
Nov 26 08:27:28.264: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 26 08:27:28.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7420 diff -f -'
Nov 26 08:27:28.541: INFO: rc: 1
Nov 26 08:27:28.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7420 delete -f -'
Nov 26 08:27:28.619: INFO: stderr: ""
Nov 26 08:27:28.619: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:27:28.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7420" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":202,"skipped":3480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:27:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 26 08:27:28.672: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 08:27:34.426: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:27:53.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4470" for this suite.

• [SLOW TEST:24.822 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":203,"skipped":3537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:27:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:28:04.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3316" for this suite.

• [SLOW TEST:11.231 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":204,"skipped":3563,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:28:04.682: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:30:00.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5854" for this suite.

• [SLOW TEST:116.094 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":205,"skipped":3567,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:30:00.776: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:30:14.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1705" for this suite.
STEP: Destroying namespace "nsdeletetest-49" for this suite.
Nov 26 08:30:14.039: INFO: Namespace nsdeletetest-49 was already deleted
STEP: Destroying namespace "nsdeletetest-8819" for this suite.

• [SLOW TEST:13.264 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":206,"skipped":3570,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:30:14.041: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 26 08:30:14.096: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 26 08:31:14.152: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Nov 26 08:31:14.177: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 26 08:31:14.184: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 26 08:31:14.206: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 26 08:31:14.218: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:31:24.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4991" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:70.264 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":207,"skipped":3570,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:31:24.305: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:31:24.363: INFO: Create a RollingUpdate DaemonSet
Nov 26 08:31:24.391: INFO: Check that daemon pods launch on every node of the cluster
Nov 26 08:31:24.402: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:24.408: INFO: Number of nodes with available pods: 0
Nov 26 08:31:24.408: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:31:25.426: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:25.428: INFO: Number of nodes with available pods: 0
Nov 26 08:31:25.428: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:31:26.411: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:26.413: INFO: Number of nodes with available pods: 2
Nov 26 08:31:26.413: INFO: Number of running nodes: 2, number of available pods: 2
Nov 26 08:31:26.413: INFO: Update the DaemonSet to trigger a rollout
Nov 26 08:31:26.417: INFO: Updating DaemonSet daemon-set
Nov 26 08:31:29.445: INFO: Roll back the DaemonSet before rollout is complete
Nov 26 08:31:29.455: INFO: Updating DaemonSet daemon-set
Nov 26 08:31:29.455: INFO: Make sure DaemonSet rollback is complete
Nov 26 08:31:29.488: INFO: Wrong image for pod: daemon-set-hfqbd. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Nov 26 08:31:29.488: INFO: Pod daemon-set-hfqbd is not available
Nov 26 08:31:29.496: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:30.513: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:31.513: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:32.501: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:33.501: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:34.502: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:35.501: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:36.501: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:37.500: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:31:38.499: INFO: Pod daemon-set-cdb5j is not available
Nov 26 08:31:38.501: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7404, will wait for the garbage collector to delete the pods
Nov 26 08:31:38.559: INFO: Deleting DaemonSet.extensions daemon-set took: 2.555611ms
Nov 26 08:31:38.660: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.064103ms
Nov 26 08:31:40.264: INFO: Number of nodes with available pods: 0
Nov 26 08:31:40.264: INFO: Number of running nodes: 0, number of available pods: 0
Nov 26 08:31:40.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"118879"},"items":null}

Nov 26 08:31:40.266: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"118879"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:31:40.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7404" for this suite.

• [SLOW TEST:15.990 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":208,"skipped":3576,"failed":0}
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:31:40.296: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 26 08:31:40.745: INFO: Pod name wrapped-volume-race-39f7cde5-1947-4ac3-aba9-1b0773bc0c51: Found 0 pods out of 5
Nov 26 08:31:45.759: INFO: Pod name wrapped-volume-race-39f7cde5-1947-4ac3-aba9-1b0773bc0c51: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-39f7cde5-1947-4ac3-aba9-1b0773bc0c51 in namespace emptydir-wrapper-5590, will wait for the garbage collector to delete the pods
Nov 26 08:31:55.848: INFO: Deleting ReplicationController wrapped-volume-race-39f7cde5-1947-4ac3-aba9-1b0773bc0c51 took: 3.04837ms
Nov 26 08:31:55.949: INFO: Terminating ReplicationController wrapped-volume-race-39f7cde5-1947-4ac3-aba9-1b0773bc0c51 pods took: 100.936133ms
STEP: Creating RC which spawns configmap-volume pods
Nov 26 08:31:59.488: INFO: Pod name wrapped-volume-race-a9d44fd2-853d-4292-8788-51c8d2ae0fa5: Found 0 pods out of 5
Nov 26 08:32:04.494: INFO: Pod name wrapped-volume-race-a9d44fd2-853d-4292-8788-51c8d2ae0fa5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a9d44fd2-853d-4292-8788-51c8d2ae0fa5 in namespace emptydir-wrapper-5590, will wait for the garbage collector to delete the pods
Nov 26 08:32:16.567: INFO: Deleting ReplicationController wrapped-volume-race-a9d44fd2-853d-4292-8788-51c8d2ae0fa5 took: 3.002424ms
Nov 26 08:32:16.668: INFO: Terminating ReplicationController wrapped-volume-race-a9d44fd2-853d-4292-8788-51c8d2ae0fa5 pods took: 100.574823ms
STEP: Creating RC which spawns configmap-volume pods
Nov 26 08:32:19.689: INFO: Pod name wrapped-volume-race-1b274d63-b19b-4577-90a2-b289d4e71a3d: Found 0 pods out of 5
Nov 26 08:32:24.718: INFO: Pod name wrapped-volume-race-1b274d63-b19b-4577-90a2-b289d4e71a3d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1b274d63-b19b-4577-90a2-b289d4e71a3d in namespace emptydir-wrapper-5590, will wait for the garbage collector to delete the pods
Nov 26 08:32:36.855: INFO: Deleting ReplicationController wrapped-volume-race-1b274d63-b19b-4577-90a2-b289d4e71a3d took: 66.526354ms
Nov 26 08:32:36.955: INFO: Terminating ReplicationController wrapped-volume-race-1b274d63-b19b-4577-90a2-b289d4e71a3d pods took: 100.5101ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:32:39.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5590" for this suite.

• [SLOW TEST:59.579 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":209,"skipped":3576,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:32:39.875: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 26 08:32:44.440: INFO: Successfully updated pod "adopt-release--1-w278q"
STEP: Checking that the Job readopts the Pod
Nov 26 08:32:44.440: INFO: Waiting up to 15m0s for pod "adopt-release--1-w278q" in namespace "job-9178" to be "adopted"
Nov 26 08:32:44.447: INFO: Pod "adopt-release--1-w278q": Phase="Running", Reason="", readiness=true. Elapsed: 7.234251ms
Nov 26 08:32:46.449: INFO: Pod "adopt-release--1-w278q": Phase="Running", Reason="", readiness=true. Elapsed: 2.00980806s
Nov 26 08:32:46.449: INFO: Pod "adopt-release--1-w278q" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 26 08:32:46.957: INFO: Successfully updated pod "adopt-release--1-w278q"
STEP: Checking that the Job releases the Pod
Nov 26 08:32:46.957: INFO: Waiting up to 15m0s for pod "adopt-release--1-w278q" in namespace "job-9178" to be "released"
Nov 26 08:32:46.967: INFO: Pod "adopt-release--1-w278q": Phase="Running", Reason="", readiness=true. Elapsed: 9.687169ms
Nov 26 08:32:48.969: INFO: Pod "adopt-release--1-w278q": Phase="Running", Reason="", readiness=true. Elapsed: 2.011951086s
Nov 26 08:32:48.969: INFO: Pod "adopt-release--1-w278q" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:32:48.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9178" for this suite.

• [SLOW TEST:9.098 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":210,"skipped":3580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:32:48.974: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4776, will wait for the garbage collector to delete the pods
Nov 26 08:32:51.099: INFO: Deleting Job.batch foo took: 3.703459ms
Nov 26 08:32:51.200: INFO: Terminating Job.batch foo pods took: 100.730172ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:23.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4776" for this suite.

• [SLOW TEST:34.734 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":211,"skipped":3626,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:23.708: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:33:23.776: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 26 08:33:28.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 create -f -'
Nov 26 08:33:30.285: INFO: stderr: ""
Nov 26 08:33:30.285: INFO: stdout: "e2e-test-crd-publish-openapi-7826-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 26 08:33:30.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 delete e2e-test-crd-publish-openapi-7826-crds test-foo'
Nov 26 08:33:30.351: INFO: stderr: ""
Nov 26 08:33:30.351: INFO: stdout: "e2e-test-crd-publish-openapi-7826-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 26 08:33:30.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 apply -f -'
Nov 26 08:33:30.588: INFO: stderr: ""
Nov 26 08:33:30.588: INFO: stdout: "e2e-test-crd-publish-openapi-7826-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 26 08:33:30.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 delete e2e-test-crd-publish-openapi-7826-crds test-foo'
Nov 26 08:33:30.643: INFO: stderr: ""
Nov 26 08:33:30.643: INFO: stdout: "e2e-test-crd-publish-openapi-7826-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 26 08:33:30.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 create -f -'
Nov 26 08:33:30.885: INFO: rc: 1
Nov 26 08:33:30.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 apply -f -'
Nov 26 08:33:31.122: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 26 08:33:31.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 create -f -'
Nov 26 08:33:31.338: INFO: rc: 1
Nov 26 08:33:31.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 --namespace=crd-publish-openapi-6403 apply -f -'
Nov 26 08:33:31.546: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 26 08:33:31.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-7826-crds'
Nov 26 08:33:31.788: INFO: stderr: ""
Nov 26 08:33:31.788: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7826-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 26 08:33:31.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-7826-crds.metadata'
Nov 26 08:33:32.051: INFO: stderr: ""
Nov 26 08:33:32.051: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7826-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 26 08:33:32.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-7826-crds.spec'
Nov 26 08:33:32.278: INFO: stderr: ""
Nov 26 08:33:32.278: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7826-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 26 08:33:32.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-7826-crds.spec.bars'
Nov 26 08:33:32.504: INFO: stderr: ""
Nov 26 08:33:32.504: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7826-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 26 08:33:32.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-6403 explain e2e-test-crd-publish-openapi-7826-crds.spec.bars2'
Nov 26 08:33:32.745: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6403" for this suite.

• [SLOW TEST:14.162 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":212,"skipped":3629,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:37.870: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:37.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-43" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":213,"skipped":3631,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:37.959: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 26 08:33:38.022: INFO: The status of Pod labelsupdate9445fb46-1cb0-47db-8f4a-5e92151770bc is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:33:40.025: INFO: The status of Pod labelsupdate9445fb46-1cb0-47db-8f4a-5e92151770bc is Running (Ready = true)
Nov 26 08:33:40.560: INFO: Successfully updated pod "labelsupdate9445fb46-1cb0-47db-8f4a-5e92151770bc"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:42.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9137" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":3635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:42.582: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:33:42.909: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:33:45.927: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:33:45.929: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:49.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4584" for this suite.
STEP: Destroying namespace "webhook-4584-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.498 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":215,"skipped":3751,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:49.081: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 26 08:33:49.181: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:49.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5746" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":216,"skipped":3769,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:49.208: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-2550/configmap-test-f35cc8a8-9cff-4aa6-9153-00ffafa98374
STEP: Creating a pod to test consume configMaps
Nov 26 08:33:49.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00" in namespace "configmap-2550" to be "Succeeded or Failed"
Nov 26 08:33:49.277: INFO: Pod "pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00": Phase="Pending", Reason="", readiness=false. Elapsed: 1.496453ms
Nov 26 08:33:51.281: INFO: Pod "pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005391291s
STEP: Saw pod success
Nov 26 08:33:51.281: INFO: Pod "pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00" satisfied condition "Succeeded or Failed"
Nov 26 08:33:51.282: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00 container env-test: <nil>
STEP: delete the pod
Nov 26 08:33:51.301: INFO: Waiting for pod pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00 to disappear
Nov 26 08:33:51.307: INFO: Pod pod-configmaps-0ddaa4ad-744f-4d39-8281-6ba7af7d0f00 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:51.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2550" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":3772,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:33:51.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3852" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":218,"skipped":3773,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:33:51.386: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4383
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Nov 26 08:33:51.438: INFO: Found 0 stateful pods, waiting for 3
Nov 26 08:34:01.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:34:01.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:34:01.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:34:01.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-4383 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:34:01.598: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:34:01.598: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:34:01.598: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Nov 26 08:34:11.624: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 26 08:34:21.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-4383 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:34:21.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:34:21.798: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:34:21.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Nov 26 08:34:31.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-4383 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:34:31.988: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:34:31.988: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:34:31.988: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:34:42.014: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 26 08:34:52.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-4383 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:34:52.213: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:34:52.213: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:34:52.213: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 08:35:02.226: INFO: Deleting all statefulset in ns statefulset-4383
Nov 26 08:35:02.228: INFO: Scaling statefulset ss2 to 0
Nov 26 08:35:12.248: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:35:12.280: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:12.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4383" for this suite.

• [SLOW TEST:80.913 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":219,"skipped":3782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:12.299: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:12.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8832" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":220,"skipped":3812,"failed":0}
SSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:12.378: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Nov 26 08:35:12.437: INFO: Major version: 1
STEP: Confirm minor version
Nov 26 08:35:12.437: INFO: cleanMinorVersion: 22
Nov 26 08:35:12.437: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:12.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7440" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":221,"skipped":3815,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:12.443: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-mxj7
STEP: Creating a pod to test atomic-volume-subpath
Nov 26 08:35:12.538: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mxj7" in namespace "subpath-1558" to be "Succeeded or Failed"
Nov 26 08:35:12.543: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392578ms
Nov 26 08:35:14.548: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010168094s
Nov 26 08:35:16.551: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013601366s
Nov 26 08:35:18.554: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 6.0164375s
Nov 26 08:35:20.558: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 8.020022875s
Nov 26 08:35:22.572: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 10.034116822s
Nov 26 08:35:24.577: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 12.038946134s
Nov 26 08:35:26.579: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 14.041580695s
Nov 26 08:35:28.583: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 16.044810676s
Nov 26 08:35:30.587: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 18.049196882s
Nov 26 08:35:32.591: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Running", Reason="", readiness=true. Elapsed: 20.053200888s
Nov 26 08:35:34.594: INFO: Pod "pod-subpath-test-projected-mxj7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056586088s
STEP: Saw pod success
Nov 26 08:35:34.594: INFO: Pod "pod-subpath-test-projected-mxj7" satisfied condition "Succeeded or Failed"
Nov 26 08:35:34.596: INFO: Trying to get logs from node cncf-node3 pod pod-subpath-test-projected-mxj7 container test-container-subpath-projected-mxj7: <nil>
STEP: delete the pod
Nov 26 08:35:34.630: INFO: Waiting for pod pod-subpath-test-projected-mxj7 to disappear
Nov 26 08:35:34.635: INFO: Pod pod-subpath-test-projected-mxj7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-mxj7
Nov 26 08:35:34.635: INFO: Deleting pod "pod-subpath-test-projected-mxj7" in namespace "subpath-1558"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:34.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1558" for this suite.

• [SLOW TEST:22.199 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":222,"skipped":3821,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:34.642: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:35:35.287: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:35:38.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5898" for this suite.
STEP: Destroying namespace "webhook-5898-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":223,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:38.466: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 26 08:35:38.599: INFO: Waiting up to 5m0s for pod "pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa" in namespace "emptydir-6496" to be "Succeeded or Failed"
Nov 26 08:35:38.602: INFO: Pod "pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.829515ms
Nov 26 08:35:40.606: INFO: Pod "pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006362698s
STEP: Saw pod success
Nov 26 08:35:40.606: INFO: Pod "pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa" satisfied condition "Succeeded or Failed"
Nov 26 08:35:40.607: INFO: Trying to get logs from node cncf-node3 pod pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa container test-container: <nil>
STEP: delete the pod
Nov 26 08:35:40.631: INFO: Waiting for pod pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa to disappear
Nov 26 08:35:40.637: INFO: Pod pod-0be7a3b9-28e5-4ea1-9c6b-dc2f32dc81aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:40.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6496" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 26 08:35:40.720: INFO: Waiting up to 5m0s for pod "pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5" in namespace "emptydir-2355" to be "Succeeded or Failed"
Nov 26 08:35:40.728: INFO: Pod "pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054941ms
Nov 26 08:35:42.731: INFO: Pod "pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01177569s
STEP: Saw pod success
Nov 26 08:35:42.731: INFO: Pod "pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5" satisfied condition "Succeeded or Failed"
Nov 26 08:35:42.733: INFO: Trying to get logs from node cncf-node3 pod pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5 container test-container: <nil>
STEP: delete the pod
Nov 26 08:35:42.757: INFO: Waiting for pod pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5 to disappear
Nov 26 08:35:42.763: INFO: Pod pod-8cec8e7f-ce64-46e5-94af-5734b7ce22d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:42.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2355" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:42.767: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-35863c5f-42a6-4b5b-9242-5c3e53666f2a
STEP: Creating a pod to test consume configMaps
Nov 26 08:35:42.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73" in namespace "configmap-6107" to be "Succeeded or Failed"
Nov 26 08:35:42.866: INFO: Pod "pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.263884ms
Nov 26 08:35:44.868: INFO: Pod "pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004822693s
Nov 26 08:35:46.872: INFO: Pod "pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008618117s
STEP: Saw pod success
Nov 26 08:35:46.872: INFO: Pod "pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73" satisfied condition "Succeeded or Failed"
Nov 26 08:35:46.874: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 08:35:46.895: INFO: Waiting for pod pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73 to disappear
Nov 26 08:35:46.905: INFO: Pod pod-configmaps-a6e7c8c8-aed1-46a0-bb81-5112b4a4ff73 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:46.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6107" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":226,"skipped":3925,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:46.924: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 26 08:35:46.981: INFO: The status of Pod pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:35:48.983: INFO: The status of Pod pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 26 08:35:49.493: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1"
Nov 26 08:35:49.493: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1" in namespace "pods-2499" to be "terminated due to deadline exceeded"
Nov 26 08:35:49.500: INFO: Pod "pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1": Phase="Running", Reason="", readiness=true. Elapsed: 7.142046ms
Nov 26 08:35:51.502: INFO: Pod "pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 2.009436447s
Nov 26 08:35:51.502: INFO: Pod "pod-update-activedeadlineseconds-a51554fc-f601-4fd1-b0b4-e123d9965be1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:51.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2499" for this suite.
•{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":3939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:51.507: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:35:51.923: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:35:54.953: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:35:54.955: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:58.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7462" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.663 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":228,"skipped":3967,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:35:58.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7891" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":229,"skipped":3971,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:35:58.266: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 26 08:35:58.311: INFO: Waiting up to 5m0s for pod "pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af" in namespace "emptydir-4876" to be "Succeeded or Failed"
Nov 26 08:35:58.314: INFO: Pod "pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.8286ms
Nov 26 08:36:00.318: INFO: Pod "pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006460366s
STEP: Saw pod success
Nov 26 08:36:00.318: INFO: Pod "pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af" satisfied condition "Succeeded or Failed"
Nov 26 08:36:00.319: INFO: Trying to get logs from node cncf-node3 pod pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af container test-container: <nil>
STEP: delete the pod
Nov 26 08:36:00.343: INFO: Waiting for pod pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af to disappear
Nov 26 08:36:00.349: INFO: Pod pod-8d077f90-f27d-4c3e-a8c2-1bf6159311af no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:36:00.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4876" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":3986,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:36:00.353: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:36:02.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5300" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":231,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:36:02.479: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Nov 26 08:36:02.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 create -f -'
Nov 26 08:36:02.998: INFO: stderr: ""
Nov 26 08:36:02.998: INFO: stdout: "pod/pause created\n"
Nov 26 08:36:02.998: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 26 08:36:02.998: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4290" to be "running and ready"
Nov 26 08:36:03.005: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919256ms
Nov 26 08:36:05.007: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009259831s
Nov 26 08:36:05.007: INFO: Pod "pause" satisfied condition "running and ready"
Nov 26 08:36:05.007: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 26 08:36:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 label pods pause testing-label=testing-label-value'
Nov 26 08:36:05.065: INFO: stderr: ""
Nov 26 08:36:05.065: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 26 08:36:05.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 get pod pause -L testing-label'
Nov 26 08:36:05.118: INFO: stderr: ""
Nov 26 08:36:05.118: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 26 08:36:05.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 label pods pause testing-label-'
Nov 26 08:36:05.179: INFO: stderr: ""
Nov 26 08:36:05.179: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 26 08:36:05.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 get pod pause -L testing-label'
Nov 26 08:36:05.229: INFO: stderr: ""
Nov 26 08:36:05.229: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Nov 26 08:36:05.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 delete --grace-period=0 --force -f -'
Nov 26 08:36:05.316: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:36:05.316: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 26 08:36:05.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 get rc,svc -l name=pause --no-headers'
Nov 26 08:36:05.380: INFO: stderr: "No resources found in kubectl-4290 namespace.\n"
Nov 26 08:36:05.380: INFO: stdout: ""
Nov 26 08:36:05.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-4290 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 26 08:36:05.435: INFO: stderr: ""
Nov 26 08:36:05.435: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:36:05.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4290" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":232,"skipped":4009,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:36:05.440: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 26 08:36:05.498: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 26 08:37:05.545: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 26 08:37:07.654: INFO: found a healthy node: cncf-node3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:37:13.761: INFO: pods created so far: [1 1 1]
Nov 26 08:37:13.761: INFO: length of pods created so far: 3
Nov 26 08:37:17.771: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8311" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:24.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1391" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:79.438 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":233,"skipped":4012,"failed":0}
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:24.878: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 26 08:37:25.773: INFO: starting watch
STEP: patching
STEP: updating
Nov 26 08:37:25.782: INFO: waiting for watch events with expected annotations
Nov 26 08:37:25.782: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:25.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2528" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":234,"skipped":4018,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:25.855: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:37:25.928: INFO: Waiting up to 5m0s for pod "downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd" in namespace "downward-api-4108" to be "Succeeded or Failed"
Nov 26 08:37:25.935: INFO: Pod "downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.799331ms
Nov 26 08:37:27.940: INFO: Pod "downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011513656s
STEP: Saw pod success
Nov 26 08:37:27.940: INFO: Pod "downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd" satisfied condition "Succeeded or Failed"
Nov 26 08:37:27.941: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd container client-container: <nil>
STEP: delete the pod
Nov 26 08:37:27.965: INFO: Waiting for pod downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd to disappear
Nov 26 08:37:27.971: INFO: Pod downwardapi-volume-799a45c8-d23e-400c-acd4-29bd984d2fdd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:27.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4108" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:27.976: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 26 08:37:28.064: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1554  962a17cb-3f9d-497c-bd23-78fb37695243 123006 0 2021-11-26 08:37:28 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[createdTime:2021-11-26T17:37:29.236127867+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:37:29.236127867+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:37:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:37:28.064: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1554  962a17cb-3f9d-497c-bd23-78fb37695243 123007 0 2021-11-26 08:37:28 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[createdTime:2021-11-26T17:37:29.236127867+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:37:29.236127867+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:37:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:28.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1554" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":236,"skipped":4095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:28.072: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:37:28.400: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:37:31.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:31.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4641" for this suite.
STEP: Destroying namespace "webhook-4641-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":237,"skipped":4184,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:31.562: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:37:31.685: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 26 08:37:31.693: INFO: Number of nodes with available pods: 0
Nov 26 08:37:31.693: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 26 08:37:31.712: INFO: Number of nodes with available pods: 0
Nov 26 08:37:31.712: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:37:32.716: INFO: Number of nodes with available pods: 0
Nov 26 08:37:32.716: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:37:33.715: INFO: Number of nodes with available pods: 1
Nov 26 08:37:33.715: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 26 08:37:33.730: INFO: Number of nodes with available pods: 1
Nov 26 08:37:33.730: INFO: Number of running nodes: 0, number of available pods: 1
Nov 26 08:37:34.734: INFO: Number of nodes with available pods: 0
Nov 26 08:37:34.734: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 26 08:37:34.748: INFO: Number of nodes with available pods: 0
Nov 26 08:37:34.748: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:37:35.750: INFO: Number of nodes with available pods: 0
Nov 26 08:37:35.750: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:37:36.751: INFO: Number of nodes with available pods: 0
Nov 26 08:37:36.751: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:37:37.751: INFO: Number of nodes with available pods: 1
Nov 26 08:37:37.751: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3140, will wait for the garbage collector to delete the pods
Nov 26 08:37:37.809: INFO: Deleting DaemonSet.extensions daemon-set took: 3.066682ms
Nov 26 08:37:37.909: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.146313ms
Nov 26 08:37:40.614: INFO: Number of nodes with available pods: 0
Nov 26 08:37:40.614: INFO: Number of running nodes: 0, number of available pods: 0
Nov 26 08:37:40.615: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"123316"},"items":null}

Nov 26 08:37:40.616: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"123316"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:40.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3140" for this suite.

• [SLOW TEST:9.076 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":238,"skipped":4189,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:40.639: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d6d74b4c-c2ce-4a0d-b816-b4cd36ca7f91
STEP: Creating a pod to test consume secrets
Nov 26 08:37:40.686: INFO: Waiting up to 5m0s for pod "pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079" in namespace "secrets-4423" to be "Succeeded or Failed"
Nov 26 08:37:40.731: INFO: Pod "pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079": Phase="Pending", Reason="", readiness=false. Elapsed: 44.986115ms
Nov 26 08:37:42.734: INFO: Pod "pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048280698s
STEP: Saw pod success
Nov 26 08:37:42.734: INFO: Pod "pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079" satisfied condition "Succeeded or Failed"
Nov 26 08:37:42.736: INFO: Trying to get logs from node cncf-node3 pod pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079 container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:37:42.758: INFO: Waiting for pod pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079 to disappear
Nov 26 08:37:42.763: INFO: Pod pod-secrets-3026c311-ee02-4cfd-b8dd-5cd28cda0079 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:42.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4423" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4202,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:42.768: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-96e9b117-12d1-40d0-9d9e-40c6d0397588
STEP: Creating a pod to test consume configMaps
Nov 26 08:37:42.857: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8" in namespace "projected-644" to be "Succeeded or Failed"
Nov 26 08:37:42.858: INFO: Pod "pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.587101ms
Nov 26 08:37:44.869: INFO: Pod "pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01249107s
Nov 26 08:37:46.872: INFO: Pod "pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015156864s
STEP: Saw pod success
Nov 26 08:37:46.872: INFO: Pod "pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8" satisfied condition "Succeeded or Failed"
Nov 26 08:37:46.873: INFO: Trying to get logs from node cncf-node3 pod pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 26 08:37:46.896: INFO: Waiting for pod pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8 to disappear
Nov 26 08:37:46.901: INFO: Pod pod-projected-configmaps-c8d02dd5-fa2b-49b9-900f-8db3654e2ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:46.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-644" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4222,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:46.906: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Nov 26 08:37:46.981: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 26 08:37:51.984: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Nov 26 08:37:51.986: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Nov 26 08:37:51.989: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Nov 26 08:37:51.991: INFO: Observed &ReplicaSet event: ADDED
Nov 26 08:37:51.991: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.991: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.991: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.991: INFO: Found replicaset test-rs in namespace replicaset-1550 with labels: map[name:sample-pod pod:httpd] annotations: map[createdTime:2021-11-26T17:37:48.191935972+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:37:48.191935972+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 26 08:37:51.991: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Nov 26 08:37:51.991: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 26 08:37:51.994: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Nov 26 08:37:51.995: INFO: Observed &ReplicaSet event: ADDED
Nov 26 08:37:51.995: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.995: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.995: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.995: INFO: Observed replicaset test-rs in namespace replicaset-1550 with annotations: map[createdTime:2021-11-26T17:37:48.191935972+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:37:48.191935972+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 26 08:37:51.995: INFO: Observed &ReplicaSet event: MODIFIED
Nov 26 08:37:51.995: INFO: Found replicaset test-rs in namespace replicaset-1550 with labels: map[name:sample-pod pod:httpd] annotations: map[createdTime:2021-11-26T17:37:48.191935972+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:37:48.191935972+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 26 08:37:51.995: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:51.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1550" for this suite.

• [SLOW TEST:5.103 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":241,"skipped":4223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:52.010: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:37:52.378: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:37:55.401: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:37:55.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9929" for this suite.
STEP: Destroying namespace "webhook-9929-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":242,"skipped":4277,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:37:55.606: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:37:55.665: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 26 08:38:01.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-8288 --namespace=crd-publish-openapi-8288 create -f -'
Nov 26 08:38:02.525: INFO: stderr: ""
Nov 26 08:38:02.525: INFO: stdout: "e2e-test-crd-publish-openapi-9020-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 26 08:38:02.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-8288 --namespace=crd-publish-openapi-8288 delete e2e-test-crd-publish-openapi-9020-crds test-cr'
Nov 26 08:38:02.584: INFO: stderr: ""
Nov 26 08:38:02.584: INFO: stdout: "e2e-test-crd-publish-openapi-9020-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 26 08:38:02.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-8288 --namespace=crd-publish-openapi-8288 apply -f -'
Nov 26 08:38:02.814: INFO: stderr: ""
Nov 26 08:38:02.814: INFO: stdout: "e2e-test-crd-publish-openapi-9020-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 26 08:38:02.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-8288 --namespace=crd-publish-openapi-8288 delete e2e-test-crd-publish-openapi-9020-crds test-cr'
Nov 26 08:38:02.870: INFO: stderr: ""
Nov 26 08:38:02.870: INFO: stdout: "e2e-test-crd-publish-openapi-9020-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 26 08:38:02.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-8288 explain e2e-test-crd-publish-openapi-9020-crds'
Nov 26 08:38:03.115: INFO: stderr: ""
Nov 26 08:38:03.115: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9020-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:38:08.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8288" for this suite.

• [SLOW TEST:12.594 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":243,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:38:08.201: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:38:08.244: INFO: Creating deployment "webserver-deployment"
Nov 26 08:38:08.248: INFO: Waiting for observed generation 1
Nov 26 08:38:10.262: INFO: Waiting for all required pods to come up
Nov 26 08:38:10.265: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 26 08:38:18.272: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 26 08:38:18.276: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 26 08:38:18.281: INFO: Updating deployment webserver-deployment
Nov 26 08:38:18.281: INFO: Waiting for observed generation 2
Nov 26 08:38:20.298: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 26 08:38:20.299: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 26 08:38:20.300: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 26 08:38:20.305: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 26 08:38:20.305: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 26 08:38:20.306: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 26 08:38:20.308: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 26 08:38:20.308: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 26 08:38:20.312: INFO: Updating deployment webserver-deployment
Nov 26 08:38:20.312: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 26 08:38:20.359: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 26 08:38:20.369: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 08:38:22.412: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8247  22b5c1bd-8a7a-4bba-aafa-4b22faeb6250 124054 3 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd] map[createdTime:2021-11-26T17:38:09.464508367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T17:38:09.464508367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e301c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-11-26 08:38:20 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-11-26 08:38:20 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 26 08:38:22.414: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8247  6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 124051 3 2021-11-26 08:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[createdTime:2021-11-26T17:38:09.464508367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T17:38:09.464508367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment webserver-deployment 22b5c1bd-8a7a-4bba-aafa-4b22faeb6250 0xc006e00d3e 0xc006e00d3f}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22b5c1bd-8a7a-4bba-aafa-4b22faeb6250\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e00e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:38:22.414: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 26 08:38:22.414: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-8247  7d6f0bb3-e179-435f-ab20-eff444996d8f 124050 3 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:09.464508367+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:38:09.464508367+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment webserver-deployment 22b5c1bd-8a7a-4bba-aafa-4b22faeb6250 0xc006e00e9e 0xc006e00e9f}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22b5c1bd-8a7a-4bba-aafa-4b22faeb6250\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:38:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e00fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:38:22.418: INFO: Pod "webserver-deployment-795d758f88-4xxms" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4xxms webserver-deployment-795d758f88- deployment-8247  e2a34f9f-d9e6-4b74-a4a2-b5baba768708 123963 0 2021-11-26 08:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:10b37b7d1b8df179a99bbd2fcef81474528ce651a530a30206f80404d3e3550c cni.projectcalico.org/podIP:10.244.35.133/32 cni.projectcalico.org/podIPs:10.244.35.133/32 createdTime:2021-11-26T17:38:19.516328793+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:19.516328793+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006db37a7 0xc006db37a8}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-26 08:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssppv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssppv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 08:38:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.418: INFO: Pod "webserver-deployment-795d758f88-7q2vs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7q2vs webserver-deployment-795d758f88- deployment-8247  402640d5-b124-4a49-bf97-0a8c1b5b5033 123987 0 2021-11-26 08:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:da81f42102d99894c7caae1cabaf1eef5b8931aed9b3fe00515beed7ebae25f7 cni.projectcalico.org/podIP:10.244.35.168/32 cni.projectcalico.org/podIPs:10.244.35.168/32 createdTime:2021-11-26T17:38:19.619085834+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:19.619085834+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006db3a3e 0xc006db3a3f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzkt4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzkt4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 08:38:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-db9s6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-db9s6 webserver-deployment-795d758f88- deployment-8247  d38d1b05-384b-4cbf-8a2b-db325cc7cdff 124091 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:cf6b767b0c66f753c52d4d5e1cfa8ddaf8d968f4b436fe7eb9615bd198578b04 cni.projectcalico.org/podIP:10.244.35.180/32 cni.projectcalico.org/podIPs:10.244.35.180/32 createdTime:2021-11-26T17:38:21.579667592+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.579667592+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006db3c8e 0xc006db3c8f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4gj9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4gj9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-kjwvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kjwvw webserver-deployment-795d758f88- deployment-8247  941cf469-29a9-4699-85ee-8776c354f796 123975 0 2021-11-26 08:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:c75a911e709f80674b82b0a7cbda1bf5a0d90bcd167484b0eb544be74beecd99 cni.projectcalico.org/podIP:10.244.35.141/32 cni.projectcalico.org/podIPs:10.244.35.141/32 createdTime:2021-11-26T17:38:19.521000233+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:19.521000233+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006db3ede 0xc006db3edf}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-26 08:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmmvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmmvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 08:38:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-mvgsb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mvgsb webserver-deployment-795d758f88- deployment-8247  6550c23a-0102-4819-ba54-fb01621a401b 124027 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[createdTime:2021-11-26T17:38:21.591149343+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591149343+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e5218e 0xc006e5218f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v985t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v985t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-n4kbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n4kbb webserver-deployment-795d758f88- deployment-8247  20eabdc1-b470-4c41-a4c0-5adac133ff9f 124112 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:81c1d74095dbe43ff78ad8d9ea2ce57d5ea97e59b5227e5b95476ef74ea409ac cni.projectcalico.org/podIP:10.244.35.182/32 cni.projectcalico.org/podIPs:10.244.35.182/32 createdTime:2021-11-26T17:38:21.591072757+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591072757+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e5239e 0xc006e5239f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2jqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2jqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-n58xq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n58xq webserver-deployment-795d758f88- deployment-8247  406ef083-181e-4979-be8b-2d9ea867b361 124011 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[createdTime:2021-11-26T17:38:21.579557802+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.579557802+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e5256e 0xc006e5256f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dqbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dqbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-n6tn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n6tn9 webserver-deployment-795d758f88- deployment-8247  a7a91ca1-b5d2-441d-a384-cb51d158a57d 123982 0 2021-11-26 08:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:6ba667ce8da34cb8f1b93262f31e94813b5a3e6f38bda07cb416591f7b693471 cni.projectcalico.org/podIP:10.244.35.137/32 cni.projectcalico.org/podIPs:10.244.35.137/32 createdTime:2021-11-26T17:38:19.521555275+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:19.521555275+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e5285e 0xc006e5285f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-26 08:38:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mx4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mx4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 08:38:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.419: INFO: Pod "webserver-deployment-795d758f88-njd7f" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-njd7f webserver-deployment-795d758f88- deployment-8247  8bf6035e-cbf5-4aa6-86dd-e7477c969611 124049 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[createdTime:2021-11-26T17:38:21.603528532+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.603528532+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e52b9e 0xc006e52b9f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4k5dw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4k5dw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-795d758f88-pmwt7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pmwt7 webserver-deployment-795d758f88- deployment-8247  a3663645-9a63-47ab-9667-a9c4caa1f661 124034 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[createdTime:2021-11-26T17:38:21.591805743+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591805743+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e52dce 0xc006e52dcf}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p255h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p255h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-795d758f88-pvcjh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pvcjh webserver-deployment-795d758f88- deployment-8247  d7f08276-64df-4d0e-a223-12d21b6c5c6f 124030 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[createdTime:2021-11-26T17:38:21.591432146+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591432146+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e52fde 0xc006e52fdf}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzxnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzxnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-795d758f88-vq6hd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vq6hd webserver-deployment-795d758f88- deployment-8247  a57f2713-5251-4648-a2a0-04ef4b09e8d5 124118 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:82be7f580673c64992478cf05b28a171b920807977cf91c9259b225f48e86610 cni.projectcalico.org/podIP:10.244.35.172/32 cni.projectcalico.org/podIPs:10.244.35.172/32 createdTime:2021-11-26T17:38:21.572383214+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.572383214+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e5319e 0xc006e5319f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twnk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twnk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-795d758f88-wd5bt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wd5bt webserver-deployment-795d758f88- deployment-8247  c1b75fee-e589-4c53-bccd-28af638d43e2 123994 0 2021-11-26 08:38:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:748978cf6532b25f37958d4c99813393faf00b01b3473a78621d9dcdcba3f74d cni.projectcalico.org/podIP:10.244.35.163/32 cni.projectcalico.org/podIPs:10.244.35.163/32 createdTime:2021-11-26T17:38:19.606717313+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:19.606717313+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 6c7dc9a5-ad6a-4f87-97d5-7611c03908c8 0xc006e5341e 0xc006e5341f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7dc9a5-ad6a-4f87-97d5-7611c03908c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:38:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wc4lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wc4lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:,StartTime:2021-11-26 08:38:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-847dcfb7fb-62nkh" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-62nkh webserver-deployment-847dcfb7fb- deployment-8247  e26860d6-d869-4aa8-a4e5-d58affa116d7 124028 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.591194951+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591194951+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e536ae 0xc006e536af}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dq27t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dq27t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-847dcfb7fb-6mnlz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6mnlz webserver-deployment-847dcfb7fb- deployment-8247  8f4e05be-dde4-47c8-a26d-1441fb10dd92 124045 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.603289727+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.603289727+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e5384e 0xc006e5384f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rsnjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rsnjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.420: INFO: Pod "webserver-deployment-847dcfb7fb-7t66p" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7t66p webserver-deployment-847dcfb7fb- deployment-8247  5e8be6e4-2557-4d6c-9474-1f2d6863334f 124102 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:bb09456b26c5eb96fc141bd13c8ba20de00fbf5246aadc458ad7850dcd1bdf5f cni.projectcalico.org/podIP:10.244.35.178/32 cni.projectcalico.org/podIPs:10.244.35.178/32 createdTime:2021-11-26T17:38:21.579754268+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.579754268+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e53a4e 0xc006e53a4f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdnqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdnqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-7tzlc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7tzlc webserver-deployment-847dcfb7fb- deployment-8247  b525d5fb-a8d3-4674-997f-f322eaf1566e 123870 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:292dce3dd3e3ad814eb4b70551d45aa229074d8f9c271c3788ff826f10079995 cni.projectcalico.org/podIP:10.244.35.129/32 cni.projectcalico.org/podIPs:10.244.35.129/32 createdTime:2021-11-26T17:38:09.544068157+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.544068157+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e53c7e 0xc006e53c7f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwjrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwjrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.129,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://87068cfad7092379fa8cd948f38a9cde1dd725925e16b77f2da33e005148a75e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-7x54b" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7x54b webserver-deployment-847dcfb7fb- deployment-8247  76057638-34a5-4109-a4cf-9ba838409f32 124095 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:b7212bb1b5e4c5584a97d8396fcf4823902335d00b25379565649cf57f563b6f cni.projectcalico.org/podIP:10.244.35.143/32 cni.projectcalico.org/podIPs:10.244.35.143/32 createdTime:2021-11-26T17:38:21.603462866+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.603462866+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e53ef7 0xc006e53ef8}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cs7qz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cs7qz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-86bjx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-86bjx webserver-deployment-847dcfb7fb- deployment-8247  796fedfc-e6bc-4431-a783-64577444dcb0 123854 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:b48c935d43b91239e7c5cbfaedee760381cc80378efb73696e8d7f230cbe32dc cni.projectcalico.org/podIP:10.244.35.130/32 cni.projectcalico.org/podIPs:10.244.35.130/32 createdTime:2021-11-26T17:38:09.533154347+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.533154347+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8a0be 0xc006e8a0bf}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdh6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdh6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.130,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://4897dcb264a7720848d2a847fe41c38a260493510040cb78c8052bfdcd488b8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-9w2jl" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9w2jl webserver-deployment-847dcfb7fb- deployment-8247  fa205e95-6ebd-4ff7-858d-5c7710c98577 123884 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:c60c50061f5b86d8fe8c075be9578ee3fcf52e5f079f0a4dc5145198f70f2044 cni.projectcalico.org/podIP:10.244.35.132/32 cni.projectcalico.org/podIPs:10.244.35.132/32 createdTime:2021-11-26T17:38:09.544420726+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.544420726+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8a377 0xc006e8a378}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2ml5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2ml5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.132,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://5cfb527fd33040a778b4ee5f756b4b12be39c1f5ee45c74453da1825d44829a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-dfkp8" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-dfkp8 webserver-deployment-847dcfb7fb- deployment-8247  27baee64-a98c-4635-82ce-1dd6c75d2d7e 123833 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:93b83bdf7ea18ddfc02c2727b73c1a099d3b2e25721f8a38a5ca2a93e3ee7b9e cni.projectcalico.org/podIP:10.244.35.161/32 cni.projectcalico.org/podIPs:10.244.35.161/32 createdTime:2021-11-26T17:38:09.544361378+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.544361378+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8a647 0xc006e8a648}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7p9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7p9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.161,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://cca1c0ec0c7725c2e0967389f36845ffda2b2d29eefb4777475696c792a98ffb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-dq9xp" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-dq9xp webserver-deployment-847dcfb7fb- deployment-8247  42c5ab0b-75a0-4027-a705-85e4faade974 124015 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.579930132+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.579930132+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8a917 0xc006e8a918}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdgmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdgmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.421: INFO: Pod "webserver-deployment-847dcfb7fb-hvm6v" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hvm6v webserver-deployment-847dcfb7fb- deployment-8247  18547462-eae7-4fd2-96c5-6e1aa7ef5d39 123841 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:dd47bb599de327ad866a838973fb3115fe68b62bcd54194e624989ba2438e345 cni.projectcalico.org/podIP:10.244.35.142/32 cni.projectcalico.org/podIPs:10.244.35.142/32 createdTime:2021-11-26T17:38:09.532970873+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.532970873+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8ab7e 0xc006e8ab7f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-522h8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-522h8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.142,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://9568e18d5d155fdff4430e23990dab36af04b6610db7fea3d31b94397d6d9c0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-jtfhs" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jtfhs webserver-deployment-847dcfb7fb- deployment-8247  e667a57e-23fb-45c7-8983-f855d1de1d8a 123895 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:16e996b7ceb32559893868c4a60ecfbaf19bec2cd36051d760f0c35fbb5b9363 cni.projectcalico.org/podIP:10.244.35.134/32 cni.projectcalico.org/podIPs:10.244.35.134/32 createdTime:2021-11-26T17:38:09.517342744+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.517342744+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8ae57 0xc006e8ae58}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtgb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtgb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.134,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://51147b26096879c5faff6015bd41f0e375a6482318c9933f4f04a36d2524b026,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-l7r7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l7r7b webserver-deployment-847dcfb7fb- deployment-8247  86cfec82-5f15-49d1-af18-85f6a4b6912d 124031 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.591347316+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591347316+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8b2a7 0xc006e8b2a8}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkckj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkckj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-l8dvb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l8dvb webserver-deployment-847dcfb7fb- deployment-8247  19b160dd-752e-4142-9552-7b6c4a6d9f54 123890 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:321b5ae437c0e9d1aa9d4d7e0734808978d186c57d460a7a6bcf52ddd5de8b7d cni.projectcalico.org/podIP:10.244.35.144/32 cni.projectcalico.org/podIPs:10.244.35.144/32 createdTime:2021-11-26T17:38:09.533470269+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.533470269+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8b50e 0xc006e8b50f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6f6v4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6f6v4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.144,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://230b1d6f471ec6fbb395a9e7dbc45fdb48d59487f9251722bebee9b2f4a725e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-mbrzs" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mbrzs webserver-deployment-847dcfb7fb- deployment-8247  eafa2d69-7b07-4db7-b1e3-ec97c5f7585b 124048 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.603636895+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.603636895+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8b7a7 0xc006e8b7a8}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cw6pf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cw6pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-mtkdl" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mtkdl webserver-deployment-847dcfb7fb- deployment-8247  2c410320-0440-42c9-bb82-902f1dc3f21d 124004 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.568682003+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.568682003+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8b93e 0xc006e8b93f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kztbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kztbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-q6htz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q6htz webserver-deployment-847dcfb7fb- deployment-8247  0be31a35-4746-4c9c-9163-0923c0f1566d 124046 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.603712469+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.603712469+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8bb2e 0xc006e8bb2f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fc7nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fc7nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.422: INFO: Pod "webserver-deployment-847dcfb7fb-ql6gc" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ql6gc webserver-deployment-847dcfb7fb- deployment-8247  9e05773e-e9a7-44f6-a168-cb4925152bf8 124032 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.591873754+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.591873754+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006e8bdde 0xc006e8bddf}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzr7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzr7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.423: INFO: Pod "webserver-deployment-847dcfb7fb-rvbxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rvbxs webserver-deployment-847dcfb7fb- deployment-8247  828f6458-db2c-48ba-89a1-5a35668b8a74 124119 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:8dd2e355b6717e96899003b2d81372e9e91dd58139eeae34e7c05a161311c5f7 cni.projectcalico.org/podIP:10.244.35.181/32 cni.projectcalico.org/podIPs:10.244.35.181/32 createdTime:2021-11-26T17:38:21.590756815+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.590756815+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006eb603e 0xc006eb603f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbqdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbqdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.423: INFO: Pod "webserver-deployment-847dcfb7fb-tqx6s" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tqx6s webserver-deployment-847dcfb7fb- deployment-8247  a5c13ba1-095c-4774-9511-b67309b6a906 123873 0 2021-11-26 08:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:9c3f62849e3a0e18fa40c42817455bacce89d9bd522ef3d4c2ab5a251d82aa2c cni.projectcalico.org/podIP:10.244.35.140/32 cni.projectcalico.org/podIPs:10.244.35.140/32 createdTime:2021-11-26T17:38:09.48282783+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:09.48282783+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006eb627e 0xc006eb627f}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:38:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:38:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x6s6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6s6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.140,StartTime:2021-11-26 08:38:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:38:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://339f72371e1af8ec0e9cfec56440e750a2862e632eeb5017ba903d707e610149,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 26 08:38:22.423: INFO: Pod "webserver-deployment-847dcfb7fb-zjfhm" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zjfhm webserver-deployment-847dcfb7fb- deployment-8247  c8759ee3-fe3f-461a-b9f3-948a59e562f7 124042 0 2021-11-26 08:38:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[createdTime:2021-11-26T17:38:21.602976114+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:38:21.602976114+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 7d6f0bb3-e179-435f-ab20-eff444996d8f 0xc006eb65d7 0xc006eb65d8}] []  [{kube-controller-manager Update v1 2021-11-26 08:38:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d6f0bb3-e179-435f-ab20-eff444996d8f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghhrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghhrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:38:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:38:22.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8247" for this suite.

• [SLOW TEST:14.227 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":244,"skipped":4295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:38:22.428: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-244.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-244.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 33.254.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.254.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.254.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.254.33_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-244.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-244.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-244.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-244.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-244.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 33.254.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.254.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.254.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.254.33_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 08:38:40.569: INFO: Unable to read wheezy_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.571: INFO: Unable to read wheezy_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.573: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.575: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.589: INFO: Unable to read jessie_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.593: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.594: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:40.606: INFO: Lookups using dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f failed for: [wheezy_udp@dns-test-service.dns-244.svc.cluster.local wheezy_tcp@dns-test-service.dns-244.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_udp@dns-test-service.dns-244.svc.cluster.local jessie_tcp@dns-test-service.dns-244.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local]

Nov 26 08:38:45.612: INFO: Unable to read wheezy_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.614: INFO: Unable to read wheezy_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.616: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.618: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.632: INFO: Unable to read jessie_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.634: INFO: Unable to read jessie_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.636: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.638: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:45.649: INFO: Lookups using dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f failed for: [wheezy_udp@dns-test-service.dns-244.svc.cluster.local wheezy_tcp@dns-test-service.dns-244.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_udp@dns-test-service.dns-244.svc.cluster.local jessie_tcp@dns-test-service.dns-244.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local]

Nov 26 08:38:50.609: INFO: Unable to read wheezy_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.611: INFO: Unable to read wheezy_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.613: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.616: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.628: INFO: Unable to read jessie_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.632: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.634: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:50.645: INFO: Lookups using dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f failed for: [wheezy_udp@dns-test-service.dns-244.svc.cluster.local wheezy_tcp@dns-test-service.dns-244.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_udp@dns-test-service.dns-244.svc.cluster.local jessie_tcp@dns-test-service.dns-244.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local]

Nov 26 08:38:55.613: INFO: Unable to read wheezy_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.615: INFO: Unable to read wheezy_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.617: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.620: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.635: INFO: Unable to read jessie_udp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.637: INFO: Unable to read jessie_tcp@dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.640: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.642: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local from pod dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f: the server could not find the requested resource (get pods dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f)
Nov 26 08:38:55.655: INFO: Lookups using dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f failed for: [wheezy_udp@dns-test-service.dns-244.svc.cluster.local wheezy_tcp@dns-test-service.dns-244.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_udp@dns-test-service.dns-244.svc.cluster.local jessie_tcp@dns-test-service.dns-244.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-244.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-244.svc.cluster.local]

Nov 26 08:39:00.646: INFO: DNS probes using dns-244/dns-test-77889f4d-f9c2-4d45-b926-4fe1e2407f4f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:39:00.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-244" for this suite.

• [SLOW TEST:38.322 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":245,"skipped":4318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:39:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8086
STEP: creating service affinity-clusterip in namespace services-8086
STEP: creating replication controller affinity-clusterip in namespace services-8086
I1126 08:39:00.807105      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-8086, replica count: 3
I1126 08:39:03.857587      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1126 08:39:06.858341      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:39:06.862: INFO: Creating new exec pod
Nov 26 08:39:09.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-8086 exec execpod-affinityfxbp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Nov 26 08:39:10.040: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 26 08:39:10.040: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:39:10.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-8086 exec execpod-affinityfxbp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.216.95 80'
Nov 26 08:39:10.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.216.95 80\nConnection to 10.96.216.95 80 port [tcp/http] succeeded!\n"
Nov 26 08:39:10.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:39:10.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-8086 exec execpod-affinityfxbp2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.216.95:80/ ; done'
Nov 26 08:39:10.427: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.216.95:80/\n"
Nov 26 08:39:10.427: INFO: stdout: "\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8\naffinity-clusterip-l8jz8"
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Received response from host: affinity-clusterip-l8jz8
Nov 26 08:39:10.427: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8086, will wait for the garbage collector to delete the pods
Nov 26 08:39:10.521: INFO: Deleting ReplicationController affinity-clusterip took: 2.438096ms
Nov 26 08:39:10.622: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.069055ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:39:12.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8086" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.130 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":246,"skipped":4367,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:39:12.881: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 26 08:39:12.922: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 26 08:40:12.963: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:40:12.965: INFO: Starting informer...
STEP: Starting pod...
Nov 26 08:40:13.175: INFO: Pod is running on cncf-node3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 26 08:40:13.188: INFO: Pod wasn't evicted. Proceeding
Nov 26 08:40:13.188: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 26 08:41:28.223: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:41:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9195" for this suite.

• [SLOW TEST:135.352 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":247,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:41:28.233: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 26 08:41:28.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2620 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 26 08:41:28.360: INFO: stderr: ""
Nov 26 08:41:28.360: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 26 08:41:28.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2620 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Nov 26 08:41:28.740: INFO: stderr: ""
Nov 26 08:41:28.740: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 26 08:41:28.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-2620 delete pods e2e-test-httpd-pod'
Nov 26 08:41:31.100: INFO: stderr: ""
Nov 26 08:41:31.100: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:41:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2620" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":248,"skipped":4414,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:41:31.107: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-0395683e-0ffe-4b08-8c6a-e482a5b60cb2 in namespace container-probe-3808
Nov 26 08:41:33.200: INFO: Started pod busybox-0395683e-0ffe-4b08-8c6a-e482a5b60cb2 in namespace container-probe-3808
STEP: checking the pod's current state and verifying that restartCount is present
Nov 26 08:41:33.202: INFO: Initial restart count of pod busybox-0395683e-0ffe-4b08-8c6a-e482a5b60cb2 is 0
Nov 26 08:42:23.341: INFO: Restart count of pod container-probe-3808/busybox-0395683e-0ffe-4b08-8c6a-e482a5b60cb2 is now 1 (50.13940869s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:42:23.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3808" for this suite.

• [SLOW TEST:52.254 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:42:23.361: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-v9lg
STEP: Creating a pod to test atomic-volume-subpath
Nov 26 08:42:23.440: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-v9lg" in namespace "subpath-5446" to be "Succeeded or Failed"
Nov 26 08:42:23.446: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.645524ms
Nov 26 08:42:25.449: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 2.008671895s
Nov 26 08:42:27.452: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 4.011784809s
Nov 26 08:42:29.457: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 6.016659373s
Nov 26 08:42:31.459: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 8.018991723s
Nov 26 08:42:33.463: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 10.022790394s
Nov 26 08:42:35.466: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 12.025772264s
Nov 26 08:42:37.470: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 14.029623094s
Nov 26 08:42:39.475: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 16.035149545s
Nov 26 08:42:41.478: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 18.037964619s
Nov 26 08:42:43.483: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Running", Reason="", readiness=true. Elapsed: 20.04262981s
Nov 26 08:42:45.493: INFO: Pod "pod-subpath-test-secret-v9lg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052550231s
STEP: Saw pod success
Nov 26 08:42:45.493: INFO: Pod "pod-subpath-test-secret-v9lg" satisfied condition "Succeeded or Failed"
Nov 26 08:42:45.494: INFO: Trying to get logs from node cncf-node3 pod pod-subpath-test-secret-v9lg container test-container-subpath-secret-v9lg: <nil>
STEP: delete the pod
Nov 26 08:42:45.524: INFO: Waiting for pod pod-subpath-test-secret-v9lg to disappear
Nov 26 08:42:45.529: INFO: Pod pod-subpath-test-secret-v9lg no longer exists
STEP: Deleting pod pod-subpath-test-secret-v9lg
Nov 26 08:42:45.529: INFO: Deleting pod "pod-subpath-test-secret-v9lg" in namespace "subpath-5446"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:42:45.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5446" for this suite.

• [SLOW TEST:22.174 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":250,"skipped":4453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:42:45.535: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-8a621144-d3b4-427c-92ee-bde5d3920526
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:42:45.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8340" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":251,"skipped":4489,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:42:45.593: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3046
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3046
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3046
Nov 26 08:42:45.652: INFO: Found 0 stateful pods, waiting for 1
Nov 26 08:42:55.656: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 26 08:42:55.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:42:55.805: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:42:55.805: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:42:55.805: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:42:55.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 26 08:43:05.814: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:43:05.814: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:43:05.831: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Nov 26 08:43:05.831: INFO: ss-0  cncf-node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:42:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:42:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:42:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:42:45 +0000 UTC  }]
Nov 26 08:43:05.831: INFO: 
Nov 26 08:43:05.831: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 26 08:43:06.840: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98824519s
Nov 26 08:43:07.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979394259s
Nov 26 08:43:08.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975003818s
Nov 26 08:43:09.856: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967436962s
Nov 26 08:43:10.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964422608s
Nov 26 08:43:11.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961282303s
Nov 26 08:43:12.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956424053s
Nov 26 08:43:13.871: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952848772s
Nov 26 08:43:14.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.43413ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3046
Nov 26 08:43:15.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:43:16.046: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:43:16.046: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:43:16.046: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:43:16.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:43:16.189: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 26 08:43:16.189: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:43:16.189: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:43:16.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:43:16.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 26 08:43:16.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:43:16.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:43:16.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 26 08:43:26.339: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:43:26.339: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:43:26.339: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 26 08:43:26.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:43:26.510: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:43:26.510: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:43:26.510: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:43:26.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:43:26.660: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:43:26.661: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:43:26.661: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:43:26.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-3046 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:43:26.799: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:43:26.799: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:43:26.799: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:43:26.799: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:43:26.801: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 26 08:43:36.807: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:43:36.807: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:43:36.807: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:43:36.824: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Nov 26 08:43:36.824: INFO: ss-0  cncf-node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:42:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:42:45 +0000 UTC  }]
Nov 26 08:43:36.824: INFO: ss-1  cncf-node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:05 +0000 UTC  }]
Nov 26 08:43:36.824: INFO: ss-2  cncf-node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:05 +0000 UTC  }]
Nov 26 08:43:36.824: INFO: 
Nov 26 08:43:36.824: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 26 08:43:37.833: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Nov 26 08:43:37.833: INFO: ss-1  cncf-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:43:05 +0000 UTC  }]
Nov 26 08:43:37.833: INFO: 
Nov 26 08:43:37.833: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 26 08:43:38.838: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.980744134s
Nov 26 08:43:39.841: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974759023s
Nov 26 08:43:40.844: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972174844s
Nov 26 08:43:41.845: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.969891803s
Nov 26 08:43:42.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.967573329s
Nov 26 08:43:43.850: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.96585628s
Nov 26 08:43:44.853: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963772471s
Nov 26 08:43:45.855: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.605132ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3046
Nov 26 08:43:46.860: INFO: Scaling statefulset ss to 0
Nov 26 08:43:46.867: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 08:43:46.869: INFO: Deleting all statefulset in ns statefulset-3046
Nov 26 08:43:46.870: INFO: Scaling statefulset ss to 0
Nov 26 08:43:46.876: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:43:46.878: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:43:46.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3046" for this suite.

• [SLOW TEST:61.309 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":252,"skipped":4507,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:43:46.902: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:43:47.407: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:43:50.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:43:50.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7571" for this suite.
STEP: Destroying namespace "webhook-7571-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":253,"skipped":4509,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:43:50.539: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:43:51.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:43:54.097: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 26 08:43:57.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=webhook-7821 attach --namespace=webhook-7821 to-be-attached-pod -i -c=container1'
Nov 26 08:43:57.262: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:43:57.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7821" for this suite.
STEP: Destroying namespace "webhook-7821-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":254,"skipped":4512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:43:57.348: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:43:57.421: INFO: The status of Pod pod-secrets-e948bcfa-2e0a-4987-af9e-de748ab5604f is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:43:59.423: INFO: The status of Pod pod-secrets-e948bcfa-2e0a-4987-af9e-de748ab5604f is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:43:59.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5928" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":255,"skipped":4547,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:43:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:43:59.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-545" for this suite.
STEP: Destroying namespace "nspatchtest-cdc5cbbf-205b-40fd-ac26-dade7ddeb7a2-9896" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":256,"skipped":4566,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:43:59.605: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:43:59.666: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558" in namespace "downward-api-1633" to be "Succeeded or Failed"
Nov 26 08:43:59.673: INFO: Pod "downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558": Phase="Pending", Reason="", readiness=false. Elapsed: 6.697282ms
Nov 26 08:44:01.675: INFO: Pod "downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008771853s
STEP: Saw pod success
Nov 26 08:44:01.675: INFO: Pod "downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558" satisfied condition "Succeeded or Failed"
Nov 26 08:44:01.677: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558 container client-container: <nil>
STEP: delete the pod
Nov 26 08:44:01.696: INFO: Waiting for pod downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558 to disappear
Nov 26 08:44:01.702: INFO: Pod downwardapi-volume-f8fcee06-9f63-4fd2-8a41-4b8525d59558 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:01.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1633" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4572,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:01.737: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Nov 26 08:44:01.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 create -f -'
Nov 26 08:44:02.030: INFO: stderr: ""
Nov 26 08:44:02.030: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 26 08:44:02.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 08:44:02.088: INFO: stderr: ""
Nov 26 08:44:02.088: INFO: stdout: "update-demo-nautilus-5ch7p update-demo-nautilus-br5hw "
Nov 26 08:44:02.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods update-demo-nautilus-5ch7p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 08:44:02.160: INFO: stderr: ""
Nov 26 08:44:02.160: INFO: stdout: ""
Nov 26 08:44:02.160: INFO: update-demo-nautilus-5ch7p is created but not running
Nov 26 08:44:07.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 26 08:44:07.215: INFO: stderr: ""
Nov 26 08:44:07.215: INFO: stdout: "update-demo-nautilus-5ch7p update-demo-nautilus-br5hw "
Nov 26 08:44:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods update-demo-nautilus-5ch7p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 08:44:07.285: INFO: stderr: ""
Nov 26 08:44:07.285: INFO: stdout: "true"
Nov 26 08:44:07.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods update-demo-nautilus-5ch7p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 08:44:07.334: INFO: stderr: ""
Nov 26 08:44:07.334: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 08:44:07.334: INFO: validating pod update-demo-nautilus-5ch7p
Nov 26 08:44:07.337: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 08:44:07.337: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 08:44:07.337: INFO: update-demo-nautilus-5ch7p is verified up and running
Nov 26 08:44:07.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods update-demo-nautilus-br5hw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 26 08:44:07.386: INFO: stderr: ""
Nov 26 08:44:07.386: INFO: stdout: "true"
Nov 26 08:44:07.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods update-demo-nautilus-br5hw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 26 08:44:07.435: INFO: stderr: ""
Nov 26 08:44:07.435: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 26 08:44:07.435: INFO: validating pod update-demo-nautilus-br5hw
Nov 26 08:44:07.439: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 26 08:44:07.439: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 26 08:44:07.439: INFO: update-demo-nautilus-br5hw is verified up and running
STEP: using delete to clean up resources
Nov 26 08:44:07.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 delete --grace-period=0 --force -f -'
Nov 26 08:44:07.493: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:44:07.493: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 26 08:44:07.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get rc,svc -l name=update-demo --no-headers'
Nov 26 08:44:07.549: INFO: stderr: "No resources found in kubectl-3817 namespace.\n"
Nov 26 08:44:07.549: INFO: stdout: ""
Nov 26 08:44:07.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3817 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 26 08:44:07.608: INFO: stderr: ""
Nov 26 08:44:07.608: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3817" for this suite.

• [SLOW TEST:5.877 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":258,"skipped":4575,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:07.614: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 26 08:44:09.682: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:09.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6998" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":4581,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:09.732: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9123.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9123.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9123.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 08:44:11.866: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.868: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.870: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.872: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.879: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.880: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.882: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.884: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:11.888: INFO: Lookups using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local]

Nov 26 08:44:16.892: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.894: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.896: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.898: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.905: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.907: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.908: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.910: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:16.914: INFO: Lookups using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local]

Nov 26 08:44:21.891: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.906: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.908: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.910: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.916: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.918: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.920: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.922: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:21.926: INFO: Lookups using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local]

Nov 26 08:44:26.892: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.894: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.897: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.899: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.907: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.909: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.911: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.913: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:26.918: INFO: Lookups using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local]

Nov 26 08:44:31.891: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.894: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.896: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.898: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.906: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.908: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:31.913: INFO: Lookups using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local]

Nov 26 08:44:36.891: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.894: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.896: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.899: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.906: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.908: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.910: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.912: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local from pod dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30: the server could not find the requested resource (get pods dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30)
Nov 26 08:44:36.916: INFO: Lookups using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9123.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9123.svc.cluster.local jessie_udp@dns-test-service-2.dns-9123.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9123.svc.cluster.local]

Nov 26 08:44:41.938: INFO: DNS probes using dns-9123/dns-test-f6861352-3b83-433e-97b5-1d8341dc3f30 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:42.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9123" for this suite.

• [SLOW TEST:32.292 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":260,"skipped":4586,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:42.025: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:44:42.067: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:42.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8683" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":261,"skipped":4599,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:42.644: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-d62ecd30-59ab-45e7-b184-0a8833cd0325
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:42.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-623" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":262,"skipped":4612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:42.727: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Nov 26 08:44:44.826: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:50.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4354" for this suite.

• [SLOW TEST:8.347 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":263,"skipped":4643,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:51.074: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 26 08:44:51.183: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8334" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":264,"skipped":4650,"failed":0}
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:53.705: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:53.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2871" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":265,"skipped":4652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:53.811: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 26 08:44:53.897: INFO: starting watch
STEP: patching
STEP: updating
Nov 26 08:44:53.940: INFO: waiting for watch events with expected annotations
Nov 26 08:44:53.940: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:44:54.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2317" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":266,"skipped":4676,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:44:54.014: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 26 08:45:04.244: INFO: The status of Pod kube-controller-manager-cncf-node1 is Running (Ready = true)
Nov 26 08:45:04.527: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 26 08:45:04.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vqvk" in namespace "gc-7672"
Nov 26 08:45:04.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gkw8" in namespace "gc-7672"
Nov 26 08:45:04.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k2rb" in namespace "gc-7672"
Nov 26 08:45:04.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbjpp" in namespace "gc-7672"
Nov 26 08:45:04.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcnfg" in namespace "gc-7672"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:04.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7672" for this suite.

• [SLOW TEST:10.665 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":267,"skipped":4679,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:04.679: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:45:04.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400" in namespace "projected-5769" to be "Succeeded or Failed"
Nov 26 08:45:04.757: INFO: Pod "downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400": Phase="Pending", Reason="", readiness=false. Elapsed: 31.184689ms
Nov 26 08:45:06.759: INFO: Pod "downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033625888s
Nov 26 08:45:08.762: INFO: Pod "downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036888115s
Nov 26 08:45:10.766: INFO: Pod "downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040542112s
STEP: Saw pod success
Nov 26 08:45:10.766: INFO: Pod "downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400" satisfied condition "Succeeded or Failed"
Nov 26 08:45:10.768: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400 container client-container: <nil>
STEP: delete the pod
Nov 26 08:45:10.786: INFO: Waiting for pod downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400 to disappear
Nov 26 08:45:10.791: INFO: Pod downwardapi-volume-ec92612e-5f48-45cf-b3f5-e0c5ef946400 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:10.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5769" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":4698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:10.796: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1235
STEP: creating service affinity-nodeport in namespace services-1235
STEP: creating replication controller affinity-nodeport in namespace services-1235
I1126 08:45:10.849182      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1235, replica count: 3
I1126 08:45:13.900955      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1126 08:45:16.901160      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:45:16.907: INFO: Creating new exec pod
Nov 26 08:45:19.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1235 exec execpod-affinityl6x2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Nov 26 08:45:20.101: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 26 08:45:20.101: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:45:20.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1235 exec execpod-affinityl6x2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.236.98 80'
Nov 26 08:45:20.253: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.236.98 80\nConnection to 10.96.236.98 80 port [tcp/http] succeeded!\n"
Nov 26 08:45:20.253: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:45:20.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1235 exec execpod-affinityl6x2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.8 30241'
Nov 26 08:45:20.390: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.8 30241\nConnection to 172.21.7.8 30241 port [tcp/*] succeeded!\n"
Nov 26 08:45:20.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:45:20.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1235 exec execpod-affinityl6x2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.12 30241'
Nov 26 08:45:20.542: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.12 30241\nConnection to 172.21.7.12 30241 port [tcp/*] succeeded!\n"
Nov 26 08:45:20.542: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:45:20.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1235 exec execpod-affinityl6x2h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.8:30241/ ; done'
Nov 26 08:45:20.794: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:30241/\n"
Nov 26 08:45:20.794: INFO: stdout: "\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74\naffinity-nodeport-sbv74"
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Received response from host: affinity-nodeport-sbv74
Nov 26 08:45:20.794: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1235, will wait for the garbage collector to delete the pods
Nov 26 08:45:20.865: INFO: Deleting ReplicationController affinity-nodeport took: 2.625619ms
Nov 26 08:45:20.966: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.349848ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:23.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1235" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.241 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":269,"skipped":4730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:23.037: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 26 08:45:23.073: INFO: The status of Pod annotationupdate9354af78-6328-4da3-9b61-ba0ad12a93bc is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:45:25.077: INFO: The status of Pod annotationupdate9354af78-6328-4da3-9b61-ba0ad12a93bc is Running (Ready = true)
Nov 26 08:45:25.590: INFO: Successfully updated pod "annotationupdate9354af78-6328-4da3-9b61-ba0ad12a93bc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:27.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3898" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4757,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:27.616: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 26 08:45:27.667: INFO: Waiting up to 5m0s for pod "downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab" in namespace "downward-api-4027" to be "Succeeded or Failed"
Nov 26 08:45:27.670: INFO: Pod "downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195274ms
Nov 26 08:45:29.676: INFO: Pod "downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008543316s
Nov 26 08:45:31.678: INFO: Pod "downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011090738s
STEP: Saw pod success
Nov 26 08:45:31.678: INFO: Pod "downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab" satisfied condition "Succeeded or Failed"
Nov 26 08:45:31.680: INFO: Trying to get logs from node cncf-node3 pod downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab container dapi-container: <nil>
STEP: delete the pod
Nov 26 08:45:31.699: INFO: Waiting for pod downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab to disappear
Nov 26 08:45:31.703: INFO: Pod downward-api-00804bb7-d779-4c4d-a4aa-64f44cee0dab no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:31.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4027" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":4757,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:31.707: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ac0181ed-c475-4ddf-9303-c9a0f73d1f6d
STEP: Creating a pod to test consume configMaps
Nov 26 08:45:31.786: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787" in namespace "projected-9561" to be "Succeeded or Failed"
Nov 26 08:45:31.797: INFO: Pod "pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787": Phase="Pending", Reason="", readiness=false. Elapsed: 10.316926ms
Nov 26 08:45:33.894: INFO: Pod "pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.107535561s
STEP: Saw pod success
Nov 26 08:45:33.894: INFO: Pod "pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787" satisfied condition "Succeeded or Failed"
Nov 26 08:45:33.896: INFO: Trying to get logs from node cncf-node3 pod pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 08:45:34.049: INFO: Waiting for pod pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787 to disappear
Nov 26 08:45:34.071: INFO: Pod pod-projected-configmaps-9d476de8-5464-4a9c-9588-737074244787 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9561" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":4775,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-431523d8-c326-46e4-be96-9582ed516ac0
STEP: Creating a pod to test consume secrets
Nov 26 08:45:34.130: INFO: Waiting up to 5m0s for pod "pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a" in namespace "secrets-9380" to be "Succeeded or Failed"
Nov 26 08:45:34.132: INFO: Pod "pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073323ms
Nov 26 08:45:36.138: INFO: Pod "pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008373172s
STEP: Saw pod success
Nov 26 08:45:36.138: INFO: Pod "pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a" satisfied condition "Succeeded or Failed"
Nov 26 08:45:36.140: INFO: Trying to get logs from node cncf-node3 pod pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:45:36.161: INFO: Waiting for pod pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a to disappear
Nov 26 08:45:36.167: INFO: Pod pod-secrets-b13044e6-b7cc-477c-b721-5f903b56ed1a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:36.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9380" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":4788,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:36.171: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:45:36.252: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 26 08:45:41.256: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 26 08:45:41.256: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 08:45:43.362: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-986  c9c465e5-65ba-4c56-8d4a-de455270d762 128794 1 2021-11-26 08:45:41 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[createdTime:2021-11-26T17:45:42.483105558+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:45:42.483105558+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update apps/v1 2021-11-26 08:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00326b4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-26 08:45:41 +0000 UTC,LastTransitionTime:2021-11-26 08:45:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2021-11-26 08:45:42 +0000 UTC,LastTransitionTime:2021-11-26 08:45:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 26 08:45:43.368: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-986  924b54a3-1c6c-4b03-a386-81253372ad47 128784 1 2021-11-26 08:45:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[createdTime:2021-11-26T17:45:42.483105558+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:45:42.483105558+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-cleanup-deployment c9c465e5-65ba-4c56-8d4a-de455270d762 0xc00326be87 0xc00326be88}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9c465e5-65ba-4c56-8d4a-de455270d762\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:45:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005018058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 26 08:45:43.370: INFO: Pod "test-cleanup-deployment-5b4d99b59b-g4mxl" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-g4mxl test-cleanup-deployment-5b4d99b59b- deployment-986  b540f816-47e6-4111-a229-80379cec9b6d 128783 0 2021-11-26 08:45:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[cni.projectcalico.org/containerID:6ecb1eb19577c8f2e663e3245f592d4f8b9f4d654a228ca2c6f47ecdb998c14a cni.projectcalico.org/podIP:10.244.35.140/32 cni.projectcalico.org/podIPs:10.244.35.140/32 createdTime:2021-11-26T17:45:42.565274987+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:45:42.565274987+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 924b54a3-1c6c-4b03-a386-81253372ad47 0xc005452987 0xc005452988}] []  [{kube-controller-manager Update v1 2021-11-26 08:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"924b54a3-1c6c-4b03-a386-81253372ad47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfvbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfvbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:45:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:45:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:45:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.140,StartTime:2021-11-26 08:45:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:45:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://c099d8ee449f086866355526b4578d8cbeb7e88a2743ef181b1d54db0ef89bae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:45:43.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-986" for this suite.

• [SLOW TEST:7.203 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":274,"skipped":4790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:45:43.374: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2690
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 26 08:45:43.417: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 26 08:45:43.449: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:45:45.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:47.453: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:49.453: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:51.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:53.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:55.454: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:57.454: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:45:59.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:46:01.452: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:46:03.453: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:46:05.453: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 26 08:46:05.457: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 26 08:46:07.487: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Nov 26 08:46:07.487: INFO: Breadth first check of 10.244.89.113 on host 172.21.7.8...
Nov 26 08:46:07.488: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.35.137:9080/dial?request=hostname&protocol=http&host=10.244.89.113&port=8083&tries=1'] Namespace:pod-network-test-2690 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:46:07.489: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 08:46:07.601: INFO: Waiting for responses: map[]
Nov 26 08:46:07.601: INFO: reached 10.244.89.113 after 0/1 tries
Nov 26 08:46:07.601: INFO: Breadth first check of 10.244.35.141 on host 172.21.7.12...
Nov 26 08:46:07.603: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.35.137:9080/dial?request=hostname&protocol=http&host=10.244.35.141&port=8083&tries=1'] Namespace:pod-network-test-2690 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:46:07.603: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 08:46:07.710: INFO: Waiting for responses: map[]
Nov 26 08:46:07.710: INFO: reached 10.244.35.141 after 0/1 tries
Nov 26 08:46:07.710: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:07.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2690" for this suite.

• [SLOW TEST:24.342 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:07.717: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6106703b-bf2e-4737-94e2-9577907ec081
STEP: Creating a pod to test consume configMaps
Nov 26 08:46:07.797: INFO: Waiting up to 5m0s for pod "pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf" in namespace "configmap-169" to be "Succeeded or Failed"
Nov 26 08:46:07.804: INFO: Pod "pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.272951ms
Nov 26 08:46:09.808: INFO: Pod "pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0117354s
STEP: Saw pod success
Nov 26 08:46:09.808: INFO: Pod "pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf" satisfied condition "Succeeded or Failed"
Nov 26 08:46:09.810: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf container configmap-volume-test: <nil>
STEP: delete the pod
Nov 26 08:46:09.833: INFO: Waiting for pod pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf to disappear
Nov 26 08:46:09.838: INFO: Pod pod-configmaps-82961c55-cbd4-4e56-bb5c-c5c911cae3cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:09.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-169" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4873,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:09.843: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:46:10.000: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 26 08:46:14.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-7234 --namespace=crd-publish-openapi-7234 create -f -'
Nov 26 08:46:16.041: INFO: stderr: ""
Nov 26 08:46:16.041: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 26 08:46:16.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-7234 --namespace=crd-publish-openapi-7234 delete e2e-test-crd-publish-openapi-7802-crds test-cr'
Nov 26 08:46:16.104: INFO: stderr: ""
Nov 26 08:46:16.104: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 26 08:46:16.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-7234 --namespace=crd-publish-openapi-7234 apply -f -'
Nov 26 08:46:16.344: INFO: stderr: ""
Nov 26 08:46:16.344: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 26 08:46:16.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-7234 --namespace=crd-publish-openapi-7234 delete e2e-test-crd-publish-openapi-7802-crds test-cr'
Nov 26 08:46:16.401: INFO: stderr: ""
Nov 26 08:46:16.401: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 26 08:46:16.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=crd-publish-openapi-7234 explain e2e-test-crd-publish-openapi-7802-crds'
Nov 26 08:46:16.674: INFO: stderr: ""
Nov 26 08:46:16.674: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7802-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:21.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7234" for this suite.

• [SLOW TEST:11.963 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":277,"skipped":4881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:21.807: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-1711
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1711 to expose endpoints map[]
Nov 26 08:46:21.991: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 26 08:46:23.009: INFO: successfully validated that service multi-endpoint-test in namespace services-1711 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1711
Nov 26 08:46:23.022: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:46:25.026: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1711 to expose endpoints map[pod1:[100]]
Nov 26 08:46:25.036: INFO: successfully validated that service multi-endpoint-test in namespace services-1711 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1711
Nov 26 08:46:25.045: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:46:27.048: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1711 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 26 08:46:27.055: INFO: successfully validated that service multi-endpoint-test in namespace services-1711 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Nov 26 08:46:27.055: INFO: Creating new exec pod
Nov 26 08:46:30.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1711 exec execpodbkjvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Nov 26 08:46:30.259: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 26 08:46:30.260: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:46:30.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1711 exec execpodbkjvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.66.92 80'
Nov 26 08:46:30.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.66.92 80\nConnection to 10.96.66.92 80 port [tcp/http] succeeded!\n"
Nov 26 08:46:30.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:46:30.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1711 exec execpodbkjvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Nov 26 08:46:30.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 26 08:46:30.535: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:46:30.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1711 exec execpodbkjvn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.66.92 81'
Nov 26 08:46:30.676: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.66.92 81\nConnection to 10.96.66.92 81 port [tcp/*] succeeded!\n"
Nov 26 08:46:30.676: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1711
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1711 to expose endpoints map[pod2:[101]]
Nov 26 08:46:30.745: INFO: successfully validated that service multi-endpoint-test in namespace services-1711 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1711
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1711 to expose endpoints map[]
Nov 26 08:46:30.778: INFO: successfully validated that service multi-endpoint-test in namespace services-1711 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:30.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1711" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.007 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":278,"skipped":4910,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:30.814: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:38.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8758" for this suite.

• [SLOW TEST:8.078 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":279,"skipped":4925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-f4ae44f5-a653-4e77-82aa-9fa38a982fa9
STEP: Creating a pod to test consume configMaps
Nov 26 08:46:38.976: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8" in namespace "projected-7025" to be "Succeeded or Failed"
Nov 26 08:46:38.985: INFO: Pod "pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.825668ms
Nov 26 08:46:40.991: INFO: Pod "pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014198756s
STEP: Saw pod success
Nov 26 08:46:40.991: INFO: Pod "pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8" satisfied condition "Succeeded or Failed"
Nov 26 08:46:40.992: INFO: Trying to get logs from node cncf-node3 pod pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 08:46:41.014: INFO: Waiting for pod pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8 to disappear
Nov 26 08:46:41.019: INFO: Pod pod-projected-configmaps-dd2e9558-3956-4f33-887c-cefe70ff23e8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:41.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7025" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":4947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:41.024: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Nov 26 08:46:41.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 26 08:46:41.142: INFO: stderr: ""
Nov 26 08:46:41.142: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Nov 26 08:46:41.142: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 26 08:46:41.142: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6387" to be "running and ready, or succeeded"
Nov 26 08:46:41.146: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.851248ms
Nov 26 08:46:43.151: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009013492s
Nov 26 08:46:43.151: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 26 08:46:43.151: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 26 08:46:43.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator'
Nov 26 08:46:43.209: INFO: stderr: ""
Nov 26 08:46:43.209: INFO: stdout: "I1126 08:46:43.036683       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/pc4 230\nI1126 08:46:43.236749       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/648n 210\nI1126 08:46:43.437267       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/gdt 275\nI1126 08:46:43.637667       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/585 369\nI1126 08:46:43.836848       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/hwj 559\nI1126 08:46:44.037252       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fqk 587\n"
Nov 26 08:46:45.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator'
Nov 26 08:46:45.270: INFO: stderr: ""
Nov 26 08:46:45.270: INFO: stdout: "I1126 08:46:43.036683       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/pc4 230\nI1126 08:46:43.236749       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/648n 210\nI1126 08:46:43.437267       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/gdt 275\nI1126 08:46:43.637667       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/585 369\nI1126 08:46:43.836848       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/hwj 559\nI1126 08:46:44.037252       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fqk 587\nI1126 08:46:44.237582       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/fkcp 308\nI1126 08:46:44.436948       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/hhcp 457\nI1126 08:46:44.637289       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/j755 500\nI1126 08:46:44.837667       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/jb92 336\nI1126 08:46:45.037039       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/rl48 277\nI1126 08:46:45.237405       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/k5q5 451\nI1126 08:46:45.436745       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/fz9x 296\nI1126 08:46:45.637194       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/zx4z 398\nI1126 08:46:45.837523       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/knh 384\nI1126 08:46:46.036747       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/4jqv 591\nI1126 08:46:46.237116       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/n8m 503\n"
STEP: limiting log lines
Nov 26 08:46:45.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator --tail=1'
Nov 26 08:46:45.330: INFO: stderr: ""
Nov 26 08:46:45.330: INFO: stdout: "I1126 08:46:46.237116       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/n8m 503\n"
Nov 26 08:46:45.330: INFO: got output "I1126 08:46:46.237116       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/n8m 503\n"
STEP: limiting log bytes
Nov 26 08:46:45.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator --limit-bytes=1'
Nov 26 08:46:45.389: INFO: stderr: ""
Nov 26 08:46:45.389: INFO: stdout: "I"
Nov 26 08:46:45.389: INFO: got output "I"
STEP: exposing timestamps
Nov 26 08:46:45.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 26 08:46:45.457: INFO: stderr: ""
Nov 26 08:46:45.457: INFO: stdout: "2021-11-26T17:46:46.437542338+09:00 I1126 08:46:46.437480       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zn7s 250\n"
Nov 26 08:46:45.457: INFO: got output "2021-11-26T17:46:46.437542338+09:00 I1126 08:46:46.437480       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zn7s 250\n"
STEP: restricting to a time range
Nov 26 08:46:47.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator --since=1s'
Nov 26 08:46:48.018: INFO: stderr: ""
Nov 26 08:46:48.018: INFO: stdout: "I1126 08:46:48.037669       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/nsqx 311\nI1126 08:46:48.237101       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/nzd 269\nI1126 08:46:48.437468       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/bnxc 332\nI1126 08:46:48.636760       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/fvzh 247\nI1126 08:46:48.837218       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/nlv 303\n"
Nov 26 08:46:48.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 logs logs-generator logs-generator --since=24h'
Nov 26 08:46:48.077: INFO: stderr: ""
Nov 26 08:46:48.077: INFO: stdout: "I1126 08:46:43.036683       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/pc4 230\nI1126 08:46:43.236749       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/648n 210\nI1126 08:46:43.437267       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/gdt 275\nI1126 08:46:43.637667       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/585 369\nI1126 08:46:43.836848       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/hwj 559\nI1126 08:46:44.037252       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/fqk 587\nI1126 08:46:44.237582       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/fkcp 308\nI1126 08:46:44.436948       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/hhcp 457\nI1126 08:46:44.637289       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/j755 500\nI1126 08:46:44.837667       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/jb92 336\nI1126 08:46:45.037039       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/rl48 277\nI1126 08:46:45.237405       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/k5q5 451\nI1126 08:46:45.436745       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/fz9x 296\nI1126 08:46:45.637194       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/zx4z 398\nI1126 08:46:45.837523       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/knh 384\nI1126 08:46:46.036747       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/4jqv 591\nI1126 08:46:46.237116       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/n8m 503\nI1126 08:46:46.437480       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zn7s 250\nI1126 08:46:46.636719       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/sms 352\nI1126 08:46:46.837127       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/8sh 221\nI1126 08:46:47.037513       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/pkl 549\nI1126 08:46:47.236780       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/zngv 333\nI1126 08:46:47.437202       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/ls2s 474\nI1126 08:46:47.637622       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/sb7j 544\nI1126 08:46:47.837071       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/bgm 212\nI1126 08:46:48.037669       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/nsqx 311\nI1126 08:46:48.237101       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/nzd 269\nI1126 08:46:48.437468       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/bnxc 332\nI1126 08:46:48.636760       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/fvzh 247\nI1126 08:46:48.837218       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/nlv 303\nI1126 08:46:49.037647       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/z6wk 463\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Nov 26 08:46:48.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-6387 delete pod logs-generator'
Nov 26 08:46:49.182: INFO: stderr: ""
Nov 26 08:46:49.182: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:49.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6387" for this suite.

• [SLOW TEST:8.165 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":281,"skipped":4985,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:49.189: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:46:49.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:46:52.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:52.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8036" for this suite.
STEP: Destroying namespace "webhook-8036-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":282,"skipped":4991,"failed":0}
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:52.816: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:52.869: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-2894
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:59.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1011" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:46:59.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2894" for this suite.

• [SLOW TEST:6.231 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":283,"skipped":4996,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:46:59.047: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 26 08:46:59.093: INFO: Waiting up to 5m0s for pod "pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9" in namespace "emptydir-7804" to be "Succeeded or Failed"
Nov 26 08:46:59.102: INFO: Pod "pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.252919ms
Nov 26 08:47:01.106: INFO: Pod "pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012643806s
STEP: Saw pod success
Nov 26 08:47:01.106: INFO: Pod "pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9" satisfied condition "Succeeded or Failed"
Nov 26 08:47:01.108: INFO: Trying to get logs from node cncf-node3 pod pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9 container test-container: <nil>
STEP: delete the pod
Nov 26 08:47:01.131: INFO: Waiting for pod pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9 to disappear
Nov 26 08:47:01.136: INFO: Pod pod-12cf1aa7-0dfc-4ae9-87bc-cee3f9eab5c9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:01.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7804" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:01.141: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:18.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3830" for this suite.

• [SLOW TEST:17.118 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":285,"skipped":5050,"failed":0}
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:18.258: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:18.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7952" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":286,"skipped":5050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:18.368: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:47:18.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:47:21.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:33.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7145" for this suite.
STEP: Destroying namespace "webhook-7145-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":287,"skipped":5078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:34.025: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 26 08:47:34.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7110 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 26 08:47:34.157: INFO: stderr: ""
Nov 26 08:47:34.157: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 26 08:47:39.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7110 get pod e2e-test-httpd-pod -o json'
Nov 26 08:47:39.263: INFO: stderr: ""
Nov 26 08:47:39.263: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"75f0cbc70f4c79de4355840107a642e95ffd63769e57059d98f3023e6055d50e\",\n            \"cni.projectcalico.org/podIP\": \"10.244.35.176/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.35.176/32\",\n            \"createdTime\": \"2021-11-26T17:47:35.370384453+09:00\",\n            \"creator\": \"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\",\n            \"updatedTime\": \"2021-11-26T17:47:35.370384453+09:00\",\n            \"updater\": \"system:serviceaccount:sonobuoy:sonobuoy-serviceaccount\"\n        },\n        \"creationTimestamp\": \"2021-11-26T08:47:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7110\",\n        \"resourceVersion\": \"130111\",\n        \"uid\": \"fc6dceeb-d2bc-47e1-afda-e536abb8574e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-46x57\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cncf-node3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-46x57\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-26T08:47:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-26T08:47:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-26T08:47:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-26T08:47:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://06bb701815203df85338316774a574f0dcc4f7bad07ffe214199863c6cd6a8c7\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-11-26T08:47:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.21.7.12\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.35.176\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.35.176\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-11-26T08:47:35Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 26 08:47:39.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7110 replace -f -'
Nov 26 08:47:39.602: INFO: stderr: ""
Nov 26 08:47:39.602: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Nov 26 08:47:39.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-7110 delete pods e2e-test-httpd-pod'
Nov 26 08:47:41.347: INFO: stderr: ""
Nov 26 08:47:41.347: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:41.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7110" for this suite.

• [SLOW TEST:7.328 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":288,"skipped":5123,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:41.354: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Nov 26 08:47:41.453: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 26 08:47:46.460: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Nov 26 08:47:46.462: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:46.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8039" for this suite.

• [SLOW TEST:5.175 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":289,"skipped":5130,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:46.529: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d565b7e6-4562-4a65-a208-aeaef8bc194f
STEP: Creating a pod to test consume secrets
Nov 26 08:47:46.605: INFO: Waiting up to 5m0s for pod "pod-secrets-412740be-24eb-4382-a323-d48cba97a11f" in namespace "secrets-4055" to be "Succeeded or Failed"
Nov 26 08:47:46.653: INFO: Pod "pod-secrets-412740be-24eb-4382-a323-d48cba97a11f": Phase="Pending", Reason="", readiness=false. Elapsed: 47.639529ms
Nov 26 08:47:48.655: INFO: Pod "pod-secrets-412740be-24eb-4382-a323-d48cba97a11f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050369918s
STEP: Saw pod success
Nov 26 08:47:48.655: INFO: Pod "pod-secrets-412740be-24eb-4382-a323-d48cba97a11f" satisfied condition "Succeeded or Failed"
Nov 26 08:47:48.657: INFO: Trying to get logs from node cncf-node3 pod pod-secrets-412740be-24eb-4382-a323-d48cba97a11f container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:47:48.671: INFO: Waiting for pod pod-secrets-412740be-24eb-4382-a323-d48cba97a11f to disappear
Nov 26 08:47:48.678: INFO: Pod pod-secrets-412740be-24eb-4382-a323-d48cba97a11f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:48.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4055" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5131,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:48.682: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:47:48.733: INFO: The status of Pod busybox-host-aliases42ccc4f8-ccae-440e-9ded-63ef609ad3f4 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:47:50.736: INFO: The status of Pod busybox-host-aliases42ccc4f8-ccae-440e-9ded-63ef609ad3f4 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9173" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5136,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:50.746: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-00f77e94-53a5-4f7c-84ab-e218dbdb6625
STEP: Creating a pod to test consume secrets
Nov 26 08:47:50.819: INFO: Waiting up to 5m0s for pod "pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb" in namespace "secrets-6926" to be "Succeeded or Failed"
Nov 26 08:47:50.825: INFO: Pod "pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.554784ms
Nov 26 08:47:52.829: INFO: Pod "pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010190771s
STEP: Saw pod success
Nov 26 08:47:52.829: INFO: Pod "pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb" satisfied condition "Succeeded or Failed"
Nov 26 08:47:52.830: INFO: Trying to get logs from node cncf-node3 pod pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb container secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:47:52.856: INFO: Waiting for pod pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb to disappear
Nov 26 08:47:52.865: INFO: Pod pod-secrets-9842bad4-128e-4e25-872d-9135d32c5edb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:52.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6926" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5137,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:52.869: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-477977ac-6b7b-441b-859f-81e667866d64
STEP: Creating a pod to test consume secrets
Nov 26 08:47:52.944: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b" in namespace "projected-7217" to be "Succeeded or Failed"
Nov 26 08:47:52.957: INFO: Pod "pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.849511ms
Nov 26 08:47:54.960: INFO: Pod "pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016321947s
STEP: Saw pod success
Nov 26 08:47:54.960: INFO: Pod "pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b" satisfied condition "Succeeded or Failed"
Nov 26 08:47:54.969: INFO: Trying to get logs from node cncf-node3 pod pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 26 08:47:54.993: INFO: Waiting for pod pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b to disappear
Nov 26 08:47:54.999: INFO: Pod pod-projected-secrets-f91e1129-3380-4c5c-b7d7-e738cf42281b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:54.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7217" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5138,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:55.003: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 26 08:47:55.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7" in namespace "downward-api-917" to be "Succeeded or Failed"
Nov 26 08:47:55.071: INFO: Pod "downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.214468ms
Nov 26 08:47:57.074: INFO: Pod "downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008943221s
STEP: Saw pod success
Nov 26 08:47:57.074: INFO: Pod "downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7" satisfied condition "Succeeded or Failed"
Nov 26 08:47:57.076: INFO: Trying to get logs from node cncf-node3 pod downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7 container client-container: <nil>
STEP: delete the pod
Nov 26 08:47:57.091: INFO: Waiting for pod downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7 to disappear
Nov 26 08:47:57.097: INFO: Pod downwardapi-volume-f9784d69-cf61-403b-a720-13712a6005a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:47:57.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-917" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5144,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:47:57.101: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4519
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4519
STEP: creating replication controller externalsvc in namespace services-4519
I1126 08:47:57.205934      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4519, replica count: 2
I1126 08:48:00.257176      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 26 08:48:00.287: INFO: Creating new exec pod
Nov 26 08:48:02.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-4519 exec execpodxps58 -- /bin/sh -x -c nslookup nodeport-service.services-4519.svc.cluster.local'
Nov 26 08:48:02.476: INFO: stderr: "+ nslookup nodeport-service.services-4519.svc.cluster.local\n"
Nov 26 08:48:02.476: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4519.svc.cluster.local\tcanonical name = externalsvc.services-4519.svc.cluster.local.\nName:\texternalsvc.services-4519.svc.cluster.local\nAddress: 10.96.196.158\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4519, will wait for the garbage collector to delete the pods
Nov 26 08:48:02.533: INFO: Deleting ReplicationController externalsvc took: 3.83583ms
Nov 26 08:48:02.634: INFO: Terminating ReplicationController externalsvc pods took: 101.1123ms
Nov 26 08:48:04.555: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:48:04.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4519" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.524 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":295,"skipped":5149,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:48:04.625: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-5635/configmap-test-44f185c4-eaed-4af6-a0ee-252782266f07
STEP: Creating a pod to test consume configMaps
Nov 26 08:48:04.667: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1" in namespace "configmap-5635" to be "Succeeded or Failed"
Nov 26 08:48:04.673: INFO: Pod "pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.813211ms
Nov 26 08:48:06.676: INFO: Pod "pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00876084s
STEP: Saw pod success
Nov 26 08:48:06.676: INFO: Pod "pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1" satisfied condition "Succeeded or Failed"
Nov 26 08:48:06.677: INFO: Trying to get logs from node cncf-node3 pod pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1 container env-test: <nil>
STEP: delete the pod
Nov 26 08:48:06.696: INFO: Waiting for pod pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1 to disappear
Nov 26 08:48:06.698: INFO: Pod pod-configmaps-9d6fbfdb-8e35-4318-af5a-2e4b1d197ee1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:48:06.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5635" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5160,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:48:06.703: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Nov 26 08:48:06.750: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Nov 26 08:48:08.762: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Nov 26 08:48:10.776: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:48:12.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9184" for this suite.

• [SLOW TEST:6.080 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":297,"skipped":5174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:48:12.783: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 26 08:48:12.846: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130800 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:48:12.846: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130800 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 26 08:48:22.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130858 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:48:22.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130858 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 26 08:48:32.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130916 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:48:32.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130916 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 26 08:48:42.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130966 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:48:42.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2378  2c7ffbc8-0fb8-4810-8e3b-9aadee2d327b 130966 0 2021-11-26 08:48:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[createdTime:2021-11-26T17:48:14.069024673+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:14.069024673+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 26 08:48:52.867: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2378  c1efdd44-f4d9-4960-a23f-29db27522f9c 131016 0 2021-11-26 08:48:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2021-11-26T17:48:54.088115444+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:54.088115444+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:48:52.867: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2378  c1efdd44-f4d9-4960-a23f-29db27522f9c 131016 0 2021-11-26 08:48:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2021-11-26T17:48:54.088115444+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:54.088115444+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 26 08:49:02.871: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2378  c1efdd44-f4d9-4960-a23f-29db27522f9c 131064 0 2021-11-26 08:48:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2021-11-26T17:48:54.088115444+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:54.088115444+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 26 08:49:02.871: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2378  c1efdd44-f4d9-4960-a23f-29db27522f9c 131064 0 2021-11-26 08:48:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[createdTime:2021-11-26T17:48:54.088115444+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:48:54.088115444+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:12.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2378" for this suite.

• [SLOW TEST:60.094 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":298,"skipped":5224,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:12.878: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 26 08:49:17.447: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3254 pod-service-account-27884b44-4338-4a64-b0d8-494b28d85fb9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 26 08:49:17.610: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3254 pod-service-account-27884b44-4338-4a64-b0d8-494b28d85fb9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 26 08:49:17.749: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3254 pod-service-account-27884b44-4338-4a64-b0d8-494b28d85fb9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:17.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3254" for this suite.

• [SLOW TEST:5.026 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":299,"skipped":5234,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:18.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4380" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":300,"skipped":5248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:18.025: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 26 08:49:18.113: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:18.115: INFO: Number of nodes with available pods: 0
Nov 26 08:49:18.115: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:49:19.127: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:19.136: INFO: Number of nodes with available pods: 0
Nov 26 08:49:19.136: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:49:20.119: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:20.121: INFO: Number of nodes with available pods: 2
Nov 26 08:49:20.121: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 26 08:49:20.134: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:20.136: INFO: Number of nodes with available pods: 1
Nov 26 08:49:20.136: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:49:21.143: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:21.151: INFO: Number of nodes with available pods: 1
Nov 26 08:49:21.151: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:49:22.141: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:22.148: INFO: Number of nodes with available pods: 1
Nov 26 08:49:22.148: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:49:23.156: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:23.157: INFO: Number of nodes with available pods: 1
Nov 26 08:49:23.157: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:49:24.142: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:49:24.144: INFO: Number of nodes with available pods: 2
Nov 26 08:49:24.144: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-271, will wait for the garbage collector to delete the pods
Nov 26 08:49:24.200: INFO: Deleting DaemonSet.extensions daemon-set took: 3.038674ms
Nov 26 08:49:24.301: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.616003ms
Nov 26 08:49:26.803: INFO: Number of nodes with available pods: 0
Nov 26 08:49:26.803: INFO: Number of running nodes: 0, number of available pods: 0
Nov 26 08:49:26.804: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"131308"},"items":null}

Nov 26 08:49:26.806: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"131308"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-271" for this suite.

• [SLOW TEST:8.789 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":301,"skipped":5270,"failed":0}
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:26.815: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:26.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8501" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":302,"skipped":5270,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:29.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-89" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":303,"skipped":5284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:29.759: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Nov 26 08:49:29.804: INFO: Waiting up to 5m0s for pod "client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8" in namespace "containers-1802" to be "Succeeded or Failed"
Nov 26 08:49:29.806: INFO: Pod "client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665419ms
Nov 26 08:49:31.823: INFO: Pod "client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019560014s
STEP: Saw pod success
Nov 26 08:49:31.823: INFO: Pod "client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8" satisfied condition "Succeeded or Failed"
Nov 26 08:49:31.832: INFO: Trying to get logs from node cncf-node3 pod client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 08:49:31.855: INFO: Waiting for pod client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8 to disappear
Nov 26 08:49:31.858: INFO: Pod client-containers-5df1d63a-dc8a-4e76-b65c-a372a6fba6b8 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:31.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1802" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:31.865: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-xktc
STEP: Creating a pod to test atomic-volume-subpath
Nov 26 08:49:31.966: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xktc" in namespace "subpath-1742" to be "Succeeded or Failed"
Nov 26 08:49:31.984: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.349111ms
Nov 26 08:49:33.988: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 2.021518879s
Nov 26 08:49:35.991: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 4.024203286s
Nov 26 08:49:37.994: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 6.02768405s
Nov 26 08:49:40.000: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 8.033178143s
Nov 26 08:49:42.003: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 10.036867355s
Nov 26 08:49:44.008: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 12.041520176s
Nov 26 08:49:46.012: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 14.045557728s
Nov 26 08:49:48.016: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 16.049638625s
Nov 26 08:49:50.020: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 18.053824537s
Nov 26 08:49:52.023: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Running", Reason="", readiness=true. Elapsed: 20.05657928s
Nov 26 08:49:54.027: INFO: Pod "pod-subpath-test-downwardapi-xktc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.060019604s
STEP: Saw pod success
Nov 26 08:49:54.027: INFO: Pod "pod-subpath-test-downwardapi-xktc" satisfied condition "Succeeded or Failed"
Nov 26 08:49:54.028: INFO: Trying to get logs from node cncf-node3 pod pod-subpath-test-downwardapi-xktc container test-container-subpath-downwardapi-xktc: <nil>
STEP: delete the pod
Nov 26 08:49:54.042: INFO: Waiting for pod pod-subpath-test-downwardapi-xktc to disappear
Nov 26 08:49:54.048: INFO: Pod pod-subpath-test-downwardapi-xktc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xktc
Nov 26 08:49:54.048: INFO: Deleting pod "pod-subpath-test-downwardapi-xktc" in namespace "subpath-1742"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:49:54.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1742" for this suite.

• [SLOW TEST:22.188 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":305,"skipped":5368,"failed":0}
SSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:49:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 26 08:49:56.114: INFO: &Pod{ObjectMeta:{send-events-36eae8cf-a0c6-4397-92ec-4cabae4a15ec  events-9811  f2f47200-73fa-4a3b-b6c7-fbb4b1511bb3 131667 0 2021-11-26 08:49:54 +0000 UTC <nil> <nil> map[name:foo time:98038856] map[cni.projectcalico.org/containerID:4123c3d5efb90e103126cd03f0ca9537ca866b69e99d37363c8502908e02612a cni.projectcalico.org/podIP:10.244.35.157/32 cni.projectcalico.org/podIPs:10.244.35.157/32 createdTime:2021-11-26T17:49:55.326571+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:49:55.326571+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{calico Update v1 2021-11-26 08:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {e2e.test Update v1 2021-11-26 08:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xsz69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xsz69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:49:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.157,StartTime:2021-11-26 08:49:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:49:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://030625ff41361613141cc1da8f58210bb3738969ed5b44cf17859f71bb7af8d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 26 08:49:58.120: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 26 08:50:00.123: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:00.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9811" for this suite.

• [SLOW TEST:6.104 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":306,"skipped":5371,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:00.157: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:16.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-135" for this suite.

• [SLOW TEST:16.161 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":307,"skipped":5383,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:16.319: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Nov 26 08:50:16.373: INFO: Waiting up to 5m0s for pod "var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6" in namespace "var-expansion-6228" to be "Succeeded or Failed"
Nov 26 08:50:16.383: INFO: Pod "var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.000971ms
Nov 26 08:50:18.387: INFO: Pod "var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014474532s
STEP: Saw pod success
Nov 26 08:50:18.387: INFO: Pod "var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6" satisfied condition "Succeeded or Failed"
Nov 26 08:50:18.389: INFO: Trying to get logs from node cncf-node3 pod var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6 container dapi-container: <nil>
STEP: delete the pod
Nov 26 08:50:18.434: INFO: Waiting for pod var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6 to disappear
Nov 26 08:50:18.445: INFO: Pod var-expansion-7fab0c74-a6ba-465d-a227-2dc2972d91f6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:18.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6228" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5385,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:18.450: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:50:18.499: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:21.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4324" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":309,"skipped":5391,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:21.643: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Nov 26 08:50:21.714: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 26 08:50:26.717: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:26.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3615" for this suite.

• [SLOW TEST:5.174 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":310,"skipped":5396,"failed":0}
SSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:26.818: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Nov 26 08:50:26.931: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:50:28.934: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.21.7.12 on the node which pod1 resides and expect scheduled
Nov 26 08:50:28.943: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:50:30.946: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.21.7.12 but use UDP protocol on the node which pod2 resides
Nov 26 08:50:30.984: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:50:32.986: INFO: The status of Pod pod3 is Running (Ready = true)
Nov 26 08:50:33.001: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:50:35.004: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Nov 26 08:50:35.006: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.21.7.12 http://127.0.0.1:54323/hostname] Namespace:hostport-1854 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:50:35.006: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.21.7.12, port: 54323
Nov 26 08:50:35.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.21.7.12:54323/hostname] Namespace:hostport-1854 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:50:35.117: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.21.7.12, port: 54323 UDP
Nov 26 08:50:35.238: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.21.7.12 54323] Namespace:hostport-1854 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:50:35.238: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:40.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1854" for this suite.

• [SLOW TEST:13.514 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":311,"skipped":5404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:40.332: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Nov 26 08:50:40.402: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 26 08:50:40.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 create -f -'
Nov 26 08:50:40.667: INFO: stderr: ""
Nov 26 08:50:40.667: INFO: stdout: "service/agnhost-replica created\n"
Nov 26 08:50:40.667: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 26 08:50:40.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 create -f -'
Nov 26 08:50:40.947: INFO: stderr: ""
Nov 26 08:50:40.947: INFO: stdout: "service/agnhost-primary created\n"
Nov 26 08:50:40.947: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 26 08:50:40.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 create -f -'
Nov 26 08:50:41.221: INFO: stderr: ""
Nov 26 08:50:41.221: INFO: stdout: "service/frontend created\n"
Nov 26 08:50:41.221: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 26 08:50:41.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 create -f -'
Nov 26 08:50:41.488: INFO: stderr: ""
Nov 26 08:50:41.488: INFO: stdout: "deployment.apps/frontend created\n"
Nov 26 08:50:41.488: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 26 08:50:41.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 create -f -'
Nov 26 08:50:41.770: INFO: stderr: ""
Nov 26 08:50:41.770: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 26 08:50:41.770: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 26 08:50:41.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 create -f -'
Nov 26 08:50:42.066: INFO: stderr: ""
Nov 26 08:50:42.066: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 26 08:50:42.066: INFO: Waiting for all frontend pods to be Running.
Nov 26 08:50:47.117: INFO: Waiting for frontend to serve content.
Nov 26 08:50:47.124: INFO: Trying to add a new entry to the guestbook.
Nov 26 08:50:47.130: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 26 08:50:47.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 delete --grace-period=0 --force -f -'
Nov 26 08:50:47.221: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:50:47.221: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 26 08:50:47.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 delete --grace-period=0 --force -f -'
Nov 26 08:50:47.416: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:50:47.416: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 26 08:50:47.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 delete --grace-period=0 --force -f -'
Nov 26 08:50:47.496: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:50:47.496: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 26 08:50:47.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 delete --grace-period=0 --force -f -'
Nov 26 08:50:47.571: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:50:47.571: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 26 08:50:47.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 delete --grace-period=0 --force -f -'
Nov 26 08:50:47.628: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:50:47.628: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 26 08:50:47.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=kubectl-3336 delete --grace-period=0 --force -f -'
Nov 26 08:50:47.685: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 26 08:50:47.685: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:47.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3336" for this suite.

• [SLOW TEST:7.364 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":312,"skipped":5438,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:47.696: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:50:47.803: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f0d8515c-6d57-4e42-9d9d-54c91167edc2" in namespace "security-context-test-6806" to be "Succeeded or Failed"
Nov 26 08:50:47.810: INFO: Pod "busybox-readonly-false-f0d8515c-6d57-4e42-9d9d-54c91167edc2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.759155ms
Nov 26 08:50:49.812: INFO: Pod "busybox-readonly-false-f0d8515c-6d57-4e42-9d9d-54c91167edc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009271741s
Nov 26 08:50:51.815: INFO: Pod "busybox-readonly-false-f0d8515c-6d57-4e42-9d9d-54c91167edc2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012118863s
Nov 26 08:50:53.828: INFO: Pod "busybox-readonly-false-f0d8515c-6d57-4e42-9d9d-54c91167edc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025522926s
Nov 26 08:50:53.828: INFO: Pod "busybox-readonly-false-f0d8515c-6d57-4e42-9d9d-54c91167edc2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:53.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6806" for this suite.

• [SLOW TEST:6.137 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:171
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5455,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:53.833: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:53.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9897" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":5473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Nov 26 08:50:54.050: INFO: Waiting up to 5m0s for pod "test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6" in namespace "svcaccounts-243" to be "Succeeded or Failed"
Nov 26 08:50:54.062: INFO: Pod "test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.743091ms
Nov 26 08:50:56.067: INFO: Pod "test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017573599s
Nov 26 08:50:58.070: INFO: Pod "test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020489761s
STEP: Saw pod success
Nov 26 08:50:58.070: INFO: Pod "test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6" satisfied condition "Succeeded or Failed"
Nov 26 08:50:58.072: INFO: Trying to get logs from node cncf-node3 pod test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6 container agnhost-container: <nil>
STEP: delete the pod
Nov 26 08:50:58.097: INFO: Waiting for pod test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6 to disappear
Nov 26 08:50:58.119: INFO: Pod test-pod-6e19f0dd-f0ac-4d18-9116-942480aaadf6 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:50:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-243" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":315,"skipped":5506,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:50:58.124: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Nov 26 08:51:18.350: INFO: EndpointSlice for Service endpointslice-4622/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:51:28.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4622" for this suite.

• [SLOW TEST:30.240 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":316,"skipped":5512,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:51:28.365: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 26 08:51:28.426: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:51:32.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7752" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":317,"skipped":5523,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:51:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 26 08:51:32.624: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 26 08:51:35.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:51:45.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1283" for this suite.
STEP: Destroying namespace "webhook-1283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.837 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":318,"skipped":5531,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:51:45.964: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3517
Nov 26 08:51:46.010: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:51:48.016: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 26 08:51:48.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-3517 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 26 08:51:48.164: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 26 08:51:48.164: INFO: stdout: "iptables"
Nov 26 08:51:48.164: INFO: proxyMode: iptables
Nov 26 08:51:48.182: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 26 08:51:48.187: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3517
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3517
I1126 08:51:48.204902      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3517, replica count: 3
I1126 08:51:51.256608      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:51:51.260: INFO: Creating new exec pod
Nov 26 08:51:54.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-3517 exec execpod-affinitymj4tt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Nov 26 08:51:54.436: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 26 08:51:54.436: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:51:54.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-3517 exec execpod-affinitymj4tt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.120.21 80'
Nov 26 08:51:54.594: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.120.21 80\nConnection to 10.96.120.21 80 port [tcp/http] succeeded!\n"
Nov 26 08:51:54.594: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:51:54.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-3517 exec execpod-affinitymj4tt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.120.21:80/ ; done'
Nov 26 08:51:54.796: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n"
Nov 26 08:51:54.796: INFO: stdout: "\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts\naffinity-clusterip-timeout-r77ts"
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Received response from host: affinity-clusterip-timeout-r77ts
Nov 26 08:51:54.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-3517 exec execpod-affinitymj4tt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.120.21:80/'
Nov 26 08:51:54.945: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n"
Nov 26 08:51:54.945: INFO: stdout: "affinity-clusterip-timeout-r77ts"
Nov 26 08:52:14.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-3517 exec execpod-affinitymj4tt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.120.21:80/'
Nov 26 08:52:15.114: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.120.21:80/\n"
Nov 26 08:52:15.114: INFO: stdout: "affinity-clusterip-timeout-pzcxs"
Nov 26 08:52:15.114: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3517, will wait for the garbage collector to delete the pods
Nov 26 08:52:15.195: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 2.370176ms
Nov 26 08:52:15.296: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.670016ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:17.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3517" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:31.468 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":319,"skipped":5540,"failed":0}
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:17.432: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 26 08:52:17.510: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:52:17.518: INFO: Number of nodes with available pods: 0
Nov 26 08:52:17.518: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:52:18.522: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:52:18.523: INFO: Number of nodes with available pods: 0
Nov 26 08:52:18.523: INFO: Node cncf-node2 is running more than one daemon pod
Nov 26 08:52:19.521: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:52:19.523: INFO: Number of nodes with available pods: 2
Nov 26 08:52:19.523: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 26 08:52:19.536: INFO: DaemonSet pods can't tolerate node cncf-node1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 26 08:52:19.541: INFO: Number of nodes with available pods: 2
Nov 26 08:52:19.541: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7857, will wait for the garbage collector to delete the pods
Nov 26 08:52:20.646: INFO: Deleting DaemonSet.extensions daemon-set took: 14.087864ms
Nov 26 08:52:20.746: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.57236ms
Nov 26 08:52:23.449: INFO: Number of nodes with available pods: 0
Nov 26 08:52:23.449: INFO: Number of running nodes: 0, number of available pods: 0
Nov 26 08:52:23.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133524"},"items":null}

Nov 26 08:52:23.452: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133524"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7857" for this suite.

• [SLOW TEST:6.029 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":320,"skipped":5540,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:23.461: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 26 08:52:23.549: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 26 08:52:23.552: INFO: starting watch
STEP: patching
STEP: updating
Nov 26 08:52:23.564: INFO: waiting for watch events with expected annotations
Nov 26 08:52:23.564: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:23.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9540" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":321,"skipped":5542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:23.589: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:52:23.629: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4774
I1126 08:52:23.639750      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4774, replica count: 1
I1126 08:52:24.691003      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1126 08:52:25.692005      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:52:25.822: INFO: Created: latency-svc-vj6mp
Nov 26 08:52:25.857: INFO: Got endpoints: latency-svc-vj6mp [64.692601ms]
Nov 26 08:52:25.874: INFO: Created: latency-svc-pp5lw
Nov 26 08:52:25.885: INFO: Created: latency-svc-nkxk9
Nov 26 08:52:25.885: INFO: Got endpoints: latency-svc-pp5lw [28.256143ms]
Nov 26 08:52:25.902: INFO: Got endpoints: latency-svc-nkxk9 [45.054789ms]
Nov 26 08:52:25.902: INFO: Created: latency-svc-k4pfk
Nov 26 08:52:25.906: INFO: Got endpoints: latency-svc-k4pfk [48.727497ms]
Nov 26 08:52:25.919: INFO: Created: latency-svc-sl9b7
Nov 26 08:52:25.923: INFO: Got endpoints: latency-svc-sl9b7 [65.891049ms]
Nov 26 08:52:25.942: INFO: Created: latency-svc-w486z
Nov 26 08:52:25.946: INFO: Got endpoints: latency-svc-w486z [89.02629ms]
Nov 26 08:52:26.000: INFO: Created: latency-svc-m88h9
Nov 26 08:52:26.011: INFO: Got endpoints: latency-svc-m88h9 [153.599509ms]
Nov 26 08:52:26.011: INFO: Created: latency-svc-gwssm
Nov 26 08:52:26.022: INFO: Got endpoints: latency-svc-gwssm [165.31571ms]
Nov 26 08:52:26.034: INFO: Created: latency-svc-wwkhl
Nov 26 08:52:26.045: INFO: Got endpoints: latency-svc-wwkhl [187.821668ms]
Nov 26 08:52:26.056: INFO: Created: latency-svc-cqrxd
Nov 26 08:52:26.068: INFO: Got endpoints: latency-svc-cqrxd [210.688611ms]
Nov 26 08:52:26.085: INFO: Created: latency-svc-8fbkv
Nov 26 08:52:26.089: INFO: Got endpoints: latency-svc-8fbkv [232.447663ms]
Nov 26 08:52:26.148: INFO: Created: latency-svc-4rzsd
Nov 26 08:52:26.154: INFO: Got endpoints: latency-svc-4rzsd [296.560348ms]
Nov 26 08:52:26.165: INFO: Created: latency-svc-fhlmz
Nov 26 08:52:26.176: INFO: Got endpoints: latency-svc-fhlmz [319.362644ms]
Nov 26 08:52:26.188: INFO: Created: latency-svc-z2skk
Nov 26 08:52:26.193: INFO: Got endpoints: latency-svc-z2skk [335.465403ms]
Nov 26 08:52:26.205: INFO: Created: latency-svc-4vkqb
Nov 26 08:52:26.210: INFO: Got endpoints: latency-svc-4vkqb [352.660822ms]
Nov 26 08:52:26.222: INFO: Created: latency-svc-6x2mk
Nov 26 08:52:26.227: INFO: Got endpoints: latency-svc-6x2mk [369.830652ms]
Nov 26 08:52:26.239: INFO: Created: latency-svc-4vnh4
Nov 26 08:52:26.244: INFO: Got endpoints: latency-svc-4vnh4 [359.196942ms]
Nov 26 08:52:26.297: INFO: Created: latency-svc-9tntx
Nov 26 08:52:26.308: INFO: Got endpoints: latency-svc-9tntx [406.060323ms]
Nov 26 08:52:26.308: INFO: Created: latency-svc-lrffk
Nov 26 08:52:26.319: INFO: Got endpoints: latency-svc-lrffk [413.655626ms]
Nov 26 08:52:26.331: INFO: Created: latency-svc-mhclg
Nov 26 08:52:26.342: INFO: Got endpoints: latency-svc-mhclg [419.524356ms]
Nov 26 08:52:26.354: INFO: Created: latency-svc-wmpt5
Nov 26 08:52:26.359: INFO: Got endpoints: latency-svc-wmpt5 [412.708616ms]
Nov 26 08:52:26.371: INFO: Created: latency-svc-llxrd
Nov 26 08:52:26.376: INFO: Got endpoints: latency-svc-llxrd [365.459971ms]
Nov 26 08:52:26.388: INFO: Created: latency-svc-95xpw
Nov 26 08:52:26.393: INFO: Got endpoints: latency-svc-95xpw [370.930904ms]
Nov 26 08:52:26.445: INFO: Created: latency-svc-shhz5
Nov 26 08:52:26.450: INFO: Got endpoints: latency-svc-shhz5 [405.549727ms]
Nov 26 08:52:26.462: INFO: Created: latency-svc-ks6m9
Nov 26 08:52:26.468: INFO: Got endpoints: latency-svc-ks6m9 [399.820269ms]
Nov 26 08:52:26.479: INFO: Created: latency-svc-5wmb8
Nov 26 08:52:26.485: INFO: Got endpoints: latency-svc-5wmb8 [395.470361ms]
Nov 26 08:52:26.497: INFO: Created: latency-svc-fvs77
Nov 26 08:52:26.508: INFO: Got endpoints: latency-svc-fvs77 [354.482813ms]
Nov 26 08:52:26.519: INFO: Created: latency-svc-gcwpv
Nov 26 08:52:26.531: INFO: Got endpoints: latency-svc-gcwpv [354.636452ms]
Nov 26 08:52:26.570: INFO: Created: latency-svc-qfk28
Nov 26 08:52:26.576: INFO: Got endpoints: latency-svc-qfk28 [383.881026ms]
Nov 26 08:52:26.588: INFO: Created: latency-svc-hgbxc
Nov 26 08:52:26.594: INFO: Got endpoints: latency-svc-hgbxc [383.794962ms]
Nov 26 08:52:26.617: INFO: Created: latency-svc-krjpq
Nov 26 08:52:26.622: INFO: Got endpoints: latency-svc-krjpq [395.417772ms]
Nov 26 08:52:26.634: INFO: Created: latency-svc-ddnvw
Nov 26 08:52:26.640: INFO: Got endpoints: latency-svc-ddnvw [395.287023ms]
Nov 26 08:52:26.651: INFO: Created: latency-svc-dqzpw
Nov 26 08:52:26.657: INFO: Got endpoints: latency-svc-dqzpw [348.854011ms]
Nov 26 08:52:26.668: INFO: Created: latency-svc-jml95
Nov 26 08:52:26.708: INFO: Got endpoints: latency-svc-jml95 [388.333376ms]
Nov 26 08:52:26.720: INFO: Created: latency-svc-bf42q
Nov 26 08:52:26.725: INFO: Got endpoints: latency-svc-bf42q [383.076211ms]
Nov 26 08:52:26.742: INFO: Created: latency-svc-rxncb
Nov 26 08:52:26.754: INFO: Got endpoints: latency-svc-rxncb [395.212464ms]
Nov 26 08:52:26.765: INFO: Created: latency-svc-jh2gn
Nov 26 08:52:26.777: INFO: Got endpoints: latency-svc-jh2gn [401.011063ms]
Nov 26 08:52:26.788: INFO: Created: latency-svc-l4pvx
Nov 26 08:52:26.794: INFO: Got endpoints: latency-svc-l4pvx [401.127113ms]
Nov 26 08:52:26.805: INFO: Created: latency-svc-l8c7l
Nov 26 08:52:26.845: INFO: Got endpoints: latency-svc-l8c7l [394.421742ms]
Nov 26 08:52:26.857: INFO: Created: latency-svc-6kxfj
Nov 26 08:52:26.863: INFO: Got endpoints: latency-svc-6kxfj [395.431343ms]
Nov 26 08:52:26.874: INFO: Created: latency-svc-5shrb
Nov 26 08:52:26.880: INFO: Got endpoints: latency-svc-5shrb [395.282288ms]
Nov 26 08:52:26.891: INFO: Created: latency-svc-fc5br
Nov 26 08:52:26.898: INFO: Got endpoints: latency-svc-fc5br [389.383079ms]
Nov 26 08:52:26.908: INFO: Created: latency-svc-r76w8
Nov 26 08:52:26.915: INFO: Got endpoints: latency-svc-r76w8 [383.453074ms]
Nov 26 08:52:26.926: INFO: Created: latency-svc-mkj9g
Nov 26 08:52:26.937: INFO: Got endpoints: latency-svc-mkj9g [360.667766ms]
Nov 26 08:52:26.971: INFO: Created: latency-svc-cvdsz
Nov 26 08:52:26.983: INFO: Got endpoints: latency-svc-cvdsz [389.311286ms]
Nov 26 08:52:26.994: INFO: Created: latency-svc-ghbjq
Nov 26 08:52:27.006: INFO: Got endpoints: latency-svc-ghbjq [383.40708ms]
Nov 26 08:52:27.017: INFO: Created: latency-svc-lk2f6
Nov 26 08:52:27.024: INFO: Got endpoints: latency-svc-lk2f6 [383.827416ms]
Nov 26 08:52:27.034: INFO: Created: latency-svc-8f9v2
Nov 26 08:52:27.041: INFO: Got endpoints: latency-svc-8f9v2 [383.924425ms]
Nov 26 08:52:27.051: INFO: Created: latency-svc-6w5tc
Nov 26 08:52:27.058: INFO: Got endpoints: latency-svc-6w5tc [350.355354ms]
Nov 26 08:52:27.068: INFO: Created: latency-svc-mvt7f
Nov 26 08:52:27.113: INFO: Got endpoints: latency-svc-mvt7f [387.97187ms]
Nov 26 08:52:27.115: INFO: Created: latency-svc-5j59g
Nov 26 08:52:27.121: INFO: Got endpoints: latency-svc-5j59g [366.80948ms]
Nov 26 08:52:27.131: INFO: Created: latency-svc-jpn5l
Nov 26 08:52:27.138: INFO: Got endpoints: latency-svc-jpn5l [361.218862ms]
Nov 26 08:52:27.148: INFO: Created: latency-svc-zkljm
Nov 26 08:52:27.160: INFO: Got endpoints: latency-svc-zkljm [365.613524ms]
Nov 26 08:52:27.171: INFO: Created: latency-svc-2zcwg
Nov 26 08:52:27.183: INFO: Got endpoints: latency-svc-2zcwg [337.944119ms]
Nov 26 08:52:27.194: INFO: Created: latency-svc-74hlt
Nov 26 08:52:27.201: INFO: Got endpoints: latency-svc-74hlt [338.131062ms]
Nov 26 08:52:27.212: INFO: Created: latency-svc-75tz6
Nov 26 08:52:27.245: INFO: Got endpoints: latency-svc-75tz6 [364.600392ms]
Nov 26 08:52:27.252: INFO: Created: latency-svc-pgsd6
Nov 26 08:52:27.259: INFO: Got endpoints: latency-svc-pgsd6 [361.185323ms]
Nov 26 08:52:27.269: INFO: Created: latency-svc-5t6x5
Nov 26 08:52:27.276: INFO: Got endpoints: latency-svc-5t6x5 [361.271977ms]
Nov 26 08:52:27.286: INFO: Created: latency-svc-bgqqf
Nov 26 08:52:27.293: INFO: Got endpoints: latency-svc-bgqqf [355.771833ms]
Nov 26 08:52:27.303: INFO: Created: latency-svc-pstp7
Nov 26 08:52:27.320: INFO: Created: latency-svc-qc5rg
Nov 26 08:52:27.332: INFO: Got endpoints: latency-svc-pstp7 [348.907633ms]
Nov 26 08:52:27.343: INFO: Created: latency-svc-l4m4t
Nov 26 08:52:27.400: INFO: Got endpoints: latency-svc-qc5rg [394.400971ms]
Nov 26 08:52:27.400: INFO: Created: latency-svc-zw8cv
Nov 26 08:52:27.418: INFO: Created: latency-svc-5jjkg
Nov 26 08:52:27.429: INFO: Got endpoints: latency-svc-l4m4t [405.229092ms]
Nov 26 08:52:27.440: INFO: Created: latency-svc-4xtrp
Nov 26 08:52:27.463: INFO: Created: latency-svc-ph99m
Nov 26 08:52:27.480: INFO: Got endpoints: latency-svc-zw8cv [439.457914ms]
Nov 26 08:52:27.480: INFO: Created: latency-svc-zlpsc
Nov 26 08:52:27.531: INFO: Created: latency-svc-9zmhn
Nov 26 08:52:27.534: INFO: Got endpoints: latency-svc-5jjkg [475.464264ms]
Nov 26 08:52:27.543: INFO: Created: latency-svc-fj9sd
Nov 26 08:52:27.560: INFO: Created: latency-svc-g7b7x
Nov 26 08:52:27.577: INFO: Created: latency-svc-c9k6g
Nov 26 08:52:27.583: INFO: Got endpoints: latency-svc-4xtrp [469.532303ms]
Nov 26 08:52:27.595: INFO: Created: latency-svc-5s8p8
Nov 26 08:52:27.612: INFO: Created: latency-svc-894ht
Nov 26 08:52:27.623: INFO: Created: latency-svc-rjwff
Nov 26 08:52:27.662: INFO: Created: latency-svc-4skk7
Nov 26 08:52:27.662: INFO: Got endpoints: latency-svc-ph99m [541.297497ms]
Nov 26 08:52:27.675: INFO: Created: latency-svc-5htms
Nov 26 08:52:27.686: INFO: Created: latency-svc-9j2b9
Nov 26 08:52:27.686: INFO: Got endpoints: latency-svc-zlpsc [547.881601ms]
Nov 26 08:52:27.697: INFO: Created: latency-svc-cv4c4
Nov 26 08:52:27.709: INFO: Created: latency-svc-fkvtw
Nov 26 08:52:27.720: INFO: Created: latency-svc-4sw2m
Nov 26 08:52:27.732: INFO: Got endpoints: latency-svc-9zmhn [572.183128ms]
Nov 26 08:52:27.732: INFO: Created: latency-svc-wzrcp
Nov 26 08:52:27.743: INFO: Created: latency-svc-9fkvq
Nov 26 08:52:27.761: INFO: Created: latency-svc-7xvk2
Nov 26 08:52:27.792: INFO: Got endpoints: latency-svc-fj9sd [609.10337ms]
Nov 26 08:52:27.806: INFO: Created: latency-svc-nwcp8
Nov 26 08:52:27.832: INFO: Got endpoints: latency-svc-g7b7x [630.835761ms]
Nov 26 08:52:27.846: INFO: Created: latency-svc-vk6qs
Nov 26 08:52:27.884: INFO: Got endpoints: latency-svc-c9k6g [638.703577ms]
Nov 26 08:52:27.915: INFO: Created: latency-svc-b8l7d
Nov 26 08:52:27.929: INFO: Got endpoints: latency-svc-5s8p8 [670.675217ms]
Nov 26 08:52:27.943: INFO: Created: latency-svc-89ldq
Nov 26 08:52:27.983: INFO: Got endpoints: latency-svc-894ht [707.385514ms]
Nov 26 08:52:27.995: INFO: Created: latency-svc-6p9ss
Nov 26 08:52:28.035: INFO: Got endpoints: latency-svc-rjwff [741.970997ms]
Nov 26 08:52:28.046: INFO: Created: latency-svc-7znnv
Nov 26 08:52:28.079: INFO: Got endpoints: latency-svc-4skk7 [746.872175ms]
Nov 26 08:52:28.098: INFO: Created: latency-svc-shbxs
Nov 26 08:52:28.165: INFO: Got endpoints: latency-svc-5htms [764.424216ms]
Nov 26 08:52:28.184: INFO: Got endpoints: latency-svc-9j2b9 [754.856266ms]
Nov 26 08:52:28.184: INFO: Created: latency-svc-xg75p
Nov 26 08:52:28.206: INFO: Created: latency-svc-8rnl4
Nov 26 08:52:28.235: INFO: Got endpoints: latency-svc-cv4c4 [754.546732ms]
Nov 26 08:52:28.246: INFO: Created: latency-svc-lnxkt
Nov 26 08:52:28.304: INFO: Got endpoints: latency-svc-fkvtw [769.91993ms]
Nov 26 08:52:28.321: INFO: Created: latency-svc-wnb2m
Nov 26 08:52:28.338: INFO: Got endpoints: latency-svc-4sw2m [754.759055ms]
Nov 26 08:52:28.355: INFO: Created: latency-svc-ctd6x
Nov 26 08:52:28.382: INFO: Got endpoints: latency-svc-wzrcp [719.351445ms]
Nov 26 08:52:28.434: INFO: Got endpoints: latency-svc-9fkvq [747.245929ms]
Nov 26 08:52:28.434: INFO: Created: latency-svc-6fgc4
Nov 26 08:52:28.458: INFO: Created: latency-svc-mzpxm
Nov 26 08:52:28.479: INFO: Got endpoints: latency-svc-7xvk2 [746.960837ms]
Nov 26 08:52:28.504: INFO: Created: latency-svc-lhldc
Nov 26 08:52:28.530: INFO: Got endpoints: latency-svc-nwcp8 [738.467565ms]
Nov 26 08:52:28.578: INFO: Created: latency-svc-9zxvr
Nov 26 08:52:28.582: INFO: Got endpoints: latency-svc-vk6qs [749.866434ms]
Nov 26 08:52:28.607: INFO: Created: latency-svc-lqr4x
Nov 26 08:52:28.628: INFO: Got endpoints: latency-svc-b8l7d [744.059688ms]
Nov 26 08:52:28.647: INFO: Created: latency-svc-5gp4c
Nov 26 08:52:28.685: INFO: Got endpoints: latency-svc-89ldq [755.625768ms]
Nov 26 08:52:28.710: INFO: Created: latency-svc-rmzqq
Nov 26 08:52:28.731: INFO: Got endpoints: latency-svc-6p9ss [747.598065ms]
Nov 26 08:52:28.761: INFO: Created: latency-svc-tnq6s
Nov 26 08:52:28.818: INFO: Got endpoints: latency-svc-7znnv [783.314287ms]
Nov 26 08:52:28.828: INFO: Got endpoints: latency-svc-shbxs [749.286935ms]
Nov 26 08:52:28.841: INFO: Created: latency-svc-8h52f
Nov 26 08:52:28.864: INFO: Created: latency-svc-kvwhd
Nov 26 08:52:28.880: INFO: Got endpoints: latency-svc-xg75p [714.939805ms]
Nov 26 08:52:28.910: INFO: Created: latency-svc-r9v4m
Nov 26 08:52:28.950: INFO: Got endpoints: latency-svc-8rnl4 [766.19378ms]
Nov 26 08:52:28.967: INFO: Created: latency-svc-8xmd2
Nov 26 08:52:28.978: INFO: Got endpoints: latency-svc-lnxkt [743.338241ms]
Nov 26 08:52:29.001: INFO: Created: latency-svc-h4gbc
Nov 26 08:52:29.028: INFO: Got endpoints: latency-svc-wnb2m [724.89778ms]
Nov 26 08:52:29.068: INFO: Created: latency-svc-ltrs9
Nov 26 08:52:29.081: INFO: Got endpoints: latency-svc-ctd6x [743.427537ms]
Nov 26 08:52:29.104: INFO: Created: latency-svc-l49c4
Nov 26 08:52:29.132: INFO: Got endpoints: latency-svc-6fgc4 [749.926871ms]
Nov 26 08:52:29.150: INFO: Created: latency-svc-6w4lg
Nov 26 08:52:29.200: INFO: Got endpoints: latency-svc-mzpxm [766.580248ms]
Nov 26 08:52:29.219: INFO: Created: latency-svc-6xjx7
Nov 26 08:52:29.230: INFO: Got endpoints: latency-svc-lhldc [750.870535ms]
Nov 26 08:52:29.247: INFO: Created: latency-svc-lcqms
Nov 26 08:52:29.280: INFO: Got endpoints: latency-svc-9zxvr [749.707173ms]
Nov 26 08:52:29.319: INFO: Created: latency-svc-gvr26
Nov 26 08:52:29.333: INFO: Got endpoints: latency-svc-lqr4x [750.948477ms]
Nov 26 08:52:29.350: INFO: Created: latency-svc-k8w2g
Nov 26 08:52:29.383: INFO: Got endpoints: latency-svc-5gp4c [755.573029ms]
Nov 26 08:52:29.401: INFO: Created: latency-svc-fjfs7
Nov 26 08:52:29.446: INFO: Got endpoints: latency-svc-rmzqq [760.97028ms]
Nov 26 08:52:29.464: INFO: Created: latency-svc-x5d5t
Nov 26 08:52:29.481: INFO: Got endpoints: latency-svc-tnq6s [749.592629ms]
Nov 26 08:52:29.499: INFO: Created: latency-svc-bvnk6
Nov 26 08:52:29.532: INFO: Got endpoints: latency-svc-8h52f [713.898517ms]
Nov 26 08:52:29.579: INFO: Got endpoints: latency-svc-kvwhd [750.59203ms]
Nov 26 08:52:29.579: INFO: Created: latency-svc-2vw8c
Nov 26 08:52:29.601: INFO: Created: latency-svc-f2tlp
Nov 26 08:52:29.629: INFO: Got endpoints: latency-svc-r9v4m [749.76983ms]
Nov 26 08:52:29.647: INFO: Created: latency-svc-wgbq9
Nov 26 08:52:29.716: INFO: Got endpoints: latency-svc-8xmd2 [765.966502ms]
Nov 26 08:52:29.733: INFO: Created: latency-svc-s5g89
Nov 26 08:52:29.733: INFO: Got endpoints: latency-svc-h4gbc [755.111647ms]
Nov 26 08:52:29.756: INFO: Created: latency-svc-n8drw
Nov 26 08:52:29.785: INFO: Got endpoints: latency-svc-ltrs9 [756.256786ms]
Nov 26 08:52:29.802: INFO: Created: latency-svc-ktsck
Nov 26 08:52:29.856: INFO: Got endpoints: latency-svc-l49c4 [774.896823ms]
Nov 26 08:52:29.876: INFO: Created: latency-svc-c684h
Nov 26 08:52:29.881: INFO: Got endpoints: latency-svc-6w4lg [749.632298ms]
Nov 26 08:52:29.905: INFO: Created: latency-svc-wlns5
Nov 26 08:52:29.927: INFO: Got endpoints: latency-svc-6xjx7 [726.975359ms]
Nov 26 08:52:29.950: INFO: Created: latency-svc-8gssm
Nov 26 08:52:29.984: INFO: Got endpoints: latency-svc-lcqms [753.951226ms]
Nov 26 08:52:30.002: INFO: Created: latency-svc-zsbqr
Nov 26 08:52:30.030: INFO: Got endpoints: latency-svc-gvr26 [749.605899ms]
Nov 26 08:52:30.048: INFO: Created: latency-svc-fdffn
Nov 26 08:52:30.081: INFO: Got endpoints: latency-svc-k8w2g [748.40901ms]
Nov 26 08:52:30.122: INFO: Created: latency-svc-h9wh6
Nov 26 08:52:30.127: INFO: Got endpoints: latency-svc-fjfs7 [743.879808ms]
Nov 26 08:52:30.151: INFO: Created: latency-svc-cjzvh
Nov 26 08:52:30.179: INFO: Got endpoints: latency-svc-x5d5t [732.495249ms]
Nov 26 08:52:30.202: INFO: Created: latency-svc-nk2c2
Nov 26 08:52:30.230: INFO: Got endpoints: latency-svc-bvnk6 [749.662879ms]
Nov 26 08:52:30.248: INFO: Created: latency-svc-29trr
Nov 26 08:52:30.282: INFO: Got endpoints: latency-svc-2vw8c [749.606721ms]
Nov 26 08:52:30.299: INFO: Created: latency-svc-kd79k
Nov 26 08:52:30.353: INFO: Got endpoints: latency-svc-f2tlp [774.505871ms]
Nov 26 08:52:30.368: INFO: Created: latency-svc-nskwr
Nov 26 08:52:30.379: INFO: Got endpoints: latency-svc-wgbq9 [749.453891ms]
Nov 26 08:52:30.396: INFO: Created: latency-svc-fxwm7
Nov 26 08:52:30.431: INFO: Got endpoints: latency-svc-s5g89 [714.830095ms]
Nov 26 08:52:30.448: INFO: Created: latency-svc-2klnv
Nov 26 08:52:30.488: INFO: Got endpoints: latency-svc-n8drw [754.12287ms]
Nov 26 08:52:30.505: INFO: Created: latency-svc-ncpzf
Nov 26 08:52:30.533: INFO: Got endpoints: latency-svc-ktsck [748.47349ms]
Nov 26 08:52:30.551: INFO: Created: latency-svc-jp2kj
Nov 26 08:52:30.596: INFO: Got endpoints: latency-svc-c684h [739.926183ms]
Nov 26 08:52:30.614: INFO: Created: latency-svc-zd7sd
Nov 26 08:52:30.631: INFO: Got endpoints: latency-svc-wlns5 [749.420288ms]
Nov 26 08:52:30.654: INFO: Created: latency-svc-2mdtq
Nov 26 08:52:30.682: INFO: Got endpoints: latency-svc-8gssm [755.271196ms]
Nov 26 08:52:30.716: INFO: Created: latency-svc-6nfbw
Nov 26 08:52:30.728: INFO: Got endpoints: latency-svc-zsbqr [744.008494ms]
Nov 26 08:52:30.751: INFO: Created: latency-svc-w5snl
Nov 26 08:52:30.780: INFO: Got endpoints: latency-svc-fdffn [749.813669ms]
Nov 26 08:52:30.796: INFO: Created: latency-svc-2jhcb
Nov 26 08:52:30.842: INFO: Got endpoints: latency-svc-h9wh6 [761.094377ms]
Nov 26 08:52:30.859: INFO: Created: latency-svc-wdqrs
Nov 26 08:52:30.883: INFO: Got endpoints: latency-svc-cjzvh [755.542055ms]
Nov 26 08:52:30.899: INFO: Created: latency-svc-t7j79
Nov 26 08:52:30.934: INFO: Got endpoints: latency-svc-nk2c2 [755.596472ms]
Nov 26 08:52:30.979: INFO: Created: latency-svc-glk4q
Nov 26 08:52:30.980: INFO: Got endpoints: latency-svc-29trr [749.326817ms]
Nov 26 08:52:31.002: INFO: Created: latency-svc-vqs2j
Nov 26 08:52:31.042: INFO: Got endpoints: latency-svc-kd79k [760.535154ms]
Nov 26 08:52:31.113: INFO: Got endpoints: latency-svc-nskwr [760.228669ms]
Nov 26 08:52:31.114: INFO: Created: latency-svc-wrljr
Nov 26 08:52:31.129: INFO: Created: latency-svc-lwnhj
Nov 26 08:52:31.129: INFO: Got endpoints: latency-svc-fxwm7 [749.902859ms]
Nov 26 08:52:31.151: INFO: Created: latency-svc-jnvjd
Nov 26 08:52:31.180: INFO: Got endpoints: latency-svc-2klnv [749.487059ms]
Nov 26 08:52:31.202: INFO: Created: latency-svc-6drns
Nov 26 08:52:31.256: INFO: Got endpoints: latency-svc-ncpzf [768.444874ms]
Nov 26 08:52:31.271: INFO: Created: latency-svc-kvv9g
Nov 26 08:52:31.283: INFO: Got endpoints: latency-svc-jp2kj [750.016701ms]
Nov 26 08:52:31.300: INFO: Created: latency-svc-8mprd
Nov 26 08:52:31.329: INFO: Got endpoints: latency-svc-zd7sd [732.787645ms]
Nov 26 08:52:31.351: INFO: Created: latency-svc-pfclv
Nov 26 08:52:31.387: INFO: Got endpoints: latency-svc-2mdtq [756.055306ms]
Nov 26 08:52:31.408: INFO: Created: latency-svc-f5wst
Nov 26 08:52:31.432: INFO: Got endpoints: latency-svc-6nfbw [749.94917ms]
Nov 26 08:52:31.454: INFO: Created: latency-svc-9lhvw
Nov 26 08:52:31.478: INFO: Got endpoints: latency-svc-w5snl [750.083318ms]
Nov 26 08:52:31.517: INFO: Created: latency-svc-kmlrx
Nov 26 08:52:31.530: INFO: Got endpoints: latency-svc-2jhcb [750.052402ms]
Nov 26 08:52:31.546: INFO: Created: latency-svc-rnq7g
Nov 26 08:52:31.581: INFO: Got endpoints: latency-svc-wdqrs [738.687753ms]
Nov 26 08:52:31.597: INFO: Created: latency-svc-jhw8l
Nov 26 08:52:31.645: INFO: Got endpoints: latency-svc-t7j79 [762.020325ms]
Nov 26 08:52:31.660: INFO: Created: latency-svc-9bvq9
Nov 26 08:52:31.683: INFO: Got endpoints: latency-svc-glk4q [748.731067ms]
Nov 26 08:52:31.700: INFO: Created: latency-svc-knk9l
Nov 26 08:52:31.730: INFO: Got endpoints: latency-svc-vqs2j [750.367576ms]
Nov 26 08:52:31.782: INFO: Created: latency-svc-tj2f9
Nov 26 08:52:31.786: INFO: Got endpoints: latency-svc-wrljr [743.495479ms]
Nov 26 08:52:31.803: INFO: Created: latency-svc-8pvpd
Nov 26 08:52:31.832: INFO: Got endpoints: latency-svc-lwnhj [718.091514ms]
Nov 26 08:52:31.849: INFO: Created: latency-svc-qr6qw
Nov 26 08:52:31.913: INFO: Got endpoints: latency-svc-jnvjd [784.327376ms]
Nov 26 08:52:32.032: INFO: Got endpoints: latency-svc-6drns [852.124019ms]
Nov 26 08:52:32.033: INFO: Got endpoints: latency-svc-kvv9g [776.539892ms]
Nov 26 08:52:32.033: INFO: Got endpoints: latency-svc-8mprd [749.333571ms]
Nov 26 08:52:32.033: INFO: Created: latency-svc-t464c
Nov 26 08:52:32.060: INFO: Created: latency-svc-6499l
Nov 26 08:52:32.077: INFO: Got endpoints: latency-svc-pfclv [748.376329ms]
Nov 26 08:52:32.078: INFO: Created: latency-svc-2fh6w
Nov 26 08:52:32.095: INFO: Created: latency-svc-845lz
Nov 26 08:52:32.117: INFO: Created: latency-svc-jbkw8
Nov 26 08:52:32.153: INFO: Got endpoints: latency-svc-f5wst [766.499851ms]
Nov 26 08:52:32.175: INFO: Created: latency-svc-ntc94
Nov 26 08:52:32.182: INFO: Got endpoints: latency-svc-9lhvw [749.901583ms]
Nov 26 08:52:32.203: INFO: Created: latency-svc-p7rq4
Nov 26 08:52:32.228: INFO: Got endpoints: latency-svc-kmlrx [749.69755ms]
Nov 26 08:52:32.249: INFO: Created: latency-svc-nmklf
Nov 26 08:52:32.280: INFO: Got endpoints: latency-svc-rnq7g [749.784327ms]
Nov 26 08:52:32.301: INFO: Created: latency-svc-hxlqv
Nov 26 08:52:32.331: INFO: Got endpoints: latency-svc-jhw8l [749.770272ms]
Nov 26 08:52:32.352: INFO: Created: latency-svc-wn99g
Nov 26 08:52:32.393: INFO: Got endpoints: latency-svc-9bvq9 [748.396746ms]
Nov 26 08:52:32.409: INFO: Created: latency-svc-88svf
Nov 26 08:52:32.428: INFO: Got endpoints: latency-svc-knk9l [745.114111ms]
Nov 26 08:52:32.449: INFO: Created: latency-svc-nlvl5
Nov 26 08:52:32.480: INFO: Got endpoints: latency-svc-tj2f9 [749.706895ms]
Nov 26 08:52:32.535: INFO: Created: latency-svc-2jrtx
Nov 26 08:52:32.543: INFO: Got endpoints: latency-svc-8pvpd [757.001801ms]
Nov 26 08:52:32.564: INFO: Created: latency-svc-24mjf
Nov 26 08:52:32.583: INFO: Got endpoints: latency-svc-qr6qw [751.194618ms]
Nov 26 08:52:32.604: INFO: Created: latency-svc-m64vd
Nov 26 08:52:32.657: INFO: Got endpoints: latency-svc-t464c [743.798034ms]
Nov 26 08:52:32.678: INFO: Got endpoints: latency-svc-6499l [645.024676ms]
Nov 26 08:52:32.678: INFO: Created: latency-svc-sn4qz
Nov 26 08:52:32.689: INFO: Created: latency-svc-8jvt8
Nov 26 08:52:32.732: INFO: Got endpoints: latency-svc-2fh6w [699.337908ms]
Nov 26 08:52:32.746: INFO: Created: latency-svc-5b7j5
Nov 26 08:52:32.812: INFO: Got endpoints: latency-svc-845lz [779.088801ms]
Nov 26 08:52:32.826: INFO: Created: latency-svc-rdrf4
Nov 26 08:52:32.832: INFO: Got endpoints: latency-svc-jbkw8 [754.586114ms]
Nov 26 08:52:32.843: INFO: Created: latency-svc-bbfr4
Nov 26 08:52:32.883: INFO: Got endpoints: latency-svc-ntc94 [730.120461ms]
Nov 26 08:52:32.895: INFO: Created: latency-svc-hkt6w
Nov 26 08:52:32.946: INFO: Got endpoints: latency-svc-p7rq4 [763.941116ms]
Nov 26 08:52:32.958: INFO: Created: latency-svc-74dp8
Nov 26 08:52:32.981: INFO: Got endpoints: latency-svc-nmklf [752.626882ms]
Nov 26 08:52:32.992: INFO: Created: latency-svc-b2npq
Nov 26 08:52:33.029: INFO: Got endpoints: latency-svc-hxlqv [749.472659ms]
Nov 26 08:52:33.072: INFO: Created: latency-svc-4kfrw
Nov 26 08:52:33.081: INFO: Got endpoints: latency-svc-wn99g [749.621105ms]
Nov 26 08:52:33.106: INFO: Created: latency-svc-pb549
Nov 26 08:52:33.133: INFO: Got endpoints: latency-svc-88svf [739.460811ms]
Nov 26 08:52:33.147: INFO: Created: latency-svc-v6m8l
Nov 26 08:52:33.190: INFO: Got endpoints: latency-svc-nlvl5 [761.415968ms]
Nov 26 08:52:33.215: INFO: Created: latency-svc-lkdgv
Nov 26 08:52:33.230: INFO: Got endpoints: latency-svc-2jrtx [749.939276ms]
Nov 26 08:52:33.244: INFO: Created: latency-svc-22f55
Nov 26 08:52:33.281: INFO: Got endpoints: latency-svc-24mjf [738.188568ms]
Nov 26 08:52:33.313: INFO: Created: latency-svc-dszhj
Nov 26 08:52:33.333: INFO: Got endpoints: latency-svc-m64vd [750.05661ms]
Nov 26 08:52:33.347: INFO: Created: latency-svc-pqvr7
Nov 26 08:52:33.384: INFO: Got endpoints: latency-svc-sn4qz [727.214632ms]
Nov 26 08:52:33.398: INFO: Created: latency-svc-9pc26
Nov 26 08:52:33.444: INFO: Got endpoints: latency-svc-8jvt8 [766.117875ms]
Nov 26 08:52:33.461: INFO: Created: latency-svc-j9gkb
Nov 26 08:52:33.482: INFO: Got endpoints: latency-svc-5b7j5 [749.784068ms]
Nov 26 08:52:33.501: INFO: Created: latency-svc-2zg4c
Nov 26 08:52:33.533: INFO: Got endpoints: latency-svc-rdrf4 [721.383066ms]
Nov 26 08:52:33.570: INFO: Created: latency-svc-bbnjz
Nov 26 08:52:33.579: INFO: Got endpoints: latency-svc-bbfr4 [746.873335ms]
Nov 26 08:52:33.598: INFO: Created: latency-svc-xqqm8
Nov 26 08:52:33.630: INFO: Got endpoints: latency-svc-hkt6w [746.916794ms]
Nov 26 08:52:33.650: INFO: Created: latency-svc-78qqj
Nov 26 08:52:33.690: INFO: Got endpoints: latency-svc-74dp8 [743.294461ms]
Nov 26 08:52:33.727: INFO: Got endpoints: latency-svc-b2npq [746.860633ms]
Nov 26 08:52:33.805: INFO: Got endpoints: latency-svc-4kfrw [775.608482ms]
Nov 26 08:52:33.838: INFO: Got endpoints: latency-svc-pb549 [757.474568ms]
Nov 26 08:52:33.882: INFO: Got endpoints: latency-svc-v6m8l [749.251018ms]
Nov 26 08:52:33.934: INFO: Got endpoints: latency-svc-lkdgv [743.98744ms]
Nov 26 08:52:33.979: INFO: Got endpoints: latency-svc-22f55 [749.388121ms]
Nov 26 08:52:34.050: INFO: Got endpoints: latency-svc-dszhj [769.285211ms]
Nov 26 08:52:34.090: INFO: Got endpoints: latency-svc-pqvr7 [757.327288ms]
Nov 26 08:52:34.136: INFO: Got endpoints: latency-svc-9pc26 [751.290262ms]
Nov 26 08:52:34.179: INFO: Got endpoints: latency-svc-j9gkb [735.353657ms]
Nov 26 08:52:34.231: INFO: Got endpoints: latency-svc-2zg4c [748.964852ms]
Nov 26 08:52:34.307: INFO: Got endpoints: latency-svc-bbnjz [773.86484ms]
Nov 26 08:52:34.328: INFO: Got endpoints: latency-svc-xqqm8 [748.890152ms]
Nov 26 08:52:34.387: INFO: Got endpoints: latency-svc-78qqj [756.647788ms]
Nov 26 08:52:34.387: INFO: Latencies: [28.256143ms 45.054789ms 48.727497ms 65.891049ms 89.02629ms 153.599509ms 165.31571ms 187.821668ms 210.688611ms 232.447663ms 296.560348ms 319.362644ms 335.465403ms 337.944119ms 338.131062ms 348.854011ms 348.907633ms 350.355354ms 352.660822ms 354.482813ms 354.636452ms 355.771833ms 359.196942ms 360.667766ms 361.185323ms 361.218862ms 361.271977ms 364.600392ms 365.459971ms 365.613524ms 366.80948ms 369.830652ms 370.930904ms 383.076211ms 383.40708ms 383.453074ms 383.794962ms 383.827416ms 383.881026ms 383.924425ms 387.97187ms 388.333376ms 389.311286ms 389.383079ms 394.400971ms 394.421742ms 395.212464ms 395.282288ms 395.287023ms 395.417772ms 395.431343ms 395.470361ms 399.820269ms 401.011063ms 401.127113ms 405.229092ms 405.549727ms 406.060323ms 412.708616ms 413.655626ms 419.524356ms 439.457914ms 469.532303ms 475.464264ms 541.297497ms 547.881601ms 572.183128ms 609.10337ms 630.835761ms 638.703577ms 645.024676ms 670.675217ms 699.337908ms 707.385514ms 713.898517ms 714.830095ms 714.939805ms 718.091514ms 719.351445ms 721.383066ms 724.89778ms 726.975359ms 727.214632ms 730.120461ms 732.495249ms 732.787645ms 735.353657ms 738.188568ms 738.467565ms 738.687753ms 739.460811ms 739.926183ms 741.970997ms 743.294461ms 743.338241ms 743.427537ms 743.495479ms 743.798034ms 743.879808ms 743.98744ms 744.008494ms 744.059688ms 745.114111ms 746.860633ms 746.872175ms 746.873335ms 746.916794ms 746.960837ms 747.245929ms 747.598065ms 748.376329ms 748.396746ms 748.40901ms 748.47349ms 748.731067ms 748.890152ms 748.964852ms 749.251018ms 749.286935ms 749.326817ms 749.333571ms 749.388121ms 749.420288ms 749.453891ms 749.472659ms 749.487059ms 749.592629ms 749.605899ms 749.606721ms 749.621105ms 749.632298ms 749.662879ms 749.69755ms 749.706895ms 749.707173ms 749.76983ms 749.770272ms 749.784068ms 749.784327ms 749.813669ms 749.866434ms 749.901583ms 749.902859ms 749.926871ms 749.939276ms 749.94917ms 750.016701ms 750.052402ms 750.05661ms 750.083318ms 750.367576ms 750.59203ms 750.870535ms 750.948477ms 751.194618ms 751.290262ms 752.626882ms 753.951226ms 754.12287ms 754.546732ms 754.586114ms 754.759055ms 754.856266ms 755.111647ms 755.271196ms 755.542055ms 755.573029ms 755.596472ms 755.625768ms 756.055306ms 756.256786ms 756.647788ms 757.001801ms 757.327288ms 757.474568ms 760.228669ms 760.535154ms 760.97028ms 761.094377ms 761.415968ms 762.020325ms 763.941116ms 764.424216ms 765.966502ms 766.117875ms 766.19378ms 766.499851ms 766.580248ms 768.444874ms 769.285211ms 769.91993ms 773.86484ms 774.505871ms 774.896823ms 775.608482ms 776.539892ms 779.088801ms 783.314287ms 784.327376ms 852.124019ms]
Nov 26 08:52:34.387: INFO: 50 %ile: 744.008494ms
Nov 26 08:52:34.387: INFO: 90 %ile: 762.020325ms
Nov 26 08:52:34.387: INFO: 99 %ile: 784.327376ms
Nov 26 08:52:34.387: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:34.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4774" for this suite.

• [SLOW TEST:10.805 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":322,"skipped":5575,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:34.395: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8573.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8573.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8573.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8573.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 26 08:52:36.517: INFO: DNS probes using dns-8573/dns-test-e477777b-7c3b-4a4e-b232-a97893827a70 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:36.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8573" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":323,"skipped":5578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:36.581: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 26 08:52:36.666: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:52:38.670: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 26 08:52:38.683: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:52:40.691: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 26 08:52:40.718: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 26 08:52:40.735: INFO: Pod pod-with-poststart-http-hook still exists
Nov 26 08:52:42.736: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 26 08:52:42.776: INFO: Pod pod-with-poststart-http-hook still exists
Nov 26 08:52:44.736: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 26 08:52:44.740: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:44.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2999" for this suite.

• [SLOW TEST:8.177 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":5617,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:44.758: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 26 08:52:44.854: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:51.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9338" for this suite.

• [SLOW TEST:7.206 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":325,"skipped":5635,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:51.964: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 26 08:52:54.029: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:54.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6411" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":5653,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:54.057: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-96a9e4ea-080b-45f9-872f-9aa2933acc64
STEP: Creating the pod
Nov 26 08:52:54.122: INFO: The status of Pod pod-configmaps-2857685c-51ec-41d3-8489-c66c5aee7304 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:52:56.126: INFO: The status of Pod pod-configmaps-2857685c-51ec-41d3-8489-c66c5aee7304 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-96a9e4ea-080b-45f9-872f-9aa2933acc64
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:52:58.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9478" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:52:58.150: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1335
Nov 26 08:52:58.265: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:53:00.282: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 26 08:53:00.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 26 08:53:00.456: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 26 08:53:00.456: INFO: stdout: "iptables"
Nov 26 08:53:00.456: INFO: proxyMode: iptables
Nov 26 08:53:00.470: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 26 08:53:00.482: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1335
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1335
I1126 08:53:00.540176      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1335, replica count: 3
I1126 08:53:03.591053      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:53:03.596: INFO: Creating new exec pod
Nov 26 08:53:06.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Nov 26 08:53:06.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 26 08:53:06.785: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:06.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.102.239 80'
Nov 26 08:53:06.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.102.239 80\nConnection to 10.96.102.239 80 port [tcp/http] succeeded!\n"
Nov 26 08:53:06.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:06.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.8 32677'
Nov 26 08:53:07.072: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.8 32677\nConnection to 172.21.7.8 32677 port [tcp/*] succeeded!\n"
Nov 26 08:53:07.072: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:07.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.12 32677'
Nov 26 08:53:07.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.12 32677\nConnection to 172.21.7.12 32677 port [tcp/*] succeeded!\n"
Nov 26 08:53:07.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:07.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.8:32677/ ; done'
Nov 26 08:53:07.474: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n"
Nov 26 08:53:07.474: INFO: stdout: "\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq\naffinity-nodeport-timeout-99mfq"
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Received response from host: affinity-nodeport-timeout-99mfq
Nov 26 08:53:07.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.8:32677/'
Nov 26 08:53:07.618: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n"
Nov 26 08:53:07.618: INFO: stdout: "affinity-nodeport-timeout-99mfq"
Nov 26 08:53:27.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-1335 exec execpod-affinity5n4h5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.7.8:32677/'
Nov 26 08:53:27.787: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.7.8:32677/\n"
Nov 26 08:53:27.787: INFO: stdout: "affinity-nodeport-timeout-g8s7s"
Nov 26 08:53:27.787: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1335, will wait for the garbage collector to delete the pods
Nov 26 08:53:27.890: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 2.545661ms
Nov 26 08:53:27.990: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.474255ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:30.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1335" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:32.485 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":328,"skipped":5713,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:30.635: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 26 08:53:30.698: INFO: Waiting up to 5m0s for pod "downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1" in namespace "downward-api-2597" to be "Succeeded or Failed"
Nov 26 08:53:30.707: INFO: Pod "downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.924222ms
Nov 26 08:53:32.736: INFO: Pod "downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037765442s
STEP: Saw pod success
Nov 26 08:53:32.736: INFO: Pod "downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1" satisfied condition "Succeeded or Failed"
Nov 26 08:53:32.738: INFO: Trying to get logs from node cncf-node3 pod downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1 container dapi-container: <nil>
STEP: delete the pod
Nov 26 08:53:32.764: INFO: Waiting for pod downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1 to disappear
Nov 26 08:53:32.769: INFO: Pod downward-api-c4b89708-9541-49b1-b4eb-02bea56b64e1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:32.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2597" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":5722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:32.774: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9283
STEP: creating service affinity-nodeport-transition in namespace services-9283
STEP: creating replication controller affinity-nodeport-transition in namespace services-9283
I1126 08:53:32.879246      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9283, replica count: 3
I1126 08:53:35.930202      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:53:35.935: INFO: Creating new exec pod
Nov 26 08:53:38.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-9283 exec execpod-affinity5zhrg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Nov 26 08:53:39.133: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 26 08:53:39.133: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:39.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-9283 exec execpod-affinity5zhrg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.175.83 80'
Nov 26 08:53:39.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.175.83 80\nConnection to 10.96.175.83 80 port [tcp/http] succeeded!\n"
Nov 26 08:53:39.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:39.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-9283 exec execpod-affinity5zhrg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.8 32156'
Nov 26 08:53:39.420: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.8 32156\nConnection to 172.21.7.8 32156 port [tcp/*] succeeded!\n"
Nov 26 08:53:39.420: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:39.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-9283 exec execpod-affinity5zhrg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.7.12 32156'
Nov 26 08:53:39.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.7.12 32156\nConnection to 172.21.7.12 32156 port [tcp/*] succeeded!\n"
Nov 26 08:53:39.552: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 26 08:53:39.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-9283 exec execpod-affinity5zhrg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.8:32156/ ; done'
Nov 26 08:53:39.783: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n"
Nov 26 08:53:39.783: INFO: stdout: "\naffinity-nodeport-transition-rmd4h\naffinity-nodeport-transition-9v4x4\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-9v4x4\naffinity-nodeport-transition-rmd4h\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-rmd4h\naffinity-nodeport-transition-rmd4h\naffinity-nodeport-transition-9v4x4\naffinity-nodeport-transition-rmd4h\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-rmd4h\naffinity-nodeport-transition-9v4x4\naffinity-nodeport-transition-ntxzn"
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-rmd4h
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-9v4x4
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-9v4x4
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-rmd4h
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-rmd4h
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-rmd4h
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-9v4x4
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-rmd4h
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-rmd4h
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-9v4x4
Nov 26 08:53:39.783: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:39.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=services-9283 exec execpod-affinity5zhrg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.7.8:32156/ ; done'
Nov 26 08:53:40.041: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.7.8:32156/\n"
Nov 26 08:53:40.041: INFO: stdout: "\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn\naffinity-nodeport-transition-ntxzn"
Nov 26 08:53:40.041: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Received response from host: affinity-nodeport-transition-ntxzn
Nov 26 08:53:40.042: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9283, will wait for the garbage collector to delete the pods
Nov 26 08:53:40.126: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.470354ms
Nov 26 08:53:40.227: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.453342ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9283" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.041 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":330,"skipped":5752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:42.815: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Nov 26 08:53:42.867: INFO: observed Pod pod-test in namespace pods-8107 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 26 08:53:42.869: INFO: observed Pod pod-test in namespace pods-8107 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:42 +0000 UTC  }]
Nov 26 08:53:43.737: INFO: observed Pod pod-test in namespace pods-8107 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:42 +0000 UTC  }]
Nov 26 08:53:44.065: INFO: observed Pod pod-test in namespace pods-8107 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:42 +0000 UTC  }]
Nov 26 08:53:44.551: INFO: Found Pod pod-test in namespace pods-8107 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-26 08:53:42 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Nov 26 08:53:44.561: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Nov 26 08:53:44.608: INFO: observed event type ADDED
Nov 26 08:53:44.608: INFO: observed event type MODIFIED
Nov 26 08:53:44.608: INFO: observed event type MODIFIED
Nov 26 08:53:44.608: INFO: observed event type MODIFIED
Nov 26 08:53:44.608: INFO: observed event type MODIFIED
Nov 26 08:53:44.609: INFO: observed event type MODIFIED
Nov 26 08:53:44.609: INFO: observed event type MODIFIED
Nov 26 08:53:44.609: INFO: observed event type MODIFIED
Nov 26 08:53:46.581: INFO: observed event type MODIFIED
Nov 26 08:53:46.879: INFO: observed event type MODIFIED
Nov 26 08:53:47.562: INFO: observed event type MODIFIED
Nov 26 08:53:47.594: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:47.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8107" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":331,"skipped":5805,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:47.620: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:49.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1427" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":332,"skipped":5808,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:49.701: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Nov 26 08:53:49.767: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:53:51.770: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:52.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9654" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":333,"skipped":5813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:52.790: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:52.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-17" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":334,"skipped":5861,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:52.915: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-brwfj in namespace proxy-9618
I1126 08:53:53.105434      22 runners.go:190] Created replication controller with name: proxy-service-brwfj, namespace: proxy-9618, replica count: 1
I1126 08:53:54.156383      22 runners.go:190] proxy-service-brwfj Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1126 08:53:55.157261      22 runners.go:190] proxy-service-brwfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1126 08:53:56.157957      22 runners.go:190] proxy-service-brwfj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 26 08:53:56.159: INFO: setup took 3.100782316s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.17088ms)
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.377034ms)
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.406318ms)
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.396504ms)
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.430414ms)
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.557009ms)
Nov 26 08:53:56.163: INFO: (0) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 4.041485ms)
Nov 26 08:53:56.164: INFO: (0) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 4.447882ms)
Nov 26 08:53:56.164: INFO: (0) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 4.512874ms)
Nov 26 08:53:56.164: INFO: (0) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 4.678783ms)
Nov 26 08:53:56.165: INFO: (0) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 5.188543ms)
Nov 26 08:53:56.169: INFO: (0) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 10.016774ms)
Nov 26 08:53:56.170: INFO: (0) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 10.109835ms)
Nov 26 08:53:56.170: INFO: (0) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 10.196271ms)
Nov 26 08:53:56.170: INFO: (0) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 10.136341ms)
Nov 26 08:53:56.170: INFO: (0) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 10.103366ms)
Nov 26 08:53:56.172: INFO: (1) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 2.469582ms)
Nov 26 08:53:56.172: INFO: (1) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.516704ms)
Nov 26 08:53:56.172: INFO: (1) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 2.50566ms)
Nov 26 08:53:56.172: INFO: (1) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 2.834453ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.146465ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.128972ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.101484ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.118259ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.122131ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.161115ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.273226ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.502358ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.495093ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.499679ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.62679ms)
Nov 26 08:53:56.173: INFO: (1) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.575487ms)
Nov 26 08:53:56.176: INFO: (2) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.630701ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.168014ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.289275ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.277818ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.576105ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.644708ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.584213ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.657084ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.633869ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.655216ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 4.031993ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 4.097147ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 4.069284ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 4.095144ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 4.21263ms)
Nov 26 08:53:56.177: INFO: (2) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 4.11181ms)
Nov 26 08:53:56.180: INFO: (3) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.493072ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.103786ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.141406ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.123028ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.067223ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.07526ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.505448ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.498835ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.545945ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.505777ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.504175ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.539917ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.516258ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.559062ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.561792ms)
Nov 26 08:53:56.181: INFO: (3) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.627158ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 2.501913ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 2.49551ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.510299ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.797992ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.830921ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 2.898566ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.854965ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.839166ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 2.914286ms)
Nov 26 08:53:56.184: INFO: (4) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 2.888823ms)
Nov 26 08:53:56.185: INFO: (4) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.397425ms)
Nov 26 08:53:56.185: INFO: (4) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.595488ms)
Nov 26 08:53:56.185: INFO: (4) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.558173ms)
Nov 26 08:53:56.185: INFO: (4) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.566789ms)
Nov 26 08:53:56.185: INFO: (4) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.549771ms)
Nov 26 08:53:56.185: INFO: (4) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.551847ms)
Nov 26 08:53:56.187: INFO: (5) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 1.765898ms)
Nov 26 08:53:56.188: INFO: (5) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.216098ms)
Nov 26 08:53:56.188: INFO: (5) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.25221ms)
Nov 26 08:53:56.188: INFO: (5) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.359609ms)
Nov 26 08:53:56.188: INFO: (5) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.429834ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.565893ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.599023ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.567538ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.581895ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.622754ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.620221ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.637687ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.614351ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.610654ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.634879ms)
Nov 26 08:53:56.189: INFO: (5) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.657297ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.060942ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.055292ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.24059ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.143394ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.229365ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.223023ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.292986ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.313686ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.343702ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.475035ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.530897ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.539478ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.569041ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.583208ms)
Nov 26 08:53:56.192: INFO: (6) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.607955ms)
Nov 26 08:53:56.193: INFO: (6) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.962291ms)
Nov 26 08:53:56.195: INFO: (7) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.500965ms)
Nov 26 08:53:56.195: INFO: (7) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 2.546545ms)
Nov 26 08:53:56.195: INFO: (7) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.529959ms)
Nov 26 08:53:56.195: INFO: (7) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.584954ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 2.833054ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.023281ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.006372ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.036178ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.042163ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.102447ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.095767ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.189273ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.34961ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.385425ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.337496ms)
Nov 26 08:53:56.196: INFO: (7) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.439082ms)
Nov 26 08:53:56.202: INFO: (8) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 6.015838ms)
Nov 26 08:53:56.202: INFO: (8) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 6.070311ms)
Nov 26 08:53:56.202: INFO: (8) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 6.209697ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 6.28757ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 6.329996ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 6.401773ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 6.417172ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 6.38545ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 6.629034ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 6.750047ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 6.968462ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 7.065203ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 7.053237ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 7.093028ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 7.057229ms)
Nov 26 08:53:56.203: INFO: (8) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 7.114329ms)
Nov 26 08:53:56.206: INFO: (9) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 2.738395ms)
Nov 26 08:53:56.206: INFO: (9) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 2.875403ms)
Nov 26 08:53:56.206: INFO: (9) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.006424ms)
Nov 26 08:53:56.206: INFO: (9) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.082638ms)
Nov 26 08:53:56.206: INFO: (9) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.038931ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.206389ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.214686ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.239967ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.299796ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.280227ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.405484ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.454803ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.45358ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.509065ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.494229ms)
Nov 26 08:53:56.207: INFO: (9) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.622051ms)
Nov 26 08:53:56.209: INFO: (10) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.411845ms)
Nov 26 08:53:56.209: INFO: (10) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 2.39759ms)
Nov 26 08:53:56.209: INFO: (10) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 2.378879ms)
Nov 26 08:53:56.210: INFO: (10) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.383397ms)
Nov 26 08:53:56.210: INFO: (10) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 2.392708ms)
Nov 26 08:53:56.210: INFO: (10) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 2.644991ms)
Nov 26 08:53:56.210: INFO: (10) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.693949ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.432985ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.418017ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.463211ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.493134ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.439124ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.493418ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.503927ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.687374ms)
Nov 26 08:53:56.211: INFO: (10) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.715194ms)
Nov 26 08:53:56.213: INFO: (11) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 1.87046ms)
Nov 26 08:53:56.214: INFO: (11) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.097615ms)
Nov 26 08:53:56.214: INFO: (11) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.10369ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.941358ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.951866ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.987388ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.97619ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 4.009049ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 4.004139ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 4.13845ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 4.405908ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 4.451653ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 4.449729ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 4.463735ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 4.442882ms)
Nov 26 08:53:56.215: INFO: (11) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 4.40987ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.182747ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.386773ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.401813ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.430534ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.533398ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.688747ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.707809ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.714723ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.786653ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.743712ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.781602ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.758229ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.750806ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.900009ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.856804ms)
Nov 26 08:53:56.219: INFO: (12) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.909373ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.684951ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 2.733983ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 2.779562ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.786978ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.851109ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 2.800317ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 2.885479ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 2.986254ms)
Nov 26 08:53:56.222: INFO: (13) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.100072ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.227138ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.197741ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.247417ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.251728ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.322229ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.605893ms)
Nov 26 08:53:56.223: INFO: (13) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.575173ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.523375ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 2.68424ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 2.972596ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.200606ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.159228ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.169464ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.175055ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.185737ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.19671ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.246036ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.386423ms)
Nov 26 08:53:56.226: INFO: (14) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.281696ms)
Nov 26 08:53:56.227: INFO: (14) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.466232ms)
Nov 26 08:53:56.227: INFO: (14) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.483269ms)
Nov 26 08:53:56.227: INFO: (14) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.473916ms)
Nov 26 08:53:56.227: INFO: (14) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.590573ms)
Nov 26 08:53:56.229: INFO: (15) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 2.732744ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.972871ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.165143ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.197673ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.248471ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.292603ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.278783ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.354665ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.673871ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.738132ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.776313ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.840897ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.767929ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.806913ms)
Nov 26 08:53:56.230: INFO: (15) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.754136ms)
Nov 26 08:53:56.231: INFO: (15) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.981569ms)
Nov 26 08:53:56.233: INFO: (16) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.302876ms)
Nov 26 08:53:56.233: INFO: (16) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.382349ms)
Nov 26 08:53:56.233: INFO: (16) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 2.688233ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.737253ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 2.783885ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.182817ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.298541ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.388833ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.42699ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.500416ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.50907ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.482415ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.628987ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.573919ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.633874ms)
Nov 26 08:53:56.234: INFO: (16) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.627007ms)
Nov 26 08:53:56.237: INFO: (17) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 2.948874ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.515366ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.53809ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.571655ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.540116ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.573422ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.636108ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.693078ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.656447ms)
Nov 26 08:53:56.238: INFO: (17) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 4.059027ms)
Nov 26 08:53:56.239: INFO: (17) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 4.098414ms)
Nov 26 08:53:56.239: INFO: (17) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 4.048745ms)
Nov 26 08:53:56.239: INFO: (17) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 4.02185ms)
Nov 26 08:53:56.239: INFO: (17) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 4.072547ms)
Nov 26 08:53:56.239: INFO: (17) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 4.07032ms)
Nov 26 08:53:56.239: INFO: (17) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 4.137628ms)
Nov 26 08:53:56.241: INFO: (18) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.08302ms)
Nov 26 08:53:56.241: INFO: (18) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 2.039689ms)
Nov 26 08:53:56.241: INFO: (18) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 2.668932ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.139226ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.09662ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.165617ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 3.139352ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.229521ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.16652ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.231438ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.309901ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 3.333899ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.371706ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.409943ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.373648ms)
Nov 26 08:53:56.242: INFO: (18) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.520549ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 2.423042ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">... (200; 2.848588ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:443/proxy/tlsrewritem... (200; 2.907416ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.11589ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname1/proxy/: foo (200; 3.155475ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname1/proxy/: tls baz (200; 3.136484ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h/proxy/rewriteme">test</a> (200; 3.110617ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname1/proxy/: foo (200; 3.111215ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/services/https:proxy-service-brwfj:tlsportname2/proxy/: tls qux (200; 3.12506ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/services/proxy-service-brwfj:portname2/proxy/: bar (200; 3.144721ms)
Nov 26 08:53:56.245: INFO: (19) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:460/proxy/: tls baz (200; 3.26272ms)
Nov 26 08:53:56.246: INFO: (19) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/: <a href="/api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:1080/proxy/rewriteme">test<... (200; 3.326427ms)
Nov 26 08:53:56.246: INFO: (19) /api/v1/namespaces/proxy-9618/services/http:proxy-service-brwfj:portname2/proxy/: bar (200; 3.383974ms)
Nov 26 08:53:56.246: INFO: (19) /api/v1/namespaces/proxy-9618/pods/http:proxy-service-brwfj-dnm2h:160/proxy/: foo (200; 3.399565ms)
Nov 26 08:53:56.246: INFO: (19) /api/v1/namespaces/proxy-9618/pods/proxy-service-brwfj-dnm2h:162/proxy/: bar (200; 3.514821ms)
Nov 26 08:53:56.246: INFO: (19) /api/v1/namespaces/proxy-9618/pods/https:proxy-service-brwfj-dnm2h:462/proxy/: tls qux (200; 3.625861ms)
STEP: deleting ReplicationController proxy-service-brwfj in namespace proxy-9618, will wait for the garbage collector to delete the pods
Nov 26 08:53:56.302: INFO: Deleting ReplicationController proxy-service-brwfj took: 3.724741ms
Nov 26 08:53:56.403: INFO: Terminating ReplicationController proxy-service-brwfj pods took: 100.58374ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:58.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9618" for this suite.

• [SLOW TEST:5.917 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":335,"skipped":5879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:58.832: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Nov 26 08:53:58.991: INFO: Found Service test-service-569wv in namespace services-7067 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 26 08:53:58.991: INFO: Service test-service-569wv created
STEP: Getting /status
Nov 26 08:53:58.997: INFO: Service test-service-569wv has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Nov 26 08:53:59.001: INFO: observed Service test-service-569wv in namespace services-7067 with annotations: map[createdTime:2021-11-26T17:54:00.186236821+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:54:00.186236821+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & LoadBalancer: {[]}
Nov 26 08:53:59.001: INFO: Found Service test-service-569wv in namespace services-7067 with annotations: map[createdTime:2021-11-26T17:54:00.186236821+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2021-11-26T17:54:00.186236821+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 26 08:53:59.001: INFO: Service test-service-569wv has service status patched
STEP: updating the ServiceStatus
Nov 26 08:53:59.005: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Nov 26 08:53:59.006: INFO: Observed Service test-service-569wv in namespace services-7067 with annotations: map[createdTime:2021-11-26T17:54:00.186236821+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount updatedTime:2021-11-26T17:54:00.186236821+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: {[]}
Nov 26 08:53:59.006: INFO: Observed event: &Service{ObjectMeta:{test-service-569wv  services-7067  3f8b1c18-5812-452d-a84c-87fc123860c3 136489 0 2021-11-26 08:53:58 +0000 UTC <nil> <nil> map[test-service-static:true] map[createdTime:2021-11-26T17:54:00.186236821+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2021-11-26T17:54:00.186236821+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [] []  [{e2e.test Update v1 2021-11-26 08:53:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2021-11-26 08:53:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.84.121,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.84.121],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 26 08:53:59.006: INFO: Found Service test-service-569wv in namespace services-7067 with annotations: map[createdTime:2021-11-26T17:54:00.186236821+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2021-11-26T17:54:00.186236821+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 26 08:53:59.006: INFO: Service test-service-569wv has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Nov 26 08:53:59.015: INFO: observed Service test-service-569wv in namespace services-7067 with labels: map[test-service-static:true]
Nov 26 08:53:59.015: INFO: observed Service test-service-569wv in namespace services-7067 with labels: map[test-service-static:true]
Nov 26 08:53:59.015: INFO: observed Service test-service-569wv in namespace services-7067 with labels: map[test-service-static:true]
Nov 26 08:53:59.015: INFO: Found Service test-service-569wv in namespace services-7067 with labels: map[test-service:patched test-service-static:true]
Nov 26 08:53:59.015: INFO: Service test-service-569wv patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Nov 26 08:53:59.028: INFO: Observed event: ADDED
Nov 26 08:53:59.028: INFO: Observed event: MODIFIED
Nov 26 08:53:59.028: INFO: Observed event: MODIFIED
Nov 26 08:53:59.028: INFO: Observed event: MODIFIED
Nov 26 08:53:59.028: INFO: Found Service test-service-569wv in namespace services-7067 with labels: map[test-service:patched test-service-static:true] & annotations: map[createdTime:2021-11-26T17:54:00.186236821+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount patchedstatus:true updatedTime:2021-11-26T17:54:00.186236821+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount]
Nov 26 08:53:59.028: INFO: Service test-service-569wv deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:53:59.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7067" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":336,"skipped":5917,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:53:59.039: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-cdfd1509-21ed-4bec-872a-3114623f90ca
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:54:01.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5415" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":337,"skipped":5919,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:54:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:54:01.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8329" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":338,"skipped":5932,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:54:01.206: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492
Nov 26 08:54:01.278: INFO: Pod name my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492: Found 0 pods out of 1
Nov 26 08:54:06.290: INFO: Pod name my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492: Found 1 pods out of 1
Nov 26 08:54:06.290: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492" are running
Nov 26 08:54:06.296: INFO: Pod "my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492-6gwgd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 08:54:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 08:54:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 08:54:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-26 08:54:01 +0000 UTC Reason: Message:}])
Nov 26 08:54:06.296: INFO: Trying to dial the pod
Nov 26 08:54:11.303: INFO: Controller my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492: Got expected result from replica 1 [my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492-6gwgd]: "my-hostname-basic-8cb99dfe-c2a1-4125-a6aa-a330acc1e492-6gwgd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:54:11.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1762" for this suite.

• [SLOW TEST:10.102 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":339,"skipped":5951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:54:11.309: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 26 08:54:11.352: INFO: Waiting up to 5m0s for pod "pod-eec93e29-7774-4d45-a169-380a8ff98f96" in namespace "emptydir-4857" to be "Succeeded or Failed"
Nov 26 08:54:11.375: INFO: Pod "pod-eec93e29-7774-4d45-a169-380a8ff98f96": Phase="Pending", Reason="", readiness=false. Elapsed: 23.616588ms
Nov 26 08:54:13.378: INFO: Pod "pod-eec93e29-7774-4d45-a169-380a8ff98f96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026226024s
Nov 26 08:54:15.382: INFO: Pod "pod-eec93e29-7774-4d45-a169-380a8ff98f96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030180169s
STEP: Saw pod success
Nov 26 08:54:15.382: INFO: Pod "pod-eec93e29-7774-4d45-a169-380a8ff98f96" satisfied condition "Succeeded or Failed"
Nov 26 08:54:15.384: INFO: Trying to get logs from node cncf-node3 pod pod-eec93e29-7774-4d45-a169-380a8ff98f96 container test-container: <nil>
STEP: delete the pod
Nov 26 08:54:15.399: INFO: Waiting for pod pod-eec93e29-7774-4d45-a169-380a8ff98f96 to disappear
Nov 26 08:54:15.405: INFO: Pod pod-eec93e29-7774-4d45-a169-380a8ff98f96 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:54:15.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4857" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":5990,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:54:15.410: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Nov 26 08:54:15.453: INFO: created test-podtemplate-1
Nov 26 08:54:15.457: INFO: created test-podtemplate-2
Nov 26 08:54:15.462: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 26 08:54:15.501: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 26 08:54:15.514: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:54:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6964" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":341,"skipped":5993,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:54:15.524: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-866
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-866
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-866
Nov 26 08:54:15.576: INFO: Found 0 stateful pods, waiting for 1
Nov 26 08:54:25.579: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 26 08:54:25.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:54:25.737: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:54:25.738: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:54:25.738: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:54:25.753: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 26 08:54:35.756: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:54:35.756: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:54:35.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999712s
Nov 26 08:54:36.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995786425s
Nov 26 08:54:37.772: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993508311s
Nov 26 08:54:38.775: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991028686s
Nov 26 08:54:39.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988581851s
Nov 26 08:54:40.781: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98457465s
Nov 26 08:54:41.784: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982283762s
Nov 26 08:54:42.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979066268s
Nov 26 08:54:43.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.976153234s
Nov 26 08:54:44.793: INFO: Verifying statefulset ss doesn't scale past 1 for another 972.745741ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-866
Nov 26 08:54:45.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:54:45.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:54:45.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:54:45.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:54:45.953: INFO: Found 1 stateful pods, waiting for 3
Nov 26 08:54:55.961: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:54:55.961: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 26 08:54:55.961: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 26 08:54:55.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:54:56.124: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:54:56.125: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:54:56.125: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:54:56.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:54:56.261: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:54:56.261: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:54:56.261: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:54:56.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 26 08:54:56.395: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 26 08:54:56.395: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 26 08:54:56.395: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 26 08:54:56.395: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:54:56.396: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 26 08:55:06.402: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:55:06.402: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:55:06.402: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 26 08:55:06.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999709s
Nov 26 08:55:07.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993775742s
Nov 26 08:55:08.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989038669s
Nov 26 08:55:09.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984496942s
Nov 26 08:55:10.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9806847s
Nov 26 08:55:11.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976650049s
Nov 26 08:55:12.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973202794s
Nov 26 08:55:13.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970648929s
Nov 26 08:55:14.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966627763s
Nov 26 08:55:15.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.349204ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-866
Nov 26 08:55:16.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:55:16.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:55:16.613: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:55:16.613: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:55:16.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:55:16.772: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:55:16.772: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:55:16.772: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:55:16.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-426159780 --namespace=statefulset-866 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 26 08:55:17.019: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 26 08:55:17.019: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 26 08:55:17.019: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 26 08:55:17.019: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 26 08:55:27.027: INFO: Deleting all statefulset in ns statefulset-866
Nov 26 08:55:27.029: INFO: Scaling statefulset ss to 0
Nov 26 08:55:27.035: INFO: Waiting for statefulset status.replicas updated to 0
Nov 26 08:55:27.037: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:55:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-866" for this suite.

• [SLOW TEST:71.527 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":342,"skipped":5997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:55:27.051: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9480
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 26 08:55:27.116: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 26 08:55:27.145: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:55:29.164: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:31.148: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:33.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:35.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:37.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:39.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:41.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:43.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:45.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:47.232: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 26 08:55:49.148: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 26 08:55:49.151: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 26 08:55:51.206: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Nov 26 08:55:51.206: INFO: Going to poll 10.244.89.123 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Nov 26 08:55:51.207: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.89.123 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9480 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:55:51.207: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 08:55:52.310: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 26 08:55:52.310: INFO: Going to poll 10.244.35.153 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Nov 26 08:55:52.311: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.35.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9480 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 26 08:55:52.311: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
Nov 26 08:55:53.414: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:55:53.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9480" for this suite.

• [SLOW TEST:26.369 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:55:53.421: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Nov 26 08:55:53.470: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.470: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.482: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.482: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.581: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.581: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.613: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:53.613: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 26 08:55:54.994: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 26 08:55:54.994: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 26 08:55:55.037: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Nov 26 08:55:55.043: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 0
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.044: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.051: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.051: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.066: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.066: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:55.082: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:55:55.082: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:55:55.111: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:55:55.111: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:55:56.020: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:56.020: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:55:56.052: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
STEP: listing Deployments
Nov 26 08:55:56.062: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Nov 26 08:55:56.070: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Nov 26 08:55:56.077: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:56.083: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:56.126: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:56.163: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:56.185: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:58.112: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:58.129: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:58.140: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:58.151: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:55:58.163: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 26 08:56:00.798: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 1
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 2
Nov 26 08:56:00.885: INFO: observed Deployment test-deployment in namespace deployment-8223 with ReadyReplicas 3
STEP: deleting the Deployment
Nov 26 08:56:00.889: INFO: observed event type MODIFIED
Nov 26 08:56:00.889: INFO: observed event type MODIFIED
Nov 26 08:56:00.889: INFO: observed event type MODIFIED
Nov 26 08:56:00.889: INFO: observed event type MODIFIED
Nov 26 08:56:00.889: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
Nov 26 08:56:00.890: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 26 08:56:00.894: INFO: Log out all the ReplicaSets if there is no deployment created
Nov 26 08:56:00.898: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-8223  06db66cd-d944-4376-8213-4ee3dd557824 137774 4 2021-11-26 08:55:55 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[createdTime:2021-11-26T17:55:54.695523609+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2 updatedTime:2021-11-26T17:55:54.695523609+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment d668438a-6c12-45eb-9732-7dd39cfd0cc5 0xc0055813f7 0xc0055813f8}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:55:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d668438a-6c12-45eb-9732-7dd39cfd0cc5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:56:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005581480 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 26 08:56:00.900: INFO: pod: "test-deployment-56c98d85f9-6bkrn":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-6bkrn test-deployment-56c98d85f9- deployment-8223  1ee1e58e-d1de-48d3-86fc-4a23e5e0a05f 137761 0 2021-11-26 08:55:56 +0000 UTC 2021-11-26 08:55:59 +0000 UTC 0xc005581928 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:9f4ba120987b09733c4d583442c6fca2da4db2bcf5cb5b8771a63962f3be941b cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: createdTime:2021-11-26T17:55:57.305938191+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:55:57.305938191+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 06db66cd-d944-4376-8213-4ee3dd557824 0xc005581987 0xc005581988}] []  [{calico Update v1 2021-11-26 08:55:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2021-11-26 08:55:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06db66cd-d944-4376-8213-4ee3dd557824\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:55:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r26f2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r26f2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.148,StartTime:2021-11-26 08:55:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:55:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:cri-o://e7eaf8b23d5b59fb918a2f181f444876d19a436c3a13e202de49af5482a4cc0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 26 08:56:00.901: INFO: pod: "test-deployment-56c98d85f9-85w2s":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-85w2s test-deployment-56c98d85f9- deployment-8223  8385f681-be87-4b1f-9225-d69ba9657554 137770 0 2021-11-26 08:55:55 +0000 UTC 2021-11-26 08:56:01 +0000 UTC 0xc005581b70 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:ea557f7bbfe13cffa8a0df8e7a1b45ca47bf3db69a56c3b408883e551d37d042 cni.projectcalico.org/podIP:10.244.35.131/32 cni.projectcalico.org/podIPs:10.244.35.131/32 createdTime:2021-11-26T17:55:56.294350055+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:55:56.294350055+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 06db66cd-d944-4376-8213-4ee3dd557824 0xc005581c07 0xc005581c08}] []  [{calico Update v1 2021-11-26 08:55:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2021-11-26 08:55:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06db66cd-d944-4376-8213-4ee3dd557824\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:55:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s254r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s254r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.131,StartTime:2021-11-26 08:55:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:55:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:cri-o://9c2d73aa7d9e1e05583720a4ce6297aba3bc93ea48b160ba9bea5191001575a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 26 08:56:00.901: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-8223  a7699b6e-c14a-48c9-a446-a1c2f21ef4d7 137613 3 2021-11-26 08:55:53 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[createdTime:2021-11-26T17:55:54.695523609+09:00 creator:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1 updatedTime:2021-11-26T17:55:54.695523609+09:00 updater:system:serviceaccount:sonobuoy:sonobuoy-serviceaccount] [{apps/v1 Deployment test-deployment d668438a-6c12-45eb-9732-7dd39cfd0cc5 0xc005581517 0xc005581518}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:55:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:createdTime":{},"f:creator":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{},"f:updatedTime":{},"f:updater":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d668438a-6c12-45eb-9732-7dd39cfd0cc5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:55:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0055815a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 26 08:56:00.903: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-8223  508bb7f7-e6c4-4372-a2fd-9aa0deb34924 137766 2 2021-11-26 08:55:56 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[createdTime:2021-11-26T17:55:57.314365483+09:00 creator:system:serviceaccount:kube-system:deployment-controller deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3 updatedTime:2021-11-26T17:55:57.314365483+09:00 updater:system:serviceaccount:kube-system:deployment-controller] [{apps/v1 Deployment test-deployment d668438a-6c12-45eb-9732-7dd39cfd0cc5 0xc005581637 0xc005581638}] []  [{kube-controller-manager Update apps/v1 2021-11-26 08:55:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d668438a-6c12-45eb-9732-7dd39cfd0cc5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-26 08:55:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0055816c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Nov 26 08:56:00.905: INFO: pod: "test-deployment-d4dfddfbf-vdkld":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-vdkld test-deployment-d4dfddfbf- deployment-8223  5a0e4834-0ff9-461b-bd09-4b8e69028f30 137675 0 2021-11-26 08:55:56 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:ac6b191b4bc2c4f9f337f6296549aaf946f7c28e9a1edefc89cb79e516767638 cni.projectcalico.org/podIP:10.244.35.152/32 cni.projectcalico.org/podIPs:10.244.35.152/32 createdTime:2021-11-26T17:55:57.355395597+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:55:57.355395597+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 508bb7f7-e6c4-4372-a2fd-9aa0deb34924 0xc0070de8c7 0xc0070de8c8}] []  [{calico Update v1 2021-11-26 08:55:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2021-11-26 08:55:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508bb7f7-e6c4-4372-a2fd-9aa0deb34924\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-26 08:55:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtt89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtt89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.152,StartTime:2021-11-26 08:55:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:55:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://fd3d3b7d647aeaa9fc75b86959a02ca4c318d0f61492cdd63922f7b60db7b656,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 26 08:56:00.905: INFO: pod: "test-deployment-d4dfddfbf-w9pvf":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-w9pvf test-deployment-d4dfddfbf- deployment-8223  74461042-2c17-4810-b403-95716f17eb66 137765 0 2021-11-26 08:55:58 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:a6ee3f38d8d4d6add440910facf682e7e0685526496475aedbf9372f87ae8168 cni.projectcalico.org/podIP:10.244.35.164/32 cni.projectcalico.org/podIPs:10.244.35.164/32 createdTime:2021-11-26T17:55:59.347015806+09:00 creator:system:serviceaccount:kube-system:replicaset-controller updatedTime:2021-11-26T17:55:59.347015806+09:00 updater:system:serviceaccount:kube-system:replicaset-controller] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 508bb7f7-e6c4-4372-a2fd-9aa0deb34924 0xc0070deb27 0xc0070deb28}] []  [{kube-controller-manager Update v1 2021-11-26 08:55:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508bb7f7-e6c4-4372-a2fd-9aa0deb34924\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-26 08:55:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-26 08:56:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.35.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cg7j5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cg7j5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cncf-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-26 08:55:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.21.7.12,PodIP:10.244.35.164,StartTime:2021-11-26 08:55:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-26 08:56:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://2a094ea13b77dacf7057caab830e7548bb93f26e81b9973d214ed9e584afb85b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.35.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:56:00.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8223" for this suite.

• [SLOW TEST:7.497 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":344,"skipped":6048,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:56:00.918: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Nov 26 08:56:00.977: INFO: Waiting up to 5m0s for pod "var-expansion-465687a8-c666-436d-ba94-c754932bf479" in namespace "var-expansion-7499" to be "Succeeded or Failed"
Nov 26 08:56:00.979: INFO: Pod "var-expansion-465687a8-c666-436d-ba94-c754932bf479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357151ms
Nov 26 08:56:02.982: INFO: Pod "var-expansion-465687a8-c666-436d-ba94-c754932bf479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004447111s
Nov 26 08:56:04.984: INFO: Pod "var-expansion-465687a8-c666-436d-ba94-c754932bf479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007382169s
Nov 26 08:56:06.992: INFO: Pod "var-expansion-465687a8-c666-436d-ba94-c754932bf479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015012597s
STEP: Saw pod success
Nov 26 08:56:06.992: INFO: Pod "var-expansion-465687a8-c666-436d-ba94-c754932bf479" satisfied condition "Succeeded or Failed"
Nov 26 08:56:06.994: INFO: Trying to get logs from node cncf-node3 pod var-expansion-465687a8-c666-436d-ba94-c754932bf479 container dapi-container: <nil>
STEP: delete the pod
Nov 26 08:56:07.021: INFO: Waiting for pod var-expansion-465687a8-c666-436d-ba94-c754932bf479 to disappear
Nov 26 08:56:07.026: INFO: Pod var-expansion-465687a8-c666-436d-ba94-c754932bf479 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:56:07.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7499" for this suite.

• [SLOW TEST:6.112 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6079,"failed":0}
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 26 08:56:07.030: INFO: >>> kubeConfig: /tmp/kubeconfig-426159780
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 26 08:56:07.112: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:56:09.123: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 26 08:56:09.141: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 26 08:56:11.204: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Nov 26 08:56:11.208: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 26 08:56:11.215: INFO: Pod pod-with-prestop-http-hook still exists
Nov 26 08:56:13.217: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 26 08:56:13.219: INFO: Pod pod-with-prestop-http-hook still exists
Nov 26 08:56:15.217: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 26 08:56:15.221: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 26 08:56:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5372" for this suite.

• [SLOW TEST:8.200 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6079,"failed":0}
SSSSSSSNov 26 08:56:15.230: INFO: Running AfterSuite actions on all nodes
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Nov 26 08:56:15.230: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Nov 26 08:56:15.230: INFO: Running AfterSuite actions on node 1
Nov 26 08:56:15.230: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6086,"failed":0}

Ran 346 of 6432 Specs in 5732.223 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6086 Skipped
PASS

Ginkgo ran 1 suite in 1h35m34.195514222s
Test Suite Passed
